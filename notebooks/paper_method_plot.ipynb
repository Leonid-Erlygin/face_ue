{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "        p(\\textbf{z}|c) &= \\mathcal{C}_d(\\kappa)\\exp\\left(\\kappa\\mu^T_{c}\\textbf{z}\\right)\\\\\n",
    "        \\mathcal{C}_d(\\kappa) &= \\frac{(\\kappa)^{d/2-1}}{(2\\pi)^{d/2}\\mathcal{I}_{d/2-1}(\\kappa)}\n",
    "\\end{align}\n",
    "\n",
    "With d=2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import iv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# mpl.style.use('classic')\n",
    "\n",
    "\n",
    "def z_Prob(z, mus, kappa, d=2, beta=0.5):\n",
    "    K = mus.shape[-1]\n",
    "    p_c = (1 - beta) / K\n",
    "    class_probs = np.array([z_vonMises_dencity(z, mu, kappa) for mu in mus.T])\n",
    "    return np.sum(class_probs * p_c) + (1 / (2 * np.pi)) * beta\n",
    "\n",
    "\n",
    "def z_class_prob(class_id, z, mus, kappa, d=2, beta=0.5):\n",
    "    p_z = z_Prob(z, mus, kappa, d, beta)\n",
    "    K = mus.shape[1]\n",
    "    if class_id == K:\n",
    "        p_c = beta\n",
    "        return (1 / (2 * np.pi)) * p_c / p_z\n",
    "    else:\n",
    "        p_c = (1 - beta) / K\n",
    "        return (z_vonMises_dencity(z, mus[:, class_id], kappa) * p_c) / p_z\n",
    "\n",
    "\n",
    "def z_vonMises_dencity(z, mu_c, kappa, d=2):\n",
    "    C_d = kappa ** (d / 2 - 1) / ((2 * np.pi) ** (d / 2) * iv(d / 2 - 1, kappa))\n",
    "    return C_d * np.exp(kappa * np.dot(z, mu_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_by_angle(angles):\n",
    "    return np.array([[np.cos(plot_angle), np.sin(plot_angle)] for plot_angle in angles])\n",
    "\n",
    "\n",
    "def compute_class_probs(class_id, zs, mus, kappa, beta):\n",
    "    class_probes = []\n",
    "    for z in zs:\n",
    "        class_prob = z_class_prob(class_id, z, mus, kappa, beta=beta)\n",
    "        class_probes.append(class_prob)\n",
    "    class_probes = np.array(class_probes)\n",
    "    return class_probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_circle(ax, linewidth, zorder=4):\n",
    "    # plot circle\n",
    "    theta = np.linspace(0, 2 * np.pi, 150)\n",
    "    a = np.cos(theta)\n",
    "    b = np.sin(theta)\n",
    "    circle = plt.Circle((0, 0), 1, color=\"blue\", zorder=4, alpha=0.1)\n",
    "    ax.add_patch(circle)\n",
    "    ax.plot(a, b, color=\"tab:gray\", zorder=4, linewidth=linewidth)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "\n",
    "def draw_decity(ax):\n",
    "    pass\n",
    "\n",
    "\n",
    "def draw_example(kappa, gallery_class_angles, text_shift, save_name, beta=0.5):\n",
    "    fontsize = 20\n",
    "    linewidth = 3\n",
    "    dot_size = 80\n",
    "\n",
    "    test_color = \"tab:cyan\"\n",
    "    # gallery_class_angles = [0.4, 0.25]\n",
    "    ident_uncertain_test_point = (\n",
    "        gallery_class_angles[-1] + gallery_class_angles[-2]\n",
    "    ) / 2 - 0.02\n",
    "    test_points_angles = [\n",
    "        ident_uncertain_test_point,\n",
    "        gallery_class_angles[-1]\n",
    "        - (ident_uncertain_test_point - gallery_class_angles[-1]),\n",
    "    ]\n",
    "\n",
    "    gallery_class_angles = np.array(gallery_class_angles) * 2 * np.pi\n",
    "    test_points_angles = np.array(test_points_angles) * 2 * np.pi\n",
    "    theta = np.linspace(0, 2 * np.pi, 150)\n",
    "\n",
    "    colors = list(mcolors.TABLEAU_COLORS)[: len(gallery_class_angles)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    draw_circle(ax, linewidth)\n",
    "\n",
    "    draw_dencity_angles = np.linspace(-np.pi / 3, np.pi / 3, 150)\n",
    "    mus = np.stack([np.cos(gallery_class_angles), np.sin(gallery_class_angles)], axis=0)\n",
    "    class_to_class_probs = []\n",
    "\n",
    "    for i, (angle, color) in enumerate(zip(gallery_class_angles, colors)):\n",
    "        mu_c = mus[:, i]\n",
    "        plot_angles = angle + draw_dencity_angles\n",
    "        zs = get_vectors_by_angle(plot_angles)\n",
    "        class_probes = compute_class_probs(i, zs, mus, kappa, beta)\n",
    "        v = zs.T * (1 + class_probes[np.newaxis, :])\n",
    "        ax.plot(v[0], v[1], c=color, linewidth=linewidth)\n",
    "\n",
    "        ax.scatter([np.cos(angle)], [np.sin(angle)], c=color, s=dot_size, zorder=5)\n",
    "        ax.scatter([0], [0], color=\"black\", s=20)\n",
    "\n",
    "        # plot decity\n",
    "        # ax.scatter(points[:, 0], points[:, 1], color=color, s=3)\n",
    "\n",
    "    # plot_uniform_prob\n",
    "    zs = get_vectors_by_angle(theta)\n",
    "    class_probes = compute_class_probs(mus.shape[1], zs, mus, kappa, beta)\n",
    "    v = zs.T * (1 + class_probes[np.newaxis, :])\n",
    "    # ax.plot(v[0], v[1], color='black')\n",
    "\n",
    "    # plot unc\n",
    "    all_probs = []\n",
    "    for i in range(mus.shape[1] + 1):\n",
    "        class_probes = compute_class_probs(i, zs, mus, kappa, beta)\n",
    "        all_probs.append(class_probes)\n",
    "    all_probs = np.stack(all_probs, axis=0)\n",
    "    unc = -np.sum(all_probs * np.log(all_probs), axis=0)\n",
    "    # unc = -np.max(all_probs, axis=0) + 1\n",
    "    v = zs.T * (1 + unc[np.newaxis, :])\n",
    "    ax.plot(v[0], v[1], color=\"tab:red\", linewidth=linewidth)\n",
    "\n",
    "    # plot test points\n",
    "    for test_angle in test_points_angles:\n",
    "        ax.scatter(\n",
    "            [np.cos(test_angle)],\n",
    "            [np.sin(test_angle)],\n",
    "            c=test_color,\n",
    "            s=dot_size,\n",
    "            zorder=5,\n",
    "        )\n",
    "\n",
    "    test_point_vectors = get_vectors_by_angle(test_points_angles)\n",
    "    # entropy value\n",
    "\n",
    "    probs_at_test_points = []\n",
    "    for i in range(mus.shape[1] + 1):\n",
    "        class_probes = compute_class_probs(i, test_point_vectors, mus, kappa, beta)\n",
    "        probs_at_test_points.append(class_probes)\n",
    "    probs_at_test_points = np.stack(probs_at_test_points, axis=0)\n",
    "\n",
    "    unc_test = -np.sum(probs_at_test_points * np.log(probs_at_test_points), axis=0)\n",
    "    unc_test = np.round(unc_test, 2)\n",
    "    # unc_test = -np.max(probs_at_test_points, axis=0) + 1\n",
    "    # unc_test = np.round(unc_test, 2)\n",
    "    ax.annotate(\n",
    "        f\"${unc_test[0]}$\",\n",
    "        xy=test_point_vectors[0],\n",
    "        xytext=[\n",
    "            test_point_vectors[0][0] + text_shift[0][0],\n",
    "            test_point_vectors[0][1] + text_shift[0][1],\n",
    "        ],\n",
    "        fontsize=fontsize,\n",
    "    )\n",
    "    ax.annotate(\n",
    "        f\"${unc_test[1]}$\",\n",
    "        xy=test_point_vectors[1],\n",
    "        xytext=[\n",
    "            test_point_vectors[1][0] + text_shift[1][0],\n",
    "            test_point_vectors[1][1] + text_shift[1][1],\n",
    "        ],\n",
    "        fontsize=fontsize,\n",
    "    )\n",
    "    fig.gca().set_aspect(\"equal\")\n",
    "    plt.savefig(save_name, dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_example(\n",
    "#     kappa=15,\n",
    "#     gallery_class_angles=[0.4, 0.25],\n",
    "#     text_shift=[[-0.2, -0.3], [-0.2, 0.1]],\n",
    "#     save_name=\"test.png\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Identification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_example(kappa = 15, gallery_class_angles = [0.4, 0.25], text_shift = [[-0.2, -0.3], [-0.2, 0.1]], save_name='/app/paper_assets/images/false_ident_example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False accept/reject example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_example(kappa = 13, gallery_class_angles = [0.48, 0.25], text_shift = [[-0.1, -0.3], [-0.35, 0.2]], save_name='/app/paper_assets/images/false_accept-reject_example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vmf_dencity(\n",
    "    angle,\n",
    "    kappa,\n",
    "    ax,\n",
    "    linewidth,\n",
    "    color,\n",
    "    range=np.pi / 3,\n",
    "    scale=1,\n",
    "    draw_center=False,\n",
    "    dot_size=40,\n",
    "):\n",
    "    draw_dencity_angles = np.linspace(-range, range, 150)\n",
    "    plot_angles = angle + draw_dencity_angles\n",
    "    zs = get_vectors_by_angle(plot_angles)\n",
    "    mu = get_vectors_by_angle([angle])[0]\n",
    "    dencities = z_vonMises_dencity(zs, mu, kappa)\n",
    "    v = zs.T * (1 + dencities * scale)\n",
    "    ax.plot(v[0], v[1], c=color, linewidth=linewidth)\n",
    "    if draw_center:\n",
    "        ax.scatter([np.cos(angle)], [np.sin(angle)], c=color, s=dot_size, zorder=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.samplers import random_VMF, VonMisesFisher\n",
    "\n",
    "gallery_features = []  # N gallery samples X 512\n",
    "gallery_unc_log = []  # N gallery samples X 1\n",
    "gallery_subject_ids_sorted = []  # N gallery samples\n",
    "\n",
    "\n",
    "class_center_angles = np.array([0, np.pi / 2, np.pi])\n",
    "class_z_kappa = np.array([9, 6, 5])\n",
    "colors = list(mcolors.TABLEAU_COLORS)[: len(class_center_angles)]\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(2)\n",
    "\n",
    "for class_id, (angle, kappa) in enumerate(zip(class_center_angles, class_z_kappa)):\n",
    "    num_samples = rng.integers(5, 8)\n",
    "    gallery_subject_ids_sorted.extend([class_id] * num_samples)\n",
    "    gallery_unc_log.extend(3 * rng.random(num_samples) + 3)\n",
    "    samples = random_VMF(\n",
    "        get_vectors_by_angle([angle])[0], kappa=kappa, size=num_samples\n",
    "    )\n",
    "    gallery_features.extend(samples)\n",
    "\n",
    "gallery_features = np.array(gallery_features)\n",
    "gallery_unc_log = np.array(gallery_unc_log).reshape(-1, 1)\n",
    "gallery_subject_ids_sorted = np.array(gallery_subject_ids_sorted)\n",
    "\n",
    "gallery_unc = np.exp(gallery_unc_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = VonMisesFisher(3)\n",
    "feature_mean = sampler(np.array([[0, 1], [1, 0]]), np.array([[10], [15]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fontsize = 20\n",
    "linewidth = 3\n",
    "dot_size = 80\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "draw_circle(ax, linewidth)\n",
    "for class_id, (angle, kappa) in enumerate(zip(class_center_angles, class_z_kappa)):\n",
    "    color = colors[class_id]\n",
    "    draw_vmf_dencity(\n",
    "        angle,\n",
    "        kappa,\n",
    "        ax,\n",
    "        linewidth=3,\n",
    "        color=color,\n",
    "        range=np.pi / 2,\n",
    "        draw_center=True,\n",
    "        dot_size=dot_size,\n",
    "    )\n",
    "    for position in np.where(gallery_subject_ids_sorted == class_id)[0]:\n",
    "        point_angle = np.angle(\n",
    "            [gallery_features[position][0] + 1j * gallery_features[position][1]]\n",
    "        )[0]\n",
    "        draw_vmf_dencity(\n",
    "            point_angle,\n",
    "            gallery_unc[position],\n",
    "            ax,\n",
    "            linewidth=1,\n",
    "            color=color,\n",
    "            range=np.pi / 2,\n",
    "            scale=0.1,\n",
    "            draw_center=True,\n",
    "            dot_size=20,\n",
    "        )\n",
    "fig.gca().set_aspect(\"equal\")\n",
    "fig.show()\n",
    "plt.savefig(\"/app/outputs/images/generation.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "gallery_features.shape\n",
    "init_mean = np.array(\n",
    "    [\n",
    "        np.mean(gallery_features[gallery_subject_ids_sorted == c], axis=0)\n",
    "        for c in range(3)\n",
    "    ]\n",
    ")\n",
    "# init_mean = init_mean / np.linalg.norm(init_mean, axis=1, keepdims=True)\n",
    "\n",
    "init_kappa = np.array(\n",
    "    [np.mean(gallery_unc[gallery_subject_ids_sorted == c], axis=0) for c in range(3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19, 2), (19, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gallery_features.shape, gallery_unc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from scipy.special import ive, hyp0f1, loggamma\n",
    "\n",
    "\n",
    "class MonteCarloPredictiveProb:\n",
    "    def __init__(\n",
    "        self,\n",
    "        M: int,\n",
    "        gallery_prior: str = \"power\",\n",
    "        unc_model: str = \"vMF\",\n",
    "        beta: float = 0.5,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        params:\n",
    "        M -- number of MC samples\n",
    "        gallery_prior -- model for p(z|c)\n",
    "        unc_model -- form of p(z|x)\n",
    "        \"\"\"\n",
    "        self.M = M\n",
    "        assert gallery_prior in [\"power\", \"vMF\"]\n",
    "        assert unc_model in [\"vMF\", \"PFE\"]\n",
    "        if unc_model == \"vMF\":\n",
    "            self.sampler = VonMisesFisher(self.M)\n",
    "        self.gallery_prior = gallery_prior\n",
    "        self.unc_model = unc_model\n",
    "        self.beta = beta\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        mean: np.array,\n",
    "        kappa: np.array,\n",
    "        gallery_means: torch.nn.Parameter,\n",
    "        gallery_kappas: torch.nn.Parameter,\n",
    "        T: torch.nn.Parameter,\n",
    "    ) -> Any:\n",
    "        self.K = gallery_means.shape[0]\n",
    "        # print(self.K)\n",
    "        zs = torch.tensor(self.sampler(mean, kappa))\n",
    "        d = torch.tensor([mean.shape[-1]])\n",
    "        # print(zs.shape, gallery_means.shape)\n",
    "        # print(zs, gallery_means)\n",
    "        similarities = zs @ gallery_means.T\n",
    "        # print(similarities.shape)\n",
    "        # print(similarities)\n",
    "        if self.gallery_prior == \"power\":\n",
    "            log_m_c_power = (\n",
    "                torch.special.gammaln(d - 1 + gallery_kappas)\n",
    "                + torch.special.gammaln(d / 2 + gallery_kappas)\n",
    "                + gallery_kappas * np.log(2)\n",
    "                - torch.special.gammaln(d / 2)\n",
    "                - torch.special.gammaln(d - 1 + 2 * gallery_kappas)\n",
    "            )\n",
    "            m_c_power = torch.exp(log_m_c_power)\n",
    "            log_uniform_dencity = (\n",
    "                torch.special.gammaln(d / 2) - np.log(2) - (d / 2) * np.log(np.pi)\n",
    "            )\n",
    "            log_normalizer = log_m_c_power + log_uniform_dencity\n",
    "        # compute log z prob\n",
    "        p_c = ((1 - self.beta) / self.K) ** (1 / T)\n",
    "        # print(similarities.shape, gallery_kappas.shape, log_uniform.shape, m_c_power.shape)\n",
    "        logit_sum = (\n",
    "            torch.sum(\n",
    "                (m_c_power[..., :, 0] ** (1 / T))\n",
    "                * ((1 + similarities) ** (gallery_kappas[..., :, 0] * (1 / T))),\n",
    "                dim=-1,\n",
    "            )\n",
    "            * p_c\n",
    "        )\n",
    "        log_z_prob = (1 / T) * log_uniform_dencity + torch.log(\n",
    "            logit_sum + (self.beta) ** (1 / T)\n",
    "        )\n",
    "\n",
    "        log_beta = np.log(self.beta)\n",
    "        # print(similarities.shape, gallery_kappas.shape)\n",
    "        uniform_log_prob = (1 / T) * (log_uniform_dencity + log_beta) - log_z_prob\n",
    "\n",
    "        # compute gallery classes log prob\n",
    "        pz_c = (\n",
    "            torch.log((1 + similarities)) * gallery_kappas[..., :, 0]\n",
    "            + log_normalizer[..., :, 0]\n",
    "        )\n",
    "        # print(pz_c.shape, log_z_prob.shape)\n",
    "        gallery_log_probs = (1 / T) * (\n",
    "            pz_c + np.log((1 - self.beta) / self.K)\n",
    "        ) - log_z_prob[..., np.newaxis]\n",
    "        # print(uniform_log_prob.shape)\n",
    "        log_probs = torch.cat(\n",
    "            [gallery_log_probs, uniform_log_prob[..., np.newaxis]], dim=-1\n",
    "        )\n",
    "        # print(log_probs.shape)\n",
    "        # print(torch.sum(log_probs, dim=-1))\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_kappa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_kappa.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9679, -0.1708],\n",
       "        [ 0.1550,  0.9253],\n",
       "        [-0.9410, -0.0842]], dtype=torch.float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(init_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2686, -0.9633],\n",
      "        [-0.9175, -0.3978],\n",
      "        [-0.5455,  0.8381]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5749],\n",
      "        [2.8657],\n",
      "        [7.7744]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 0, Loss: 3.280250166701953\n",
      "tensor([[ 0.2803, -0.9599],\n",
      "        [-0.9226, -0.3857],\n",
      "        [-0.5570,  0.8305]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5649],\n",
      "        [2.8558],\n",
      "        [7.7644]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 1, Loss: 3.1645472112001882\n",
      "tensor([[ 0.2921, -0.9564],\n",
      "        [-0.9276, -0.3735],\n",
      "        [-0.5683,  0.8228]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5549],\n",
      "        [2.8458],\n",
      "        [7.7545]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 2, Loss: 3.0652109687330946\n",
      "tensor([[ 0.3037, -0.9528],\n",
      "        [-0.9325, -0.3613],\n",
      "        [-0.5796,  0.8149]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5451],\n",
      "        [2.8359],\n",
      "        [7.7446]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 3, Loss: 2.957366991370287\n",
      "tensor([[ 0.3152, -0.9490],\n",
      "        [-0.9371, -0.3490],\n",
      "        [-0.5907,  0.8069]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5352],\n",
      "        [2.8259],\n",
      "        [7.7347]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 4, Loss: 2.991992141897187\n",
      "tensor([[ 0.3267, -0.9451],\n",
      "        [-0.9416, -0.3367],\n",
      "        [-0.6018,  0.7987]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5254],\n",
      "        [2.8160],\n",
      "        [7.7248]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 5, Loss: 2.907704825530848\n",
      "tensor([[ 0.3381, -0.9411],\n",
      "        [-0.9459, -0.3244],\n",
      "        [-0.6126,  0.7904]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5156],\n",
      "        [2.8061],\n",
      "        [7.7149]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 6, Loss: 2.8176856003534194\n",
      "tensor([[ 0.3495, -0.9369],\n",
      "        [-0.9501, -0.3121],\n",
      "        [-0.6234,  0.7819]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5059],\n",
      "        [2.7963],\n",
      "        [7.7051]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 7, Loss: 2.7443733378628146\n",
      "tensor([[ 0.3607, -0.9327],\n",
      "        [-0.9540, -0.2997],\n",
      "        [-0.6339,  0.7734]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4962],\n",
      "        [2.7865],\n",
      "        [7.6954]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 8, Loss: 2.750211384177421\n",
      "tensor([[ 0.3719, -0.9283],\n",
      "        [-0.9579, -0.2873],\n",
      "        [-0.6443,  0.7648]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4865],\n",
      "        [2.7766],\n",
      "        [7.6858]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 9, Loss: 2.736013869561457\n",
      "tensor([[ 0.3830, -0.9238],\n",
      "        [-0.9615, -0.2748],\n",
      "        [-0.6546,  0.7560]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4769],\n",
      "        [2.7668],\n",
      "        [7.6762]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 10, Loss: 2.667813197910873\n",
      "tensor([[ 0.3940, -0.9191],\n",
      "        [-0.9650, -0.2624],\n",
      "        [-0.6647,  0.7471]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4674],\n",
      "        [2.7570],\n",
      "        [7.6667]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 11, Loss: 2.591028470115775\n",
      "tensor([[ 0.4048, -0.9144],\n",
      "        [-0.9683, -0.2500],\n",
      "        [-0.6746,  0.7382]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4579],\n",
      "        [2.7473],\n",
      "        [7.6572]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 12, Loss: 2.532891708520676\n",
      "tensor([[ 0.4156, -0.9096],\n",
      "        [-0.9714, -0.2375],\n",
      "        [-0.6844,  0.7291]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4486],\n",
      "        [2.7376],\n",
      "        [7.6478]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 13, Loss: 2.504907075192764\n",
      "tensor([[ 0.4261, -0.9047],\n",
      "        [-0.9744, -0.2250],\n",
      "        [-0.6939,  0.7200]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4394],\n",
      "        [2.7279],\n",
      "        [7.6386]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 14, Loss: 2.384928458702909\n",
      "tensor([[ 0.4366, -0.8997],\n",
      "        [-0.9772, -0.2125],\n",
      "        [-0.7033,  0.7109]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4303],\n",
      "        [2.7181],\n",
      "        [7.6295]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 15, Loss: 2.4152419681621984\n",
      "tensor([[ 0.4470, -0.8946],\n",
      "        [-0.9798, -0.2000],\n",
      "        [-0.7125,  0.7017]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4212],\n",
      "        [2.7085],\n",
      "        [7.6205]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 16, Loss: 2.406333821789005\n",
      "tensor([[ 0.4572, -0.8894],\n",
      "        [-0.9823, -0.1875],\n",
      "        [-0.7215,  0.6924]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4123],\n",
      "        [2.6988],\n",
      "        [7.6117]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 17, Loss: 2.308306571738476\n",
      "tensor([[ 0.4673, -0.8841],\n",
      "        [-0.9846, -0.1751],\n",
      "        [-0.7303,  0.6831]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4035],\n",
      "        [2.6892],\n",
      "        [7.6029]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 18, Loss: 2.2703605201461423\n",
      "tensor([[ 0.4773, -0.8787],\n",
      "        [-0.9867, -0.1627],\n",
      "        [-0.7390,  0.6737]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3948],\n",
      "        [2.6797],\n",
      "        [7.5942]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 19, Loss: 2.2222985968733133\n",
      "tensor([[ 0.4871, -0.8733],\n",
      "        [-0.9886, -0.1504],\n",
      "        [-0.7475,  0.6643]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3862],\n",
      "        [2.6703],\n",
      "        [7.5857]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 20, Loss: 2.08971881280482\n",
      "tensor([[ 0.4968, -0.8679],\n",
      "        [-0.9904, -0.1381],\n",
      "        [-0.7557,  0.6549]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3778],\n",
      "        [2.6609],\n",
      "        [7.5774]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 21, Loss: 2.1383592794718855\n",
      "tensor([[ 0.5063, -0.8623],\n",
      "        [-0.9920, -0.1259],\n",
      "        [-0.7638,  0.6455]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3694],\n",
      "        [2.6516],\n",
      "        [7.5692]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 22, Loss: 2.032378184143037\n",
      "tensor([[ 0.5158, -0.8567],\n",
      "        [-0.9935, -0.1138],\n",
      "        [-0.7716,  0.6361]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3612],\n",
      "        [2.6424],\n",
      "        [7.5613]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 23, Loss: 2.0373361553616958\n",
      "tensor([[ 0.5250, -0.8511],\n",
      "        [-0.9948, -0.1018],\n",
      "        [-0.7793,  0.6266]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3531],\n",
      "        [2.6333],\n",
      "        [7.5535]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 24, Loss: 2.004817972729958\n",
      "tensor([[ 0.5341, -0.8454],\n",
      "        [-0.9960, -0.0899],\n",
      "        [-0.7868,  0.6172]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3452],\n",
      "        [2.6243],\n",
      "        [7.5458]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 25, Loss: 1.8885409132992768\n",
      "tensor([[ 0.5431, -0.8397],\n",
      "        [-0.9970, -0.0780],\n",
      "        [-0.7941,  0.6077]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3374],\n",
      "        [2.6153],\n",
      "        [7.5383]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 26, Loss: 1.9428876636472387\n",
      "tensor([[ 0.5518, -0.8340],\n",
      "        [-0.9978, -0.0662],\n",
      "        [-0.8012,  0.5983]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3299],\n",
      "        [2.6065],\n",
      "        [7.5310]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 27, Loss: 1.8424380941087606\n",
      "tensor([[ 0.5604, -0.8282],\n",
      "        [-0.9985, -0.0545],\n",
      "        [-0.8082,  0.5890]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3225],\n",
      "        [2.5977],\n",
      "        [7.5239]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 28, Loss: 1.8592748863879418\n",
      "tensor([[ 0.5689, -0.8224],\n",
      "        [-0.9991, -0.0428],\n",
      "        [-0.8149,  0.5796]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3153],\n",
      "        [2.5889],\n",
      "        [7.5170]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 29, Loss: 1.835947082091982\n",
      "tensor([[ 0.5772, -0.8166],\n",
      "        [-0.9995, -0.0312],\n",
      "        [-0.8214,  0.5703]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3082],\n",
      "        [2.5802],\n",
      "        [7.5101]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 30, Loss: 1.816870427350339\n",
      "tensor([[ 0.5853, -0.8108],\n",
      "        [-0.9998, -0.0197],\n",
      "        [-0.8278,  0.5610]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3013],\n",
      "        [2.5717],\n",
      "        [7.5035]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 31, Loss: 1.7311236660898606\n",
      "tensor([[ 0.5933, -0.8050],\n",
      "        [-1.0000, -0.0084],\n",
      "        [-0.8340,  0.5517]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2946],\n",
      "        [2.5632],\n",
      "        [7.4970]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 32, Loss: 1.7113027569582169\n",
      "tensor([[ 0.6011, -0.7991],\n",
      "        [-1.0000,  0.0030],\n",
      "        [-0.8401,  0.5424]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2879],\n",
      "        [2.5548],\n",
      "        [7.4906]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 33, Loss: 1.7799471584866442\n",
      "tensor([[ 0.6089, -0.7933],\n",
      "        [-0.9999,  0.0142],\n",
      "        [-0.8460,  0.5333]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2814],\n",
      "        [2.5464],\n",
      "        [7.4844]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 34, Loss: 1.6942128904704237\n",
      "tensor([[ 0.6165, -0.7873],\n",
      "        [-0.9997,  0.0254],\n",
      "        [-0.8517,  0.5241]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2749],\n",
      "        [2.5381],\n",
      "        [7.4784]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 35, Loss: 1.6974437160378382\n",
      "tensor([[ 0.6240, -0.7814],\n",
      "        [-0.9993,  0.0365],\n",
      "        [-0.8572,  0.5150]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2687],\n",
      "        [2.5299],\n",
      "        [7.4725]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 36, Loss: 1.593000536136784\n",
      "tensor([[ 0.6313, -0.7755],\n",
      "        [-0.9989,  0.0475],\n",
      "        [-0.8626,  0.5060]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2626],\n",
      "        [2.5218],\n",
      "        [7.4668]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 37, Loss: 1.6088375830584072\n",
      "tensor([[ 0.6386, -0.7696],\n",
      "        [-0.9983,  0.0584],\n",
      "        [-0.8678,  0.4970]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2566],\n",
      "        [2.5139],\n",
      "        [7.4613]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 38, Loss: 1.5779176099366083\n",
      "tensor([[ 0.6457, -0.7636],\n",
      "        [-0.9976,  0.0691],\n",
      "        [-0.8728,  0.4880]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2508],\n",
      "        [2.5059],\n",
      "        [7.4560]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 39, Loss: 1.6156039568144462\n",
      "tensor([[ 0.6526, -0.7577],\n",
      "        [-0.9968,  0.0798],\n",
      "        [-0.8778,  0.4791]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2451],\n",
      "        [2.4981],\n",
      "        [7.4508]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 40, Loss: 1.5472558337122027\n",
      "tensor([[ 0.6594, -0.7518],\n",
      "        [-0.9959,  0.0905],\n",
      "        [-0.8825,  0.4702]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2396],\n",
      "        [2.4903],\n",
      "        [7.4456]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 41, Loss: 1.5762707017380788\n",
      "tensor([[ 0.6661, -0.7459],\n",
      "        [-0.9949,  0.1010],\n",
      "        [-0.8872,  0.4614]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2343],\n",
      "        [2.4827],\n",
      "        [7.4407]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 42, Loss: 1.4578377243567235\n",
      "tensor([[ 0.6726, -0.7400],\n",
      "        [-0.9938,  0.1114],\n",
      "        [-0.8917,  0.4527]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2291],\n",
      "        [2.4751],\n",
      "        [7.4359]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 43, Loss: 1.455998121515828\n",
      "tensor([[ 0.6789, -0.7342],\n",
      "        [-0.9926,  0.1217],\n",
      "        [-0.8960,  0.4441]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2242],\n",
      "        [2.4676],\n",
      "        [7.4314]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 44, Loss: 1.4616498313903428\n",
      "tensor([[ 0.6851, -0.7285],\n",
      "        [-0.9912,  0.1320],\n",
      "        [-0.9002,  0.4356]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2194],\n",
      "        [2.4601],\n",
      "        [7.4270]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 45, Loss: 1.4308588134224698\n",
      "tensor([[ 0.6911, -0.7227],\n",
      "        [-0.9898,  0.1422],\n",
      "        [-0.9042,  0.4271]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2148],\n",
      "        [2.4528],\n",
      "        [7.4227]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 46, Loss: 1.4180828199683007\n",
      "tensor([[ 0.6970, -0.7171],\n",
      "        [-0.9883,  0.1523],\n",
      "        [-0.9081,  0.4187]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2104],\n",
      "        [2.4455],\n",
      "        [7.4186]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 47, Loss: 1.3702356394044042\n",
      "tensor([[ 0.7028, -0.7114],\n",
      "        [-0.9868,  0.1622],\n",
      "        [-0.9119,  0.4104]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2061],\n",
      "        [2.4384],\n",
      "        [7.4147]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 48, Loss: 1.3517172285528651\n",
      "tensor([[ 0.7085, -0.7058],\n",
      "        [-0.9851,  0.1720],\n",
      "        [-0.9155,  0.4022]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2020],\n",
      "        [2.4313],\n",
      "        [7.4110]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 49, Loss: 1.370032782636572\n",
      "tensor([[ 0.7140, -0.7001],\n",
      "        [-0.9833,  0.1818],\n",
      "        [-0.9191,  0.3941]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1979],\n",
      "        [2.4243],\n",
      "        [7.4074]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 50, Loss: 1.3476428716185176\n",
      "tensor([[ 0.7195, -0.6945],\n",
      "        [-0.9815,  0.1914],\n",
      "        [-0.9225,  0.3860]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1940],\n",
      "        [2.4175],\n",
      "        [7.4039]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 51, Loss: 1.3661051694842987\n",
      "tensor([[ 0.7248, -0.6889],\n",
      "        [-0.9796,  0.2010],\n",
      "        [-0.9258,  0.3780]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1901],\n",
      "        [2.4107],\n",
      "        [7.4005]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 52, Loss: 1.3261912498546722\n",
      "tensor([[ 0.7301, -0.6834],\n",
      "        [-0.9776,  0.2104],\n",
      "        [-0.9290,  0.3700]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1865],\n",
      "        [2.4040],\n",
      "        [7.3972]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 53, Loss: 1.2730556160002948\n",
      "tensor([[ 0.7352, -0.6779],\n",
      "        [-0.9756,  0.2197],\n",
      "        [-0.9321,  0.3621]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1830],\n",
      "        [2.3975],\n",
      "        [7.3940]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 54, Loss: 1.2871432287231344\n",
      "tensor([[ 0.7401, -0.6725],\n",
      "        [-0.9734,  0.2289],\n",
      "        [-0.9351,  0.3543]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1797],\n",
      "        [2.3910],\n",
      "        [7.3909]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 55, Loss: 1.2440221598732437\n",
      "tensor([[ 0.7450, -0.6671],\n",
      "        [-0.9713,  0.2380],\n",
      "        [-0.9380,  0.3466]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1765],\n",
      "        [2.3847],\n",
      "        [7.3880]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 56, Loss: 1.2580905754266194\n",
      "tensor([[ 0.7497, -0.6617],\n",
      "        [-0.9690,  0.2470],\n",
      "        [-0.9408,  0.3389]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1735],\n",
      "        [2.3784],\n",
      "        [7.3852]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 57, Loss: 1.244366125763479\n",
      "tensor([[ 0.7544, -0.6564],\n",
      "        [-0.9667,  0.2559],\n",
      "        [-0.9435,  0.3314]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1706],\n",
      "        [2.3722],\n",
      "        [7.3825]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 58, Loss: 1.2073658119661397\n",
      "tensor([[ 0.7589, -0.6512],\n",
      "        [-0.9643,  0.2647],\n",
      "        [-0.9461,  0.3239]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1679],\n",
      "        [2.3661],\n",
      "        [7.3800]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 59, Loss: 1.1929850240826467\n",
      "tensor([[ 0.7633, -0.6460],\n",
      "        [-0.9619,  0.2735],\n",
      "        [-0.9486,  0.3166]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1653],\n",
      "        [2.3601],\n",
      "        [7.3777]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 60, Loss: 1.195477701185339\n",
      "tensor([[ 0.7677, -0.6408],\n",
      "        [-0.9594,  0.2821],\n",
      "        [-0.9510,  0.3093]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1629],\n",
      "        [2.3542],\n",
      "        [7.3754]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 61, Loss: 1.1872461739699822\n",
      "tensor([[ 0.7719, -0.6358],\n",
      "        [-0.9568,  0.2906],\n",
      "        [-0.9533,  0.3021]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1606],\n",
      "        [2.3484],\n",
      "        [7.3733]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 62, Loss: 1.1508014795363084\n",
      "tensor([[ 0.7760, -0.6307],\n",
      "        [-0.9542,  0.2991],\n",
      "        [-0.9555,  0.2950]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1584],\n",
      "        [2.3427],\n",
      "        [7.3713]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 63, Loss: 1.1699913579039423\n",
      "tensor([[ 0.7801, -0.6257],\n",
      "        [-0.9516,  0.3074],\n",
      "        [-0.9576,  0.2880]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1563],\n",
      "        [2.3370],\n",
      "        [7.3695]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 64, Loss: 1.1405753691105616\n",
      "tensor([[ 0.7840, -0.6207],\n",
      "        [-0.9488,  0.3157],\n",
      "        [-0.9597,  0.2812]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1545],\n",
      "        [2.3313],\n",
      "        [7.3677]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 65, Loss: 1.1398333429744152\n",
      "tensor([[ 0.7878, -0.6159],\n",
      "        [-0.9461,  0.3239],\n",
      "        [-0.9616,  0.2744]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1527],\n",
      "        [2.3258],\n",
      "        [7.3661]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 66, Loss: 1.1237266653394276\n",
      "tensor([[ 0.7916, -0.6111],\n",
      "        [-0.9433,  0.3321],\n",
      "        [-0.9635,  0.2677]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1511],\n",
      "        [2.3203],\n",
      "        [7.3646]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 67, Loss: 1.119242200785629\n",
      "tensor([[ 0.7952, -0.6063],\n",
      "        [-0.9404,  0.3401],\n",
      "        [-0.9653,  0.2610]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1497],\n",
      "        [2.3150],\n",
      "        [7.3631]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 68, Loss: 1.0926717893399567\n",
      "tensor([[ 0.7988, -0.6016],\n",
      "        [-0.9375,  0.3480],\n",
      "        [-0.9671,  0.2544]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1482],\n",
      "        [2.3097],\n",
      "        [7.3618]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 69, Loss: 1.1106146285575493\n",
      "tensor([[ 0.8023, -0.5969],\n",
      "        [-0.9346,  0.3558],\n",
      "        [-0.9688,  0.2479]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1469],\n",
      "        [2.3045],\n",
      "        [7.3606]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 70, Loss: 1.0822681719656413\n",
      "tensor([[ 0.8058, -0.5922],\n",
      "        [-0.9316,  0.3636],\n",
      "        [-0.9704,  0.2415]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1457],\n",
      "        [2.2994],\n",
      "        [7.3594]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 71, Loss: 1.11105913161773\n",
      "tensor([[ 0.8092, -0.5876],\n",
      "        [-0.9285,  0.3713],\n",
      "        [-0.9720,  0.2351]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1445],\n",
      "        [2.2943],\n",
      "        [7.3583]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 72, Loss: 1.1063321267149404\n",
      "tensor([[ 0.8125, -0.5829],\n",
      "        [-0.9254,  0.3790],\n",
      "        [-0.9735,  0.2289]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1434],\n",
      "        [2.2893],\n",
      "        [7.3573]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 73, Loss: 1.0655872674771572\n",
      "tensor([[ 0.8158, -0.5783],\n",
      "        [-0.9223,  0.3865],\n",
      "        [-0.9749,  0.2227]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1424],\n",
      "        [2.2844],\n",
      "        [7.3564]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 74, Loss: 1.0615129297224084\n",
      "tensor([[ 0.8190, -0.5738],\n",
      "        [-0.9191,  0.3940],\n",
      "        [-0.9763,  0.2166]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1415],\n",
      "        [2.2796],\n",
      "        [7.3556]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 75, Loss: 1.0236652885888604\n",
      "tensor([[ 0.8222, -0.5692],\n",
      "        [-0.9159,  0.4013],\n",
      "        [-0.9776,  0.2106]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1407],\n",
      "        [2.2749],\n",
      "        [7.3549]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 76, Loss: 1.0360455765151642\n",
      "tensor([[ 0.8252, -0.5648],\n",
      "        [-0.9127,  0.4086],\n",
      "        [-0.9788,  0.2046]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1399],\n",
      "        [2.2703],\n",
      "        [7.3543]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 77, Loss: 1.0295540648601764\n",
      "tensor([[ 0.8282, -0.5604],\n",
      "        [-0.9095,  0.4158],\n",
      "        [-0.9800,  0.1988]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1394],\n",
      "        [2.2657],\n",
      "        [7.3538]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 78, Loss: 1.0208512010970936\n",
      "tensor([[ 0.8312, -0.5560],\n",
      "        [-0.9062,  0.4229],\n",
      "        [-0.9812,  0.1930]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1389],\n",
      "        [2.2612],\n",
      "        [7.3534]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 79, Loss: 1.003955150034077\n",
      "tensor([[ 0.8340, -0.5518],\n",
      "        [-0.9029,  0.4299],\n",
      "        [-0.9823,  0.1874]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1385],\n",
      "        [2.2568],\n",
      "        [7.3531]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 80, Loss: 0.9905001820016144\n",
      "tensor([[ 0.8368, -0.5475],\n",
      "        [-0.8995,  0.4368],\n",
      "        [-0.9833,  0.1818]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1382],\n",
      "        [2.2525],\n",
      "        [7.3528]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 81, Loss: 1.02766100366064\n",
      "tensor([[ 0.8395, -0.5433],\n",
      "        [-0.8962,  0.4437],\n",
      "        [-0.9843,  0.1764]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1380],\n",
      "        [2.2482],\n",
      "        [7.3526]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 82, Loss: 0.9983126231041339\n",
      "tensor([[ 0.8422, -0.5391],\n",
      "        [-0.8928,  0.4505],\n",
      "        [-0.9853,  0.1710]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1379],\n",
      "        [2.2440],\n",
      "        [7.3525]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 83, Loss: 0.9882972402183176\n",
      "tensor([[ 0.8449, -0.5350],\n",
      "        [-0.8894,  0.4572],\n",
      "        [-0.9862,  0.1656]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1378],\n",
      "        [2.2399],\n",
      "        [7.3525]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 84, Loss: 0.9754089288226574\n",
      "tensor([[ 0.8474, -0.5309],\n",
      "        [-0.8859,  0.4639],\n",
      "        [-0.9871,  0.1603]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1378],\n",
      "        [2.2359],\n",
      "        [7.3525]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 85, Loss: 0.9581295815261376\n",
      "tensor([[ 0.8499, -0.5269],\n",
      "        [-0.8825,  0.4704],\n",
      "        [-0.9879,  0.1551]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1379],\n",
      "        [2.2320],\n",
      "        [7.3527]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 86, Loss: 0.94370365203189\n",
      "tensor([[ 0.8524, -0.5229],\n",
      "        [-0.8790,  0.4768],\n",
      "        [-0.9887,  0.1500]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1381],\n",
      "        [2.2282],\n",
      "        [7.3530]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 87, Loss: 0.9486020208692936\n",
      "tensor([[ 0.8549, -0.5189],\n",
      "        [-0.8755,  0.4832],\n",
      "        [-0.9894,  0.1450]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1383],\n",
      "        [2.2244],\n",
      "        [7.3534]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 88, Loss: 0.9508401085678123\n",
      "tensor([[ 0.8573, -0.5149],\n",
      "        [-0.8720,  0.4895],\n",
      "        [-0.9901,  0.1401]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1386],\n",
      "        [2.2208],\n",
      "        [7.3538]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 89, Loss: 0.9458585222272112\n",
      "tensor([[ 0.8596, -0.5109],\n",
      "        [-0.8685,  0.4958],\n",
      "        [-0.9908,  0.1352]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1389],\n",
      "        [2.2172],\n",
      "        [7.3543]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 90, Loss: 0.9445624755761052\n",
      "tensor([[ 0.8620, -0.5070],\n",
      "        [-0.8649,  0.5019],\n",
      "        [-0.9915,  0.1304]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1393],\n",
      "        [2.2137],\n",
      "        [7.3549]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 91, Loss: 0.9472276133655624\n",
      "tensor([[ 0.8643, -0.5031],\n",
      "        [-0.8613,  0.5080],\n",
      "        [-0.9921,  0.1257]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1397],\n",
      "        [2.2102],\n",
      "        [7.3554]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 92, Loss: 0.9417098687234191\n",
      "tensor([[ 0.8665, -0.4992],\n",
      "        [-0.8578,  0.5140],\n",
      "        [-0.9926,  0.1211]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1402],\n",
      "        [2.2068],\n",
      "        [7.3561]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 93, Loss: 0.9268313686198099\n",
      "tensor([[ 0.8687, -0.4954],\n",
      "        [-0.8542,  0.5200],\n",
      "        [-0.9932,  0.1165]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1408],\n",
      "        [2.2035],\n",
      "        [7.3567]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 94, Loss: 0.9342398665911872\n",
      "tensor([[ 0.8708, -0.4916],\n",
      "        [-0.8505,  0.5259],\n",
      "        [-0.9937,  0.1119]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1414],\n",
      "        [2.2003],\n",
      "        [7.3574]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 95, Loss: 0.9139527053539244\n",
      "tensor([[ 0.8729, -0.4879],\n",
      "        [-0.8469,  0.5318],\n",
      "        [-0.9942,  0.1074]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1421],\n",
      "        [2.1970],\n",
      "        [7.3581]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 96, Loss: 0.9245549283798447\n",
      "tensor([[ 0.8749, -0.4843],\n",
      "        [-0.8432,  0.5376],\n",
      "        [-0.9947,  0.1030]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1430],\n",
      "        [2.1939],\n",
      "        [7.3589]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 97, Loss: 0.9179872998066855\n",
      "tensor([[ 0.8769, -0.4806],\n",
      "        [-0.8395,  0.5434],\n",
      "        [-0.9951,  0.0985]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1438],\n",
      "        [2.1908],\n",
      "        [7.3597]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 98, Loss: 0.9166451110652688\n",
      "tensor([[ 0.8789, -0.4770],\n",
      "        [-0.8358,  0.5491],\n",
      "        [-0.9955,  0.0943]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1447],\n",
      "        [2.1878],\n",
      "        [7.3606]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 99, Loss: 0.8852797794033173\n",
      "tensor([[ 0.8808, -0.4734],\n",
      "        [-0.8320,  0.5547],\n",
      "        [-0.9959,  0.0900]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1457],\n",
      "        [2.1848],\n",
      "        [7.3615]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 100, Loss: 0.9103622987858656\n",
      "tensor([[ 0.8827, -0.4699],\n",
      "        [-0.8283,  0.5603],\n",
      "        [-0.9963,  0.0858]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1468],\n",
      "        [2.1819],\n",
      "        [7.3625]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 101, Loss: 0.8873342025796918\n",
      "tensor([[ 0.8845, -0.4665],\n",
      "        [-0.8245,  0.5658],\n",
      "        [-0.9967,  0.0817]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1478],\n",
      "        [2.1791],\n",
      "        [7.3636]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 102, Loss: 0.8896773752814036\n",
      "tensor([[ 0.8864, -0.4630],\n",
      "        [-0.8208,  0.5713],\n",
      "        [-0.9970,  0.0777]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1489],\n",
      "        [2.1763],\n",
      "        [7.3646]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 103, Loss: 0.894132175576473\n",
      "tensor([[ 0.8882, -0.4595],\n",
      "        [-0.8170,  0.5766],\n",
      "        [-0.9973,  0.0737]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1501],\n",
      "        [2.1736],\n",
      "        [7.3657]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 104, Loss: 0.8873660316785084\n",
      "tensor([[ 0.8899, -0.4561],\n",
      "        [-0.8133,  0.5819],\n",
      "        [-0.9976,  0.0697]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1513],\n",
      "        [2.1711],\n",
      "        [7.3668]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 105, Loss: 0.8626422308870408\n",
      "tensor([[ 0.8916, -0.4528],\n",
      "        [-0.8095,  0.5871],\n",
      "        [-0.9978,  0.0659]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1526],\n",
      "        [2.1685],\n",
      "        [7.3680]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 106, Loss: 0.8635757092944595\n",
      "tensor([[ 0.8933, -0.4494],\n",
      "        [-0.8057,  0.5923],\n",
      "        [-0.9981,  0.0621]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1539],\n",
      "        [2.1661],\n",
      "        [7.3693]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 107, Loss: 0.852369996235691\n",
      "tensor([[ 0.8950, -0.4461],\n",
      "        [-0.8019,  0.5974],\n",
      "        [-0.9983,  0.0584]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1553],\n",
      "        [2.1637],\n",
      "        [7.3705]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 108, Loss: 0.8850764365745933\n",
      "tensor([[ 0.8966, -0.4429],\n",
      "        [-0.7981,  0.6025],\n",
      "        [-0.9985,  0.0547]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1567],\n",
      "        [2.1613],\n",
      "        [7.3718]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 109, Loss: 0.861983311879571\n",
      "tensor([[ 0.8982, -0.4397],\n",
      "        [-0.7942,  0.6076],\n",
      "        [-0.9987,  0.0511]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1582],\n",
      "        [2.1590],\n",
      "        [7.3732]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 110, Loss: 0.8719007956253353\n",
      "tensor([[ 0.8997, -0.4365],\n",
      "        [-0.7904,  0.6126],\n",
      "        [-0.9989,  0.0475]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1598],\n",
      "        [2.1567],\n",
      "        [7.3746]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 111, Loss: 0.8470125991604774\n",
      "tensor([[ 0.9012, -0.4333],\n",
      "        [-0.7865,  0.6176],\n",
      "        [-0.9990,  0.0441]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1613],\n",
      "        [2.1544],\n",
      "        [7.3761]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 112, Loss: 0.8621734338040867\n",
      "tensor([[ 0.9027, -0.4303],\n",
      "        [-0.7826,  0.6225],\n",
      "        [-0.9992,  0.0407]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1630],\n",
      "        [2.1523],\n",
      "        [7.3776]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 113, Loss: 0.8380053139027634\n",
      "tensor([[ 0.9041, -0.4273],\n",
      "        [-0.7787,  0.6274],\n",
      "        [-0.9993,  0.0374]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1647],\n",
      "        [2.1501],\n",
      "        [7.3792]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 114, Loss: 0.8526692405512504\n",
      "tensor([[ 0.9055, -0.4244],\n",
      "        [-0.7748,  0.6322],\n",
      "        [-0.9994,  0.0342]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1664],\n",
      "        [2.1480],\n",
      "        [7.3808]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 115, Loss: 0.8324732213148397\n",
      "tensor([[ 0.9068, -0.4215],\n",
      "        [-0.7709,  0.6370],\n",
      "        [-0.9995,  0.0311]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1683],\n",
      "        [2.1460],\n",
      "        [7.3825]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 116, Loss: 0.819941019012734\n",
      "tensor([[ 0.9082, -0.4186],\n",
      "        [-0.7670,  0.6417],\n",
      "        [-0.9996,  0.0280]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1701],\n",
      "        [2.1441],\n",
      "        [7.3842]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 117, Loss: 0.8407417391050451\n",
      "tensor([[ 0.9094, -0.4158],\n",
      "        [-0.7631,  0.6463],\n",
      "        [-0.9997,  0.0250]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1720],\n",
      "        [2.1422],\n",
      "        [7.3860]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 118, Loss: 0.8348228709391448\n",
      "tensor([[ 0.9107, -0.4130],\n",
      "        [-0.7591,  0.6510],\n",
      "        [-0.9998,  0.0220]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1740],\n",
      "        [2.1403],\n",
      "        [7.3878]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 119, Loss: 0.8452789173273745\n",
      "tensor([[ 0.9120, -0.4102],\n",
      "        [-0.7552,  0.6555],\n",
      "        [-0.9998,  0.0190]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1759],\n",
      "        [2.1385],\n",
      "        [7.3896]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 120, Loss: 0.8305190290950256\n",
      "tensor([[ 0.9132, -0.4075],\n",
      "        [-0.7512,  0.6601],\n",
      "        [-0.9999,  0.0161]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1780],\n",
      "        [2.1367],\n",
      "        [7.3914]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 121, Loss: 0.8185630799464276\n",
      "tensor([[ 0.9144, -0.4047],\n",
      "        [-0.7472,  0.6646],\n",
      "        [-0.9999,  0.0132]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1800],\n",
      "        [2.1350],\n",
      "        [7.3933]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 122, Loss: 0.8371470874780238\n",
      "tensor([[ 0.9157, -0.4019],\n",
      "        [-0.7432,  0.6690],\n",
      "        [-0.9999,  0.0104]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1821],\n",
      "        [2.1333],\n",
      "        [7.3952]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 123, Loss: 0.8186776308636936\n",
      "tensor([[ 0.9169, -0.3991],\n",
      "        [-0.7393,  0.6734],\n",
      "        [-1.0000,  0.0076]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1841],\n",
      "        [2.1318],\n",
      "        [7.3970]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 124, Loss: 0.8276149183844901\n",
      "tensor([[ 0.9181, -0.3964],\n",
      "        [-0.7353,  0.6777],\n",
      "        [-1.0000,  0.0049]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1862],\n",
      "        [2.1302],\n",
      "        [7.3989]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 125, Loss: 0.8367124885808648\n",
      "tensor([[ 0.9193, -0.3936],\n",
      "        [-0.7313,  0.6820],\n",
      "        [-1.0000,  0.0021]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1883],\n",
      "        [2.1287],\n",
      "        [7.4008]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 126, Loss: 0.8316603558465733\n",
      "tensor([[ 9.2046e-01, -3.9084e-01],\n",
      "        [-7.2733e-01,  6.8629e-01],\n",
      "        [-1.0000e+00, -5.7508e-04]], dtype=torch.float64,\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1904],\n",
      "        [2.1273],\n",
      "        [7.4027]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 127, Loss: 0.8039151226279548\n",
      "tensor([[ 0.9216, -0.3881],\n",
      "        [-0.7233,  0.6905],\n",
      "        [-1.0000, -0.0033]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1926],\n",
      "        [2.1260],\n",
      "        [7.4046]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 128, Loss: 0.8073180051547609\n",
      "tensor([[ 0.9227, -0.3854],\n",
      "        [-0.7193,  0.6947],\n",
      "        [-1.0000, -0.0059]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1948],\n",
      "        [2.1246],\n",
      "        [7.4065]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 129, Loss: 0.8188610502928494\n",
      "tensor([[ 0.9238, -0.3828],\n",
      "        [-0.7153,  0.6988],\n",
      "        [-1.0000, -0.0086]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1971],\n",
      "        [2.1234],\n",
      "        [7.4085]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 130, Loss: 0.8078923360798846\n",
      "tensor([[ 0.9249, -0.3802],\n",
      "        [-0.7113,  0.7029],\n",
      "        [-0.9999, -0.0112]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1994],\n",
      "        [2.1221],\n",
      "        [7.4105]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 131, Loss: 0.7883793804073772\n",
      "tensor([[ 0.9260, -0.3776],\n",
      "        [-0.7073,  0.7070],\n",
      "        [-0.9999, -0.0138]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2018],\n",
      "        [2.1210],\n",
      "        [7.4125]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 132, Loss: 0.8123492913834379\n",
      "tensor([[ 0.9270, -0.3750],\n",
      "        [-0.7032,  0.7110],\n",
      "        [-0.9999, -0.0163]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2041],\n",
      "        [2.1198],\n",
      "        [7.4145]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 133, Loss: 0.811103318545442\n",
      "tensor([[ 0.9281, -0.3724],\n",
      "        [-0.6992,  0.7149],\n",
      "        [-0.9998, -0.0187]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2065],\n",
      "        [2.1188],\n",
      "        [7.4166]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 134, Loss: 0.8023480768796617\n",
      "tensor([[ 0.9291, -0.3699],\n",
      "        [-0.6952,  0.7189],\n",
      "        [-0.9998, -0.0211]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2089],\n",
      "        [2.1177],\n",
      "        [7.4187]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 135, Loss: 0.7888194815561506\n",
      "tensor([[ 0.9301, -0.3673],\n",
      "        [-0.6911,  0.7227],\n",
      "        [-0.9997, -0.0234]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2114],\n",
      "        [2.1168],\n",
      "        [7.4208]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 136, Loss: 0.788427164460659\n",
      "tensor([[ 0.9311, -0.3648],\n",
      "        [-0.6871,  0.7266],\n",
      "        [-0.9997, -0.0257]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2138],\n",
      "        [2.1159],\n",
      "        [7.4230]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 137, Loss: 0.7967601976346914\n",
      "tensor([[ 0.9321, -0.3623],\n",
      "        [-0.6831,  0.7304],\n",
      "        [-0.9996, -0.0280]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2163],\n",
      "        [2.1150],\n",
      "        [7.4252]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 138, Loss: 0.789654355164242\n",
      "tensor([[ 0.9330, -0.3598],\n",
      "        [-0.6790,  0.7341],\n",
      "        [-0.9995, -0.0302]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2189],\n",
      "        [2.1142],\n",
      "        [7.4274]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 139, Loss: 0.7917133665945728\n",
      "tensor([[ 0.9340, -0.3573],\n",
      "        [-0.6750,  0.7379],\n",
      "        [-0.9995, -0.0323]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2214],\n",
      "        [2.1134],\n",
      "        [7.4296]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 140, Loss: 0.7999312107153225\n",
      "tensor([[ 0.9349, -0.3548],\n",
      "        [-0.6709,  0.7415],\n",
      "        [-0.9994, -0.0345]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2240],\n",
      "        [2.1127],\n",
      "        [7.4318]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 141, Loss: 0.7949953604051271\n",
      "tensor([[ 0.9359, -0.3523],\n",
      "        [-0.6668,  0.7452],\n",
      "        [-0.9993, -0.0367]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2266],\n",
      "        [2.1120],\n",
      "        [7.4340]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 142, Loss: 0.7845287983987214\n",
      "tensor([[ 0.9368, -0.3499],\n",
      "        [-0.6627,  0.7488],\n",
      "        [-0.9992, -0.0388]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2292],\n",
      "        [2.1113],\n",
      "        [7.4363]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 143, Loss: 0.7881201504521687\n",
      "tensor([[ 0.9376, -0.3476],\n",
      "        [-0.6587,  0.7524],\n",
      "        [-0.9992, -0.0409]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2319],\n",
      "        [2.1107],\n",
      "        [7.4385]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 144, Loss: 0.7890729846104243\n",
      "tensor([[ 0.9385, -0.3453],\n",
      "        [-0.6545,  0.7560],\n",
      "        [-0.9991, -0.0429]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2346],\n",
      "        [2.1101],\n",
      "        [7.4407]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 145, Loss: 0.7822111062496662\n",
      "tensor([[ 0.9393, -0.3430],\n",
      "        [-0.6505,  0.7595],\n",
      "        [-0.9990, -0.0449]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2374],\n",
      "        [2.1096],\n",
      "        [7.4429]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 146, Loss: 0.7895486106611731\n",
      "tensor([[ 0.9401, -0.3408],\n",
      "        [-0.6464,  0.7630],\n",
      "        [-0.9989, -0.0468]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2401],\n",
      "        [2.1091],\n",
      "        [7.4451]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 147, Loss: 0.7726032158567896\n",
      "tensor([[ 0.9409, -0.3386],\n",
      "        [-0.6423,  0.7665],\n",
      "        [-0.9988, -0.0487]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2429],\n",
      "        [2.1087],\n",
      "        [7.4472]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 148, Loss: 0.788776019053541\n",
      "tensor([[ 0.9417, -0.3364],\n",
      "        [-0.6381,  0.7699],\n",
      "        [-0.9987, -0.0505]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2458],\n",
      "        [2.1083],\n",
      "        [7.4494]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 149, Loss: 0.7933396421086619\n",
      "tensor([[ 0.9425, -0.3343],\n",
      "        [-0.6340,  0.7733],\n",
      "        [-0.9986, -0.0522]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2487],\n",
      "        [2.1079],\n",
      "        [7.4515]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 150, Loss: 0.7836661743458441\n",
      "tensor([[ 0.9432, -0.3322],\n",
      "        [-0.6299,  0.7767],\n",
      "        [-0.9985, -0.0539]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2517],\n",
      "        [2.1075],\n",
      "        [7.4536]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 151, Loss: 0.778426862736589\n",
      "tensor([[ 0.9440, -0.3301],\n",
      "        [-0.6257,  0.7800],\n",
      "        [-0.9985, -0.0556]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2546],\n",
      "        [2.1072],\n",
      "        [7.4557]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 152, Loss: 0.7806974245286307\n",
      "tensor([[ 0.9447, -0.3280],\n",
      "        [-0.6216,  0.7833],\n",
      "        [-0.9984, -0.0573]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2576],\n",
      "        [2.1069],\n",
      "        [7.4579]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 153, Loss: 0.7719184645682402\n",
      "tensor([[ 0.9454, -0.3260],\n",
      "        [-0.6175,  0.7866],\n",
      "        [-0.9983, -0.0589]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2606],\n",
      "        [2.1067],\n",
      "        [7.4601]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 154, Loss: 0.756044693117606\n",
      "tensor([[ 0.9461, -0.3240],\n",
      "        [-0.6134,  0.7898],\n",
      "        [-0.9982, -0.0606]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2636],\n",
      "        [2.1066],\n",
      "        [7.4623]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 155, Loss: 0.7579142955417901\n",
      "tensor([[ 0.9467, -0.3221],\n",
      "        [-0.6093,  0.7930],\n",
      "        [-0.9981, -0.0622]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2667],\n",
      "        [2.1065],\n",
      "        [7.4645]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 156, Loss: 0.7668827906467863\n",
      "tensor([[ 0.9474, -0.3201],\n",
      "        [-0.6052,  0.7961],\n",
      "        [-0.9980, -0.0637]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2697],\n",
      "        [2.1065],\n",
      "        [7.4668]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 157, Loss: 0.7597599208346726\n",
      "tensor([[ 0.9480, -0.3183],\n",
      "        [-0.6011,  0.7992],\n",
      "        [-0.9979, -0.0652]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2729],\n",
      "        [2.1065],\n",
      "        [7.4690]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 158, Loss: 0.7652201784323253\n",
      "tensor([[ 0.9486, -0.3164],\n",
      "        [-0.5970,  0.8023],\n",
      "        [-0.9978, -0.0666]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2760],\n",
      "        [2.1065],\n",
      "        [7.4712]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 159, Loss: 0.7741068064525185\n",
      "tensor([[ 0.9493, -0.3145],\n",
      "        [-0.5929,  0.8053],\n",
      "        [-0.9977, -0.0681]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2791],\n",
      "        [2.1066],\n",
      "        [7.4735]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 160, Loss: 0.7542861210240889\n",
      "tensor([[ 0.9499, -0.3126],\n",
      "        [-0.5888,  0.8083],\n",
      "        [-0.9976, -0.0696]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2823],\n",
      "        [2.1067],\n",
      "        [7.4758]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 161, Loss: 0.7492210827862811\n",
      "tensor([[ 0.9505, -0.3107],\n",
      "        [-0.5847,  0.8113],\n",
      "        [-0.9975, -0.0710]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2854],\n",
      "        [2.1069],\n",
      "        [7.4782]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 162, Loss: 0.755539463948048\n",
      "tensor([[ 0.9511, -0.3089],\n",
      "        [-0.5806,  0.8142],\n",
      "        [-0.9974, -0.0725]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2887],\n",
      "        [2.1071],\n",
      "        [7.4806]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 163, Loss: 0.749427499186658\n",
      "tensor([[ 0.9516, -0.3072],\n",
      "        [-0.5764,  0.8171],\n",
      "        [-0.9973, -0.0739]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2920],\n",
      "        [2.1074],\n",
      "        [7.4830]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 164, Loss: 0.764411379324504\n",
      "tensor([[ 0.9522, -0.3055],\n",
      "        [-0.5723,  0.8201],\n",
      "        [-0.9972, -0.0753]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2953],\n",
      "        [2.1076],\n",
      "        [7.4854]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 165, Loss: 0.7493098162807513\n",
      "tensor([[ 0.9527, -0.3039],\n",
      "        [-0.5682,  0.8229],\n",
      "        [-0.9971, -0.0766]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2986],\n",
      "        [2.1079],\n",
      "        [7.4879]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 166, Loss: 0.7403886802055231\n",
      "tensor([[ 0.9532, -0.3022],\n",
      "        [-0.5641,  0.8257],\n",
      "        [-0.9970, -0.0780]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3020],\n",
      "        [2.1083],\n",
      "        [7.4903]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 167, Loss: 0.7431339725506954\n",
      "tensor([[ 0.9537, -0.3007],\n",
      "        [-0.5599,  0.8285],\n",
      "        [-0.9969, -0.0793]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3054],\n",
      "        [2.1087],\n",
      "        [7.4928]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 168, Loss: 0.7528492379319772\n",
      "tensor([[ 0.9542, -0.2990],\n",
      "        [-0.5559,  0.8313],\n",
      "        [-0.9967, -0.0806]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3088],\n",
      "        [2.1092],\n",
      "        [7.4952]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 169, Loss: 0.7478000088779753\n",
      "tensor([[ 0.9548, -0.2974],\n",
      "        [-0.5518,  0.8340],\n",
      "        [-0.9966, -0.0819]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3122],\n",
      "        [2.1097],\n",
      "        [7.4976]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 170, Loss: 0.7535852685406014\n",
      "tensor([[ 0.9553, -0.2958],\n",
      "        [-0.5477,  0.8367],\n",
      "        [-0.9965, -0.0831]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3157],\n",
      "        [2.1103],\n",
      "        [7.5001]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 171, Loss: 0.7347376884778878\n",
      "tensor([[ 0.9558, -0.2942],\n",
      "        [-0.5436,  0.8393],\n",
      "        [-0.9964, -0.0842]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3191],\n",
      "        [2.1109],\n",
      "        [7.5025]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 172, Loss: 0.7411910132799205\n",
      "tensor([[ 0.9562, -0.2926],\n",
      "        [-0.5396,  0.8419],\n",
      "        [-0.9964, -0.0853]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3225],\n",
      "        [2.1116],\n",
      "        [7.5050]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 173, Loss: 0.7407026584574988\n",
      "tensor([[ 0.9567, -0.2910],\n",
      "        [-0.5356,  0.8445],\n",
      "        [-0.9963, -0.0865]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3260],\n",
      "        [2.1123],\n",
      "        [7.5075]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 174, Loss: 0.7366766176934532\n",
      "tensor([[ 0.9572, -0.2895],\n",
      "        [-0.5315,  0.8470],\n",
      "        [-0.9962, -0.0876]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3295],\n",
      "        [2.1130],\n",
      "        [7.5100]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 175, Loss: 0.7354959104675799\n",
      "tensor([[ 0.9576, -0.2880],\n",
      "        [-0.5275,  0.8496],\n",
      "        [-0.9961, -0.0887]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3331],\n",
      "        [2.1138],\n",
      "        [7.5125]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 176, Loss: 0.7474157613370881\n",
      "tensor([[ 0.9581, -0.2866],\n",
      "        [-0.5235,  0.8521],\n",
      "        [-0.9960, -0.0897]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3366],\n",
      "        [2.1146],\n",
      "        [7.5149]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 177, Loss: 0.7528970134600828\n",
      "tensor([[ 0.9585, -0.2851],\n",
      "        [-0.5194,  0.8545],\n",
      "        [-0.9959, -0.0908]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3402],\n",
      "        [2.1155],\n",
      "        [7.5173]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 178, Loss: 0.7321027356828335\n",
      "tensor([[ 0.9589, -0.2836],\n",
      "        [-0.5154,  0.8569],\n",
      "        [-0.9958, -0.0919]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3437],\n",
      "        [2.1164],\n",
      "        [7.5197]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 179, Loss: 0.7352921395203443\n",
      "tensor([[ 0.9594, -0.2822],\n",
      "        [-0.5114,  0.8593],\n",
      "        [-0.9957, -0.0929]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3473],\n",
      "        [2.1173],\n",
      "        [7.5222]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 180, Loss: 0.743626271789504\n",
      "tensor([[ 0.9598, -0.2807],\n",
      "        [-0.5074,  0.8617],\n",
      "        [-0.9956, -0.0939]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3508],\n",
      "        [2.1183],\n",
      "        [7.5246]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 181, Loss: 0.7405994916480907\n",
      "tensor([[ 0.9602, -0.2793],\n",
      "        [-0.5034,  0.8641],\n",
      "        [-0.9955, -0.0948]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3544],\n",
      "        [2.1193],\n",
      "        [7.5270]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 182, Loss: 0.7335169199067708\n",
      "tensor([[ 0.9606, -0.2780],\n",
      "        [-0.4994,  0.8664],\n",
      "        [-0.9954, -0.0956]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3580],\n",
      "        [2.1204],\n",
      "        [7.5295]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 183, Loss: 0.7210380786639699\n",
      "tensor([[ 0.9610, -0.2766],\n",
      "        [-0.4955,  0.8686],\n",
      "        [-0.9953, -0.0964]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3616],\n",
      "        [2.1215],\n",
      "        [7.5319]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 184, Loss: 0.725418563916906\n",
      "tensor([[ 0.9613, -0.2754],\n",
      "        [-0.4915,  0.8709],\n",
      "        [-0.9953, -0.0972]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3652],\n",
      "        [2.1227],\n",
      "        [7.5344]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 185, Loss: 0.7281136230627901\n",
      "tensor([[ 0.9617, -0.2742],\n",
      "        [-0.4876,  0.8731],\n",
      "        [-0.9952, -0.0979]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3688],\n",
      "        [2.1239],\n",
      "        [7.5369]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 186, Loss: 0.7367781070054769\n",
      "tensor([[ 0.9620, -0.2730],\n",
      "        [-0.4837,  0.8752],\n",
      "        [-0.9951, -0.0986]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3724],\n",
      "        [2.1251],\n",
      "        [7.5393]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 187, Loss: 0.7383265583337455\n",
      "tensor([[ 0.9624, -0.2718],\n",
      "        [-0.4798,  0.8774],\n",
      "        [-0.9951, -0.0993]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3761],\n",
      "        [2.1264],\n",
      "        [7.5417]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 188, Loss: 0.7375129419769483\n",
      "tensor([[ 0.9627, -0.2706],\n",
      "        [-0.4759,  0.8795],\n",
      "        [-0.9950, -0.1000]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3798],\n",
      "        [2.1277],\n",
      "        [7.5442]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 189, Loss: 0.7184191000744365\n",
      "tensor([[ 0.9630, -0.2695],\n",
      "        [-0.4719,  0.8816],\n",
      "        [-0.9949, -0.1007]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3835],\n",
      "        [2.1290],\n",
      "        [7.5467]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 190, Loss: 0.721247970728645\n",
      "tensor([[ 0.9633, -0.2683],\n",
      "        [-0.4681,  0.8837],\n",
      "        [-0.9948, -0.1014]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3872],\n",
      "        [2.1304],\n",
      "        [7.5492]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 191, Loss: 0.7248159937282601\n",
      "tensor([[ 0.9637, -0.2671],\n",
      "        [-0.4642,  0.8857],\n",
      "        [-0.9948, -0.1021]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3909],\n",
      "        [2.1318],\n",
      "        [7.5517]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 192, Loss: 0.7192115829138613\n",
      "tensor([[ 0.9640, -0.2660],\n",
      "        [-0.4603,  0.8878],\n",
      "        [-0.9947, -0.1028]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3947],\n",
      "        [2.1333],\n",
      "        [7.5542]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 193, Loss: 0.7253023472583777\n",
      "tensor([[ 0.9643, -0.2648],\n",
      "        [-0.4564,  0.8898],\n",
      "        [-0.9946, -0.1034]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3985],\n",
      "        [2.1348],\n",
      "        [7.5567]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 194, Loss: 0.7172910052849382\n",
      "tensor([[ 0.9646, -0.2638],\n",
      "        [-0.4526,  0.8917],\n",
      "        [-0.9946, -0.1040]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4022],\n",
      "        [2.1363],\n",
      "        [7.5592]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 195, Loss: 0.7217279766392825\n",
      "tensor([[ 0.9649, -0.2627],\n",
      "        [-0.4488,  0.8937],\n",
      "        [-0.9945, -0.1046]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4060],\n",
      "        [2.1379],\n",
      "        [7.5617]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 196, Loss: 0.7272734504699585\n",
      "tensor([[ 0.9652, -0.2615],\n",
      "        [-0.4449,  0.8956],\n",
      "        [-0.9945, -0.1051]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4098],\n",
      "        [2.1395],\n",
      "        [7.5642]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 197, Loss: 0.7239029274710694\n",
      "tensor([[ 0.9655, -0.2604],\n",
      "        [-0.4411,  0.8975],\n",
      "        [-0.9944, -0.1057]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4135],\n",
      "        [2.1411],\n",
      "        [7.5667]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 198, Loss: 0.7192900026395371\n",
      "tensor([[ 0.9658, -0.2592],\n",
      "        [-0.4373,  0.8993],\n",
      "        [-0.9943, -0.1063]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4173],\n",
      "        [2.1428],\n",
      "        [7.5693]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 199, Loss: 0.7081637797168092\n",
      "tensor([[ 0.9661, -0.2580],\n",
      "        [-0.4335,  0.9011],\n",
      "        [-0.9943, -0.1068]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4211],\n",
      "        [2.1446],\n",
      "        [7.5718]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 200, Loss: 0.7131439295686111\n",
      "tensor([[ 0.9665, -0.2568],\n",
      "        [-0.4297,  0.9030],\n",
      "        [-0.9942, -0.1073]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4250],\n",
      "        [2.1464],\n",
      "        [7.5743]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 201, Loss: 0.7173038701631583\n",
      "tensor([[ 0.9668, -0.2557],\n",
      "        [-0.4260,  0.9047],\n",
      "        [-0.9942, -0.1077]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4288],\n",
      "        [2.1481],\n",
      "        [7.5769]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 202, Loss: 0.712002992039969\n",
      "tensor([[ 0.9671, -0.2545],\n",
      "        [-0.4222,  0.9065],\n",
      "        [-0.9941, -0.1081]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4327],\n",
      "        [2.1500],\n",
      "        [7.5795]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 203, Loss: 0.7029742863856606\n",
      "tensor([[ 0.9674, -0.2533],\n",
      "        [-0.4185,  0.9082],\n",
      "        [-0.9941, -0.1085]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4365],\n",
      "        [2.1519],\n",
      "        [7.5821]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 204, Loss: 0.7150210131909164\n",
      "tensor([[ 0.9677, -0.2522],\n",
      "        [-0.4147,  0.9099],\n",
      "        [-0.9941, -0.1088]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4404],\n",
      "        [2.1538],\n",
      "        [7.5847]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 205, Loss: 0.7164068587946888\n",
      "tensor([[ 0.9680, -0.2510],\n",
      "        [-0.4110,  0.9116],\n",
      "        [-0.9940, -0.1092]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4442],\n",
      "        [2.1557],\n",
      "        [7.5873]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 206, Loss: 0.7189633636325037\n",
      "tensor([[ 0.9683, -0.2499],\n",
      "        [-0.4073,  0.9133],\n",
      "        [-0.9940, -0.1096]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4480],\n",
      "        [2.1577],\n",
      "        [7.5899]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 207, Loss: 0.7131910975337705\n",
      "tensor([[ 0.9685, -0.2488],\n",
      "        [-0.4036,  0.9149],\n",
      "        [-0.9939, -0.1101]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4519],\n",
      "        [2.1597],\n",
      "        [7.5925]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 208, Loss: 0.703925844500444\n",
      "tensor([[ 0.9688, -0.2478],\n",
      "        [-0.3999,  0.9165],\n",
      "        [-0.9939, -0.1105]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4558],\n",
      "        [2.1617],\n",
      "        [7.5951]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 209, Loss: 0.7093520758725367\n",
      "tensor([[ 0.9691, -0.2468],\n",
      "        [-0.3962,  0.9181],\n",
      "        [-0.9938, -0.1110]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4597],\n",
      "        [2.1638],\n",
      "        [7.5978]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 210, Loss: 0.6978746120299743\n",
      "tensor([[ 0.9693, -0.2458],\n",
      "        [-0.3926,  0.9197],\n",
      "        [-0.9938, -0.1115]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4636],\n",
      "        [2.1659],\n",
      "        [7.6004]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 211, Loss: 0.7045220038862932\n",
      "tensor([[ 0.9696, -0.2448],\n",
      "        [-0.3889,  0.9213],\n",
      "        [-0.9937, -0.1119]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4675],\n",
      "        [2.1680],\n",
      "        [7.6030]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 212, Loss: 0.7126708161461591\n",
      "tensor([[ 0.9698, -0.2438],\n",
      "        [-0.3853,  0.9228],\n",
      "        [-0.9937, -0.1122]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4714],\n",
      "        [2.1701],\n",
      "        [7.6057]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 213, Loss: 0.700437275726124\n",
      "tensor([[ 0.9701, -0.2428],\n",
      "        [-0.3816,  0.9243],\n",
      "        [-0.9936, -0.1126]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4754],\n",
      "        [2.1723],\n",
      "        [7.6083]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 214, Loss: 0.7093929412590769\n",
      "tensor([[ 0.9703, -0.2419],\n",
      "        [-0.3780,  0.9258],\n",
      "        [-0.9936, -0.1128]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4793],\n",
      "        [2.1745],\n",
      "        [7.6110]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 215, Loss: 0.7142138285369051\n",
      "tensor([[ 0.9705, -0.2410],\n",
      "        [-0.3743,  0.9273],\n",
      "        [-0.9936, -0.1131]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4834],\n",
      "        [2.1767],\n",
      "        [7.6136]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 216, Loss: 0.7045900711612918\n",
      "tensor([[ 0.9708, -0.2400],\n",
      "        [-0.3707,  0.9287],\n",
      "        [-0.9935, -0.1135]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4874],\n",
      "        [2.1790],\n",
      "        [7.6163]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 217, Loss: 0.6906520881446709\n",
      "tensor([[ 0.9710, -0.2391],\n",
      "        [-0.3671,  0.9302],\n",
      "        [-0.9935, -0.1138]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4914],\n",
      "        [2.1813],\n",
      "        [7.6188]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 218, Loss: 0.7098663437797338\n",
      "tensor([[ 0.9712, -0.2382],\n",
      "        [-0.3636,  0.9316],\n",
      "        [-0.9935, -0.1141]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4954],\n",
      "        [2.1836],\n",
      "        [7.6214]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 219, Loss: 0.7059741183658512\n",
      "tensor([[ 0.9715, -0.2372],\n",
      "        [-0.3600,  0.9330],\n",
      "        [-0.9934, -0.1143]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4994],\n",
      "        [2.1859],\n",
      "        [7.6240]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 220, Loss: 0.713097098115697\n",
      "tensor([[ 0.9717, -0.2363],\n",
      "        [-0.3564,  0.9343],\n",
      "        [-0.9934, -0.1146]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5035],\n",
      "        [2.1883],\n",
      "        [7.6266]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 221, Loss: 0.6983428404416367\n",
      "tensor([[ 0.9719, -0.2355],\n",
      "        [-0.3529,  0.9357],\n",
      "        [-0.9934, -0.1148]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5075],\n",
      "        [2.1907],\n",
      "        [7.6292]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 222, Loss: 0.6945322798801625\n",
      "tensor([[ 0.9721, -0.2346],\n",
      "        [-0.3494,  0.9370],\n",
      "        [-0.9934, -0.1151]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5116],\n",
      "        [2.1932],\n",
      "        [7.6318]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 223, Loss: 0.6821803905128824\n",
      "tensor([[ 0.9723, -0.2337],\n",
      "        [-0.3459,  0.9383],\n",
      "        [-0.9933, -0.1153]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5156],\n",
      "        [2.1957],\n",
      "        [7.6344]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 224, Loss: 0.6899411879721854\n",
      "tensor([[ 0.9725, -0.2328],\n",
      "        [-0.3424,  0.9396],\n",
      "        [-0.9933, -0.1156]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5197],\n",
      "        [2.1982],\n",
      "        [7.6369]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 225, Loss: 0.7040924426762575\n",
      "tensor([[ 0.9727, -0.2319],\n",
      "        [-0.3389,  0.9408],\n",
      "        [-0.9933, -0.1160]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5238],\n",
      "        [2.2007],\n",
      "        [7.6395]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 226, Loss: 0.6909971484015817\n",
      "tensor([[ 0.9729, -0.2312],\n",
      "        [-0.3354,  0.9421],\n",
      "        [-0.9932, -0.1163]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5279],\n",
      "        [2.2033],\n",
      "        [7.6420]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 227, Loss: 0.703486709486962\n",
      "tensor([[ 0.9731, -0.2305],\n",
      "        [-0.3319,  0.9433],\n",
      "        [-0.9932, -0.1165]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5320],\n",
      "        [2.2058],\n",
      "        [7.6446]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 228, Loss: 0.7021250492756944\n",
      "tensor([[ 0.9733, -0.2297],\n",
      "        [-0.3284,  0.9445],\n",
      "        [-0.9931, -0.1168]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5362],\n",
      "        [2.2084],\n",
      "        [7.6472]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 229, Loss: 0.7001318735223968\n",
      "tensor([[ 0.9734, -0.2290],\n",
      "        [-0.3249,  0.9457],\n",
      "        [-0.9931, -0.1171]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5403],\n",
      "        [2.2110],\n",
      "        [7.6497]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 230, Loss: 0.7082049002076054\n",
      "tensor([[ 0.9736, -0.2283],\n",
      "        [-0.3214,  0.9469],\n",
      "        [-0.9931, -0.1173]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5444],\n",
      "        [2.2135],\n",
      "        [7.6523]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 231, Loss: 0.7001047007384711\n",
      "tensor([[ 0.9737, -0.2277],\n",
      "        [-0.3179,  0.9481],\n",
      "        [-0.9931, -0.1175]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5486],\n",
      "        [2.2162],\n",
      "        [7.6548]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 232, Loss: 0.7112771983011253\n",
      "tensor([[ 0.9739, -0.2272],\n",
      "        [-0.3144,  0.9493],\n",
      "        [-0.9930, -0.1178]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5527],\n",
      "        [2.2188],\n",
      "        [7.6573]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 233, Loss: 0.6935340742114097\n",
      "tensor([[ 0.9740, -0.2267],\n",
      "        [-0.3110,  0.9504],\n",
      "        [-0.9930, -0.1180]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5568],\n",
      "        [2.2214],\n",
      "        [7.6599]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 234, Loss: 0.683190004619246\n",
      "tensor([[ 0.9741, -0.2262],\n",
      "        [-0.3075,  0.9516],\n",
      "        [-0.9930, -0.1182]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5610],\n",
      "        [2.2241],\n",
      "        [7.6625]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 235, Loss: 0.6954822819029848\n",
      "tensor([[ 0.9742, -0.2256],\n",
      "        [-0.3040,  0.9527],\n",
      "        [-0.9930, -0.1184]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5652],\n",
      "        [2.2268],\n",
      "        [7.6650]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 236, Loss: 0.690922894942527\n",
      "tensor([[ 0.9743, -0.2251],\n",
      "        [-0.3006,  0.9538],\n",
      "        [-0.9929, -0.1186]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5694],\n",
      "        [2.2295],\n",
      "        [7.6676]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 237, Loss: 0.705021067385639\n",
      "tensor([[ 0.9745, -0.2245],\n",
      "        [-0.2972,  0.9548],\n",
      "        [-0.9929, -0.1188]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5736],\n",
      "        [2.2323],\n",
      "        [7.6702]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 238, Loss: 0.6804703644321899\n",
      "tensor([[ 0.9746, -0.2239],\n",
      "        [-0.2938,  0.9559],\n",
      "        [-0.9929, -0.1191]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5778],\n",
      "        [2.2350],\n",
      "        [7.6728]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 239, Loss: 0.6851273464643495\n",
      "tensor([[ 0.9747, -0.2234],\n",
      "        [-0.2903,  0.9569],\n",
      "        [-0.9928, -0.1194]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5820],\n",
      "        [2.2378],\n",
      "        [7.6755]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 240, Loss: 0.7012195844863406\n",
      "tensor([[ 0.9748, -0.2229],\n",
      "        [-0.2869,  0.9580],\n",
      "        [-0.9928, -0.1196]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5863],\n",
      "        [2.2406],\n",
      "        [7.6781]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 241, Loss: 0.6837517591746772\n",
      "tensor([[ 0.9750, -0.2224],\n",
      "        [-0.2835,  0.9590],\n",
      "        [-0.9928, -0.1198]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5906],\n",
      "        [2.2434],\n",
      "        [7.6807]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 242, Loss: 0.7011513900395447\n",
      "tensor([[ 0.9751, -0.2219],\n",
      "        [-0.2802,  0.9600],\n",
      "        [-0.9928, -0.1200]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5948],\n",
      "        [2.2462],\n",
      "        [7.6832]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 243, Loss: 0.7038308555865671\n",
      "tensor([[ 0.9752, -0.2215],\n",
      "        [-0.2768,  0.9609],\n",
      "        [-0.9927, -0.1202]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5991],\n",
      "        [2.2491],\n",
      "        [7.6857]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 244, Loss: 0.6869593931769868\n",
      "tensor([[ 0.9752, -0.2211],\n",
      "        [-0.2735,  0.9619],\n",
      "        [-0.9927, -0.1205]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6034],\n",
      "        [2.2519],\n",
      "        [7.6883]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 245, Loss: 0.681136327124645\n",
      "tensor([[ 0.9753, -0.2207],\n",
      "        [-0.2701,  0.9628],\n",
      "        [-0.9927, -0.1206]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6076],\n",
      "        [2.2548],\n",
      "        [7.6909]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 246, Loss: 0.6869741946143593\n",
      "tensor([[ 0.9754, -0.2203],\n",
      "        [-0.2668,  0.9638],\n",
      "        [-0.9927, -0.1208]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6119],\n",
      "        [2.2578],\n",
      "        [7.6935]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 247, Loss: 0.6855535738958858\n",
      "tensor([[ 0.9755, -0.2199],\n",
      "        [-0.2635,  0.9647],\n",
      "        [-0.9927, -0.1209]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6162],\n",
      "        [2.2607],\n",
      "        [7.6961]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 248, Loss: 0.6897449198387299\n",
      "tensor([[ 0.9756, -0.2195],\n",
      "        [-0.2601,  0.9656],\n",
      "        [-0.9926, -0.1210]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6206],\n",
      "        [2.2636],\n",
      "        [7.6987]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 249, Loss: 0.6787047153316638\n",
      "tensor([[ 0.9757, -0.2191],\n",
      "        [-0.2568,  0.9665],\n",
      "        [-0.9926, -0.1212]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6249],\n",
      "        [2.2666],\n",
      "        [7.7013]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 250, Loss: 0.6826875004221441\n",
      "tensor([[ 0.9758, -0.2186],\n",
      "        [-0.2535,  0.9673],\n",
      "        [-0.9926, -0.1213]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6292],\n",
      "        [2.2696],\n",
      "        [7.7039]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 251, Loss: 0.6884613025652997\n",
      "tensor([[ 0.9759, -0.2182],\n",
      "        [-0.2503,  0.9682],\n",
      "        [-0.9926, -0.1215]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6336],\n",
      "        [2.2726],\n",
      "        [7.7065]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 252, Loss: 0.6888439171678626\n",
      "tensor([[ 0.9760, -0.2179],\n",
      "        [-0.2470,  0.9690],\n",
      "        [-0.9926, -0.1217]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6379],\n",
      "        [2.2757],\n",
      "        [7.7091]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 253, Loss: 0.6791801084512407\n",
      "tensor([[ 0.9760, -0.2176],\n",
      "        [-0.2438,  0.9698],\n",
      "        [-0.9926, -0.1218]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6422],\n",
      "        [2.2787],\n",
      "        [7.7116]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 254, Loss: 0.6940748102975064\n",
      "tensor([[ 0.9761, -0.2174],\n",
      "        [-0.2405,  0.9706],\n",
      "        [-0.9925, -0.1219]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6466],\n",
      "        [2.2817],\n",
      "        [7.7142]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 255, Loss: 0.6863119635283746\n",
      "tensor([[ 0.9761, -0.2172],\n",
      "        [-0.2373,  0.9714],\n",
      "        [-0.9925, -0.1221]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6510],\n",
      "        [2.2848],\n",
      "        [7.7168]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 256, Loss: 0.665273029396308\n",
      "tensor([[ 0.9762, -0.2169],\n",
      "        [-0.2341,  0.9722],\n",
      "        [-0.9925, -0.1223]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6553],\n",
      "        [2.2879],\n",
      "        [7.7195]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 257, Loss: 0.6699468280271903\n",
      "tensor([[ 0.9762, -0.2167],\n",
      "        [-0.2310,  0.9730],\n",
      "        [-0.9925, -0.1225]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6597],\n",
      "        [2.2911],\n",
      "        [7.7221]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 258, Loss: 0.6747571679317762\n",
      "tensor([[ 0.9763, -0.2164],\n",
      "        [-0.2279,  0.9737],\n",
      "        [-0.9924, -0.1227]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6641],\n",
      "        [2.2942],\n",
      "        [7.7247]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 259, Loss: 0.6787256086439818\n",
      "tensor([[ 0.9764, -0.2161],\n",
      "        [-0.2248,  0.9744],\n",
      "        [-0.9924, -0.1230]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6685],\n",
      "        [2.2974],\n",
      "        [7.7273]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 260, Loss: 0.6713934349134787\n",
      "tensor([[ 0.9764, -0.2158],\n",
      "        [-0.2217,  0.9751],\n",
      "        [-0.9924, -0.1232]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6729],\n",
      "        [2.3007],\n",
      "        [7.7299]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 261, Loss: 0.6629872166039814\n",
      "tensor([[ 0.9765, -0.2155],\n",
      "        [-0.2187,  0.9758],\n",
      "        [-0.9924, -0.1234]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6773],\n",
      "        [2.3039],\n",
      "        [7.7325]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 262, Loss: 0.6764999136282871\n",
      "tensor([[ 0.9766, -0.2152],\n",
      "        [-0.2156,  0.9765],\n",
      "        [-0.9923, -0.1238]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6818],\n",
      "        [2.3072],\n",
      "        [7.7351]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 263, Loss: 0.674268228679383\n",
      "tensor([[ 0.9766, -0.2149],\n",
      "        [-0.2126,  0.9771],\n",
      "        [-0.9923, -0.1240]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6862],\n",
      "        [2.3104],\n",
      "        [7.7377]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 264, Loss: 0.6796425451690932\n",
      "tensor([[ 0.9767, -0.2147],\n",
      "        [-0.2096,  0.9778],\n",
      "        [-0.9923, -0.1242]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6906],\n",
      "        [2.3137],\n",
      "        [7.7403]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 265, Loss: 0.692932197420691\n",
      "tensor([[ 0.9767, -0.2145],\n",
      "        [-0.2066,  0.9784],\n",
      "        [-0.9922, -0.1243]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6950],\n",
      "        [2.3170],\n",
      "        [7.7428]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 266, Loss: 0.6688844799412477\n",
      "tensor([[ 0.9768, -0.2143],\n",
      "        [-0.2036,  0.9791],\n",
      "        [-0.9922, -0.1244]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6995],\n",
      "        [2.3203],\n",
      "        [7.7454]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 267, Loss: 0.6850745221550104\n",
      "tensor([[ 0.9768, -0.2140],\n",
      "        [-0.2006,  0.9797],\n",
      "        [-0.9922, -0.1244]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7039],\n",
      "        [2.3236],\n",
      "        [7.7479]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 268, Loss: 0.6766284095150794\n",
      "tensor([[ 0.9769, -0.2136],\n",
      "        [-0.1976,  0.9803],\n",
      "        [-0.9922, -0.1244]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7083],\n",
      "        [2.3269],\n",
      "        [7.7506]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 269, Loss: 0.676527996308564\n",
      "tensor([[ 0.9770, -0.2133],\n",
      "        [-0.1946,  0.9809],\n",
      "        [-0.9922, -0.1245]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7128],\n",
      "        [2.3302],\n",
      "        [7.7532]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 270, Loss: 0.6665097275574389\n",
      "tensor([[ 0.9770, -0.2130],\n",
      "        [-0.1917,  0.9815],\n",
      "        [-0.9922, -0.1247]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7172],\n",
      "        [2.3336],\n",
      "        [7.7558]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 271, Loss: 0.6698189014120272\n",
      "tensor([[ 0.9771, -0.2128],\n",
      "        [-0.1888,  0.9820],\n",
      "        [-0.9922, -0.1248]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7217],\n",
      "        [2.3370],\n",
      "        [7.7584]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 272, Loss: 0.6619716316150981\n",
      "tensor([[ 0.9771, -0.2126],\n",
      "        [-0.1859,  0.9826],\n",
      "        [-0.9922, -0.1250]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7262],\n",
      "        [2.3403],\n",
      "        [7.7610]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 273, Loss: 0.6780759281943008\n",
      "tensor([[ 0.9772, -0.2124],\n",
      "        [-0.1831,  0.9831],\n",
      "        [-0.9921, -0.1253]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7306],\n",
      "        [2.3437],\n",
      "        [7.7636]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 274, Loss: 0.6637042415679172\n",
      "tensor([[ 0.9772, -0.2122],\n",
      "        [-0.1803,  0.9836],\n",
      "        [-0.9921, -0.1255]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7351],\n",
      "        [2.3472],\n",
      "        [7.7663]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 275, Loss: 0.6757871057851655\n",
      "tensor([[ 0.9773, -0.2119],\n",
      "        [-0.1774,  0.9841],\n",
      "        [-0.9921, -0.1258]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7395],\n",
      "        [2.3506],\n",
      "        [7.7689]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 276, Loss: 0.6721573385284367\n",
      "tensor([[ 0.9773, -0.2117],\n",
      "        [-0.1747,  0.9846],\n",
      "        [-0.9920, -0.1261]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7440],\n",
      "        [2.3541],\n",
      "        [7.7715]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 277, Loss: 0.6707508371944799\n",
      "tensor([[ 0.9774, -0.2115],\n",
      "        [-0.1719,  0.9851],\n",
      "        [-0.9920, -0.1263]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7484],\n",
      "        [2.3575],\n",
      "        [7.7740]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 278, Loss: 0.6711673684048741\n",
      "tensor([[ 0.9774, -0.2113],\n",
      "        [-0.1692,  0.9856],\n",
      "        [-0.9920, -0.1265]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7529],\n",
      "        [2.3610],\n",
      "        [7.7766]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 279, Loss: 0.6779450948500462\n",
      "tensor([[ 0.9775, -0.2111],\n",
      "        [-0.1664,  0.9861],\n",
      "        [-0.9919, -0.1267]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7574],\n",
      "        [2.3645],\n",
      "        [7.7790]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 280, Loss: 0.6816780647405188\n",
      "tensor([[ 0.9775, -0.2109],\n",
      "        [-0.1637,  0.9865],\n",
      "        [-0.9919, -0.1270]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7618],\n",
      "        [2.3680],\n",
      "        [7.7815]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 281, Loss: 0.6640776760014704\n",
      "tensor([[ 0.9775, -0.2107],\n",
      "        [-0.1610,  0.9870],\n",
      "        [-0.9919, -0.1273]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7663],\n",
      "        [2.3715],\n",
      "        [7.7840]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 282, Loss: 0.6689889014050183\n",
      "tensor([[ 0.9776, -0.2106],\n",
      "        [-0.1582,  0.9874],\n",
      "        [-0.9918, -0.1275]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7708],\n",
      "        [2.3750],\n",
      "        [7.7865]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 283, Loss: 0.6765989683063869\n",
      "tensor([[ 0.9776, -0.2105],\n",
      "        [-0.1555,  0.9878],\n",
      "        [-0.9918, -0.1277]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7753],\n",
      "        [2.3785],\n",
      "        [7.7890]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 284, Loss: 0.6625651589483855\n",
      "tensor([[ 0.9776, -0.2104],\n",
      "        [-0.1529,  0.9882],\n",
      "        [-0.9918, -0.1280]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7797],\n",
      "        [2.3820],\n",
      "        [7.7915]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 285, Loss: 0.6703103602345047\n",
      "tensor([[ 0.9777, -0.2102],\n",
      "        [-0.1502,  0.9887],\n",
      "        [-0.9918, -0.1282]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7841],\n",
      "        [2.3856],\n",
      "        [7.7939]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 286, Loss: 0.6768370443400918\n",
      "tensor([[ 0.9777, -0.2100],\n",
      "        [-0.1476,  0.9891],\n",
      "        [-0.9917, -0.1284]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7885],\n",
      "        [2.3891],\n",
      "        [7.7964]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 287, Loss: 0.6679996813302086\n",
      "tensor([[ 0.9777, -0.2098],\n",
      "        [-0.1450,  0.9894],\n",
      "        [-0.9917, -0.1285]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7929],\n",
      "        [2.3927],\n",
      "        [7.7989]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 288, Loss: 0.6608683048253012\n",
      "tensor([[ 0.9778, -0.2096],\n",
      "        [-0.1424,  0.9898],\n",
      "        [-0.9917, -0.1286]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7974],\n",
      "        [2.3963],\n",
      "        [7.8015]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 289, Loss: 0.6716639994333157\n",
      "tensor([[ 0.9778, -0.2094],\n",
      "        [-0.1398,  0.9902],\n",
      "        [-0.9917, -0.1287]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8018],\n",
      "        [2.3999],\n",
      "        [7.8040]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 290, Loss: 0.6564323622342725\n",
      "tensor([[ 0.9779, -0.2091],\n",
      "        [-0.1372,  0.9905],\n",
      "        [-0.9917, -0.1287]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8062],\n",
      "        [2.4035],\n",
      "        [7.8067]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 291, Loss: 0.6590489049799418\n",
      "tensor([[ 0.9780, -0.2088],\n",
      "        [-0.1347,  0.9909],\n",
      "        [-0.9917, -0.1288]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8107],\n",
      "        [2.4071],\n",
      "        [7.8093]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 292, Loss: 0.6538714061293397\n",
      "tensor([[ 0.9780, -0.2084],\n",
      "        [-0.1322,  0.9912],\n",
      "        [-0.9917, -0.1288]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8151],\n",
      "        [2.4108],\n",
      "        [7.8120]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 293, Loss: 0.6663686148425753\n",
      "tensor([[ 0.9781, -0.2081],\n",
      "        [-0.1297,  0.9916],\n",
      "        [-0.9917, -0.1288]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8195],\n",
      "        [2.4144],\n",
      "        [7.8148]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 294, Loss: 0.6625397014211059\n",
      "tensor([[ 0.9782, -0.2077],\n",
      "        [-0.1272,  0.9919],\n",
      "        [-0.9917, -0.1288]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8239],\n",
      "        [2.4181],\n",
      "        [7.8175]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 295, Loss: 0.6624849582043705\n",
      "tensor([[ 0.9783, -0.2073],\n",
      "        [-0.1247,  0.9922],\n",
      "        [-0.9917, -0.1289]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8284],\n",
      "        [2.4217],\n",
      "        [7.8202]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 296, Loss: 0.6631605006982048\n",
      "tensor([[ 0.9783, -0.2070],\n",
      "        [-0.1223,  0.9925],\n",
      "        [-0.9916, -0.1290]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8328],\n",
      "        [2.4254],\n",
      "        [7.8229]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 297, Loss: 0.6536326622590556\n",
      "tensor([[ 0.9784, -0.2066],\n",
      "        [-0.1199,  0.9928],\n",
      "        [-0.9916, -0.1290]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8373],\n",
      "        [2.4291],\n",
      "        [7.8256]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 298, Loss: 0.655945255819101\n",
      "tensor([[ 0.9785, -0.2064],\n",
      "        [-0.1174,  0.9931],\n",
      "        [-0.9916, -0.1290]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8418],\n",
      "        [2.4328],\n",
      "        [7.8283]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 299, Loss: 0.6653755321123064\n",
      "tensor([[ 0.9785, -0.2061],\n",
      "        [-0.1150,  0.9934],\n",
      "        [-0.9916, -0.1290]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8463],\n",
      "        [2.4365],\n",
      "        [7.8310]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 300, Loss: 0.6524082541866739\n",
      "tensor([[ 0.9786, -0.2058],\n",
      "        [-0.1126,  0.9936],\n",
      "        [-0.9916, -0.1290]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8508],\n",
      "        [2.4402],\n",
      "        [7.8337]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 301, Loss: 0.6710639785386382\n",
      "tensor([[ 0.9786, -0.2056],\n",
      "        [-0.1102,  0.9939],\n",
      "        [-0.9917, -0.1289]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8554],\n",
      "        [2.4439],\n",
      "        [7.8364]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 302, Loss: 0.6547408861776237\n",
      "tensor([[ 0.9787, -0.2055],\n",
      "        [-0.1077,  0.9942],\n",
      "        [-0.9916, -0.1290]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8600],\n",
      "        [2.4476],\n",
      "        [7.8391]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 303, Loss: 0.6507769584245289\n",
      "tensor([[ 0.9787, -0.2054],\n",
      "        [-0.1053,  0.9944],\n",
      "        [-0.9916, -0.1290]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8646],\n",
      "        [2.4513],\n",
      "        [7.8417]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 304, Loss: 0.6662351069847027\n",
      "tensor([[ 0.9787, -0.2053],\n",
      "        [-0.1029,  0.9947],\n",
      "        [-0.9916, -0.1291]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8692],\n",
      "        [2.4550],\n",
      "        [7.8444]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 305, Loss: 0.6529929974754185\n",
      "tensor([[ 0.9787, -0.2052],\n",
      "        [-0.1006,  0.9949],\n",
      "        [-0.9916, -0.1291]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8738],\n",
      "        [2.4587],\n",
      "        [7.8470]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 306, Loss: 0.6601897498220467\n",
      "tensor([[ 0.9787, -0.2052],\n",
      "        [-0.0983,  0.9952],\n",
      "        [-0.9916, -0.1291]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8784],\n",
      "        [2.4624],\n",
      "        [7.8496]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 307, Loss: 0.6664121592727774\n",
      "tensor([[ 0.9787, -0.2051],\n",
      "        [-0.0960,  0.9954],\n",
      "        [-0.9916, -0.1292]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8829],\n",
      "        [2.4662],\n",
      "        [7.8523]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 308, Loss: 0.6601019940057078\n",
      "tensor([[ 0.9787, -0.2051],\n",
      "        [-0.0937,  0.9956],\n",
      "        [-0.9916, -0.1293]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8875],\n",
      "        [2.4699],\n",
      "        [7.8549]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 309, Loss: 0.661954132673948\n",
      "tensor([[ 0.9788, -0.2050],\n",
      "        [-0.0913,  0.9958],\n",
      "        [-0.9916, -0.1294]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8921],\n",
      "        [2.4736],\n",
      "        [7.8575]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 310, Loss: 0.6621170033960394\n",
      "tensor([[ 0.9788, -0.2050],\n",
      "        [-0.0890,  0.9960],\n",
      "        [-0.9916, -0.1296]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8968],\n",
      "        [2.4773],\n",
      "        [7.8602]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 311, Loss: 0.6567122334526759\n",
      "tensor([[ 0.9787, -0.2051],\n",
      "        [-0.0868,  0.9962],\n",
      "        [-0.9915, -0.1298]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9014],\n",
      "        [2.4810],\n",
      "        [7.8629]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 312, Loss: 0.6556736515892159\n",
      "tensor([[ 0.9787, -0.2052],\n",
      "        [-0.0845,  0.9964],\n",
      "        [-0.9915, -0.1300]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9060],\n",
      "        [2.4848],\n",
      "        [7.8655]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 313, Loss: 0.6618471469259471\n",
      "tensor([[ 0.9787, -0.2052],\n",
      "        [-0.0823,  0.9966],\n",
      "        [-0.9915, -0.1301]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9106],\n",
      "        [2.4885],\n",
      "        [7.8682]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 314, Loss: 0.6498674985571399\n",
      "tensor([[ 0.9787, -0.2052],\n",
      "        [-0.0802,  0.9968],\n",
      "        [-0.9915, -0.1302]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9152],\n",
      "        [2.4923],\n",
      "        [7.8708]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 315, Loss: 0.6506361828109758\n",
      "tensor([[ 0.9787, -0.2052],\n",
      "        [-0.0780,  0.9970],\n",
      "        [-0.9915, -0.1303]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9198],\n",
      "        [2.4961],\n",
      "        [7.8735]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 316, Loss: 0.6399718116110522\n",
      "tensor([[ 0.9787, -0.2051],\n",
      "        [-0.0758,  0.9971],\n",
      "        [-0.9915, -0.1303]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9245],\n",
      "        [2.4999],\n",
      "        [7.8762]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 317, Loss: 0.6654209371356813\n",
      "tensor([[ 0.9788, -0.2049],\n",
      "        [-0.0737,  0.9973],\n",
      "        [-0.9915, -0.1303]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9291],\n",
      "        [2.5036],\n",
      "        [7.8788]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 318, Loss: 0.6668968696283338\n",
      "tensor([[ 0.9788, -0.2049],\n",
      "        [-0.0716,  0.9974],\n",
      "        [-0.9915, -0.1302]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9337],\n",
      "        [2.5074],\n",
      "        [7.8813]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 319, Loss: 0.668434698721986\n",
      "tensor([[ 0.9788, -0.2049],\n",
      "        [-0.0695,  0.9976],\n",
      "        [-0.9915, -0.1301]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9383],\n",
      "        [2.5112],\n",
      "        [7.8838]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 320, Loss: 0.6599085052923426\n",
      "tensor([[ 0.9788, -0.2049],\n",
      "        [-0.0673,  0.9977],\n",
      "        [-0.9915, -0.1300]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9430],\n",
      "        [2.5150],\n",
      "        [7.8864]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 321, Loss: 0.6632298455015999\n",
      "tensor([[ 0.9788, -0.2050],\n",
      "        [-0.0652,  0.9979],\n",
      "        [-0.9915, -0.1299]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9477],\n",
      "        [2.5188],\n",
      "        [7.8889]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 322, Loss: 0.6442489646779171\n",
      "tensor([[ 0.9788, -0.2050],\n",
      "        [-0.0631,  0.9980],\n",
      "        [-0.9915, -0.1298]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9524],\n",
      "        [2.5225],\n",
      "        [7.8913]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 323, Loss: 0.6729733607720503\n",
      "tensor([[ 0.9788, -0.2049],\n",
      "        [-0.0611,  0.9981],\n",
      "        [-0.9916, -0.1296]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9571],\n",
      "        [2.5263],\n",
      "        [7.8938]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 324, Loss: 0.6498854292892454\n",
      "tensor([[ 0.9788, -0.2048],\n",
      "        [-0.0590,  0.9983],\n",
      "        [-0.9916, -0.1295]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9618],\n",
      "        [2.5301],\n",
      "        [7.8964]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 325, Loss: 0.6443811039366862\n",
      "tensor([[ 0.9788, -0.2047],\n",
      "        [-0.0569,  0.9984],\n",
      "        [-0.9916, -0.1293]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9665],\n",
      "        [2.5339],\n",
      "        [7.8989]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 326, Loss: 0.6608150569579415\n",
      "tensor([[ 0.9788, -0.2047],\n",
      "        [-0.0549,  0.9985],\n",
      "        [-0.9916, -0.1291]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9712],\n",
      "        [2.5377],\n",
      "        [7.9014]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 327, Loss: 0.6637977301292157\n",
      "tensor([[ 0.9788, -0.2046],\n",
      "        [-0.0529,  0.9986],\n",
      "        [-0.9917, -0.1289]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9758],\n",
      "        [2.5415],\n",
      "        [7.9039]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 328, Loss: 0.6506267084847328\n",
      "tensor([[ 0.9789, -0.2044],\n",
      "        [-0.0509,  0.9987],\n",
      "        [-0.9917, -0.1288]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9805],\n",
      "        [2.5454],\n",
      "        [7.9065]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 329, Loss: 0.6467138835679546\n",
      "tensor([[ 0.9789, -0.2042],\n",
      "        [-0.0490,  0.9988],\n",
      "        [-0.9917, -0.1286]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9852],\n",
      "        [2.5492],\n",
      "        [7.9090]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 330, Loss: 0.6456011659660524\n",
      "tensor([[ 0.9790, -0.2039],\n",
      "        [-0.0470,  0.9989],\n",
      "        [-0.9917, -0.1286]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9899],\n",
      "        [2.5531],\n",
      "        [7.9115]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 331, Loss: 0.6523473086090128\n",
      "tensor([[ 0.9790, -0.2036],\n",
      "        [-0.0451,  0.9990],\n",
      "        [-0.9917, -0.1285]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9946],\n",
      "        [2.5569],\n",
      "        [7.9141]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 332, Loss: 0.6447042503380997\n",
      "tensor([[ 0.9791, -0.2033],\n",
      "        [-0.0433,  0.9991],\n",
      "        [-0.9917, -0.1284]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9992],\n",
      "        [2.5608],\n",
      "        [7.9166]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 333, Loss: 0.6628726834773553\n",
      "tensor([[ 0.9792, -0.2029],\n",
      "        [-0.0414,  0.9991],\n",
      "        [-0.9917, -0.1283]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0039],\n",
      "        [2.5647],\n",
      "        [7.9192]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 334, Loss: 0.6467629278620354\n",
      "tensor([[ 0.9793, -0.2026],\n",
      "        [-0.0396,  0.9992],\n",
      "        [-0.9917, -0.1282]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0085],\n",
      "        [2.5685],\n",
      "        [7.9218]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 335, Loss: 0.64836878138613\n",
      "tensor([[ 0.9793, -0.2022],\n",
      "        [-0.0378,  0.9993],\n",
      "        [-0.9917, -0.1283]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0132],\n",
      "        [2.5724],\n",
      "        [7.9244]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 336, Loss: 0.6622658661554311\n",
      "tensor([[ 0.9794, -0.2017],\n",
      "        [-0.0360,  0.9994],\n",
      "        [-0.9917, -0.1282]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0178],\n",
      "        [2.5763],\n",
      "        [7.9269]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 337, Loss: 0.663018569740855\n",
      "tensor([[ 0.9795, -0.2013],\n",
      "        [-0.0342,  0.9994],\n",
      "        [-0.9918, -0.1282]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0225],\n",
      "        [2.5802],\n",
      "        [7.9295]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 338, Loss: 0.6467693036209107\n",
      "tensor([[ 0.9796, -0.2009],\n",
      "        [-0.0324,  0.9995],\n",
      "        [-0.9918, -0.1281]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0271],\n",
      "        [2.5840],\n",
      "        [7.9320]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 339, Loss: 0.6511269317084658\n",
      "tensor([[ 0.9797, -0.2005],\n",
      "        [-0.0306,  0.9995],\n",
      "        [-0.9918, -0.1281]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0318],\n",
      "        [2.5879],\n",
      "        [7.9346]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 340, Loss: 0.650353330517349\n",
      "tensor([[ 0.9798, -0.2001],\n",
      "        [-0.0289,  0.9996],\n",
      "        [-0.9918, -0.1280]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0364],\n",
      "        [2.5918],\n",
      "        [7.9371]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 341, Loss: 0.6539760842313974\n",
      "tensor([[ 0.9798, -0.1998],\n",
      "        [-0.0271,  0.9996],\n",
      "        [-0.9918, -0.1280]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0411],\n",
      "        [2.5957],\n",
      "        [7.9396]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 342, Loss: 0.6462400084590584\n",
      "tensor([[ 0.9799, -0.1995],\n",
      "        [-0.0254,  0.9997],\n",
      "        [-0.9918, -0.1280]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0458],\n",
      "        [2.5996],\n",
      "        [7.9421]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 343, Loss: 0.639377524544547\n",
      "tensor([[ 0.9799, -0.1993],\n",
      "        [-0.0236,  0.9997],\n",
      "        [-0.9918, -0.1279]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0504],\n",
      "        [2.6035],\n",
      "        [7.9447]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 344, Loss: 0.6585544976688322\n",
      "tensor([[ 0.9800, -0.1992],\n",
      "        [-0.0219,  0.9998],\n",
      "        [-0.9918, -0.1279]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0551],\n",
      "        [2.6073],\n",
      "        [7.9473]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 345, Loss: 0.6372297530360301\n",
      "tensor([[ 0.9800, -0.1990],\n",
      "        [-0.0203,  0.9998],\n",
      "        [-0.9918, -0.1280]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0598],\n",
      "        [2.6112],\n",
      "        [7.9499]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 346, Loss: 0.6475996687854044\n",
      "tensor([[ 0.9800, -0.1988],\n",
      "        [-0.0186,  0.9998],\n",
      "        [-0.9918, -0.1282]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0644],\n",
      "        [2.6151],\n",
      "        [7.9525]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 347, Loss: 0.6461800634799824\n",
      "tensor([[ 0.9801, -0.1986],\n",
      "        [-0.0170,  0.9999],\n",
      "        [-0.9917, -0.1283]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0691],\n",
      "        [2.6190],\n",
      "        [7.9551]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 348, Loss: 0.6355539350041054\n",
      "tensor([[ 0.9801, -0.1984],\n",
      "        [-0.0154,  0.9999],\n",
      "        [-0.9917, -0.1285]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0739],\n",
      "        [2.6229],\n",
      "        [7.9577]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 349, Loss: 0.6485697923608801\n",
      "tensor([[ 0.9801, -0.1983],\n",
      "        [-0.0138,  0.9999],\n",
      "        [-0.9917, -0.1286]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0786],\n",
      "        [2.6268],\n",
      "        [7.9602]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 350, Loss: 0.6499230328137937\n",
      "tensor([[ 0.9802, -0.1981],\n",
      "        [-0.0122,  0.9999],\n",
      "        [-0.9917, -0.1287]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0833],\n",
      "        [2.6307],\n",
      "        [7.9628]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 351, Loss: 0.6377290260218508\n",
      "tensor([[ 0.9802, -0.1979],\n",
      "        [-0.0106,  0.9999],\n",
      "        [-0.9917, -0.1288]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0881],\n",
      "        [2.6346],\n",
      "        [7.9654]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 352, Loss: 0.6489550688919894\n",
      "tensor([[ 0.9803, -0.1976],\n",
      "        [-0.0091,  1.0000],\n",
      "        [-0.9916, -0.1290]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0928],\n",
      "        [2.6385],\n",
      "        [7.9679]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 353, Loss: 0.6556582908211089\n",
      "tensor([[ 0.9803, -0.1973],\n",
      "        [-0.0076,  1.0000],\n",
      "        [-0.9916, -0.1291]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0976],\n",
      "        [2.6424],\n",
      "        [7.9705]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 354, Loss: 0.6424138050606898\n",
      "tensor([[ 0.9804, -0.1970],\n",
      "        [-0.0061,  1.0000],\n",
      "        [-0.9916, -0.1293]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1024],\n",
      "        [2.6463],\n",
      "        [7.9730]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 355, Loss: 0.6383670134903746\n",
      "tensor([[ 0.9805, -0.1967],\n",
      "        [-0.0045,  1.0000],\n",
      "        [-0.9916, -0.1294]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1072],\n",
      "        [2.6502],\n",
      "        [7.9756]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 356, Loss: 0.649880506438865\n",
      "tensor([[ 0.9805, -0.1964],\n",
      "        [-0.0030,  1.0000],\n",
      "        [-0.9916, -0.1295]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1119],\n",
      "        [2.6541],\n",
      "        [7.9782]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 357, Loss: 0.6491241535400584\n",
      "tensor([[ 0.9806, -0.1962],\n",
      "        [-0.0015,  1.0000],\n",
      "        [-0.9916, -0.1296]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1167],\n",
      "        [2.6581],\n",
      "        [7.9807]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 358, Loss: 0.6389085771679902\n",
      "tensor([[ 9.8064e-01, -1.9584e-01],\n",
      "        [-3.4934e-05,  1.0000e+00],\n",
      "        [-9.9156e-01, -1.2966e-01]], dtype=torch.float64,\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1215],\n",
      "        [2.6620],\n",
      "        [7.9833]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 359, Loss: 0.6425754828842133\n",
      "tensor([[ 0.9807, -0.1955],\n",
      "        [ 0.0015,  1.0000],\n",
      "        [-0.9916, -0.1297]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1263],\n",
      "        [2.6659],\n",
      "        [7.9859]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 360, Loss: 0.6399318247595739\n",
      "tensor([[ 0.9808, -0.1953],\n",
      "        [ 0.0030,  1.0000],\n",
      "        [-0.9915, -0.1297]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1311],\n",
      "        [2.6698],\n",
      "        [7.9885]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 361, Loss: 0.654691943529205\n",
      "tensor([[ 0.9808, -0.1949],\n",
      "        [ 0.0044,  1.0000],\n",
      "        [-0.9915, -0.1298]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1359],\n",
      "        [2.6737],\n",
      "        [7.9911]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 362, Loss: 0.6456899234674541\n",
      "tensor([[ 0.9809, -0.1946],\n",
      "        [ 0.0059,  1.0000],\n",
      "        [-0.9915, -0.1298]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1407],\n",
      "        [2.6776],\n",
      "        [7.9936]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 363, Loss: 0.637195750569432\n",
      "tensor([[ 0.9809, -0.1944],\n",
      "        [ 0.0073,  1.0000],\n",
      "        [-0.9915, -0.1298]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1454],\n",
      "        [2.6815],\n",
      "        [7.9962]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 364, Loss: 0.6374326142236343\n",
      "tensor([[ 0.9810, -0.1942],\n",
      "        [ 0.0086,  1.0000],\n",
      "        [-0.9915, -0.1300]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1502],\n",
      "        [2.6855],\n",
      "        [7.9988]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 365, Loss: 0.6389819537233221\n",
      "tensor([[ 0.9810, -0.1940],\n",
      "        [ 0.0099,  1.0000],\n",
      "        [-0.9915, -0.1302]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1549],\n",
      "        [2.6894],\n",
      "        [8.0014]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 366, Loss: 0.6283147727410976\n",
      "tensor([[ 0.9810, -0.1940],\n",
      "        [ 0.0112,  0.9999],\n",
      "        [-0.9915, -0.1304]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1597],\n",
      "        [2.6933],\n",
      "        [8.0041]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 367, Loss: 0.6456515149165319\n",
      "tensor([[ 0.9810, -0.1940],\n",
      "        [ 0.0125,  0.9999],\n",
      "        [-0.9914, -0.1306]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1645],\n",
      "        [2.6972],\n",
      "        [8.0068]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 368, Loss: 0.6392895846960209\n",
      "tensor([[ 0.9810, -0.1940],\n",
      "        [ 0.0139,  0.9999],\n",
      "        [-0.9914, -0.1307]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1692],\n",
      "        [2.7011],\n",
      "        [8.0095]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 369, Loss: 0.6336326399170801\n",
      "tensor([[ 0.9810, -0.1941],\n",
      "        [ 0.0152,  0.9999],\n",
      "        [-0.9914, -0.1308]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1741],\n",
      "        [2.7050],\n",
      "        [8.0122]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 370, Loss: 0.6231145189187899\n",
      "tensor([[ 0.9810, -0.1941],\n",
      "        [ 0.0164,  0.9999],\n",
      "        [-0.9914, -0.1310]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1789],\n",
      "        [2.7089],\n",
      "        [8.0150]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 371, Loss: 0.6303058369794664\n",
      "tensor([[ 0.9810, -0.1941],\n",
      "        [ 0.0176,  0.9998],\n",
      "        [-0.9914, -0.1312]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1836],\n",
      "        [2.7129],\n",
      "        [8.0177]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 372, Loss: 0.6407445483370183\n",
      "tensor([[ 0.9810, -0.1940],\n",
      "        [ 0.0188,  0.9998],\n",
      "        [-0.9913, -0.1314]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1884],\n",
      "        [2.7168],\n",
      "        [8.0203]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 373, Loss: 0.6467586996716336\n",
      "tensor([[ 0.9810, -0.1939],\n",
      "        [ 0.0200,  0.9998],\n",
      "        [-0.9913, -0.1315]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1932],\n",
      "        [2.7207],\n",
      "        [8.0230]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 374, Loss: 0.6337971627886083\n",
      "tensor([[ 0.9811, -0.1937],\n",
      "        [ 0.0211,  0.9998],\n",
      "        [-0.9913, -0.1317]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1980],\n",
      "        [2.7246],\n",
      "        [8.0257]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 375, Loss: 0.6391320933919316\n",
      "tensor([[ 0.9811, -0.1937],\n",
      "        [ 0.0223,  0.9998],\n",
      "        [-0.9913, -0.1320]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2028],\n",
      "        [2.7285],\n",
      "        [8.0284]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 376, Loss: 0.6518937796509684\n",
      "tensor([[ 0.9811, -0.1936],\n",
      "        [ 0.0234,  0.9997],\n",
      "        [-0.9912, -0.1321]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2075],\n",
      "        [2.7325],\n",
      "        [8.0310]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 377, Loss: 0.6432471459397237\n",
      "tensor([[ 0.9811, -0.1935],\n",
      "        [ 0.0246,  0.9997],\n",
      "        [-0.9912, -0.1323]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2123],\n",
      "        [2.7364],\n",
      "        [8.0336]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 378, Loss: 0.6281423844286994\n",
      "tensor([[ 0.9811, -0.1934],\n",
      "        [ 0.0257,  0.9997],\n",
      "        [-0.9912, -0.1325]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2171],\n",
      "        [2.7403],\n",
      "        [8.0362]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 379, Loss: 0.6414384794246335\n",
      "tensor([[ 0.9811, -0.1933],\n",
      "        [ 0.0269,  0.9996],\n",
      "        [-0.9912, -0.1325]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2219],\n",
      "        [2.7442],\n",
      "        [8.0388]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 380, Loss: 0.6389917974450152\n",
      "tensor([[ 0.9811, -0.1933],\n",
      "        [ 0.0280,  0.9996],\n",
      "        [-0.9912, -0.1325]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2267],\n",
      "        [2.7481],\n",
      "        [8.0414]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 381, Loss: 0.6436444830948987\n",
      "tensor([[ 0.9812, -0.1931],\n",
      "        [ 0.0291,  0.9996],\n",
      "        [-0.9912, -0.1325]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2316],\n",
      "        [2.7520],\n",
      "        [8.0440]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 382, Loss: 0.6407856895266942\n",
      "tensor([[ 0.9812, -0.1930],\n",
      "        [ 0.0302,  0.9995],\n",
      "        [-0.9912, -0.1326]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2364],\n",
      "        [2.7559],\n",
      "        [8.0467]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 383, Loss: 0.6277234748314849\n",
      "tensor([[ 0.9812, -0.1927],\n",
      "        [ 0.0312,  0.9995],\n",
      "        [-0.9912, -0.1326]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2413],\n",
      "        [2.7598],\n",
      "        [8.0494]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 384, Loss: 0.6377754225222362\n",
      "tensor([[ 0.9813, -0.1924],\n",
      "        [ 0.0323,  0.9995],\n",
      "        [-0.9912, -0.1326]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2462],\n",
      "        [2.7637],\n",
      "        [8.0520]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 385, Loss: 0.6328456920718268\n",
      "tensor([[ 0.9814, -0.1922],\n",
      "        [ 0.0333,  0.9994],\n",
      "        [-0.9912, -0.1327]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2510],\n",
      "        [2.7676],\n",
      "        [8.0547]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 386, Loss: 0.6354764311262866\n",
      "tensor([[ 0.9814, -0.1919],\n",
      "        [ 0.0343,  0.9994],\n",
      "        [-0.9911, -0.1328]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2558],\n",
      "        [2.7716],\n",
      "        [8.0575]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 387, Loss: 0.6207843631237729\n",
      "tensor([[ 0.9815, -0.1917],\n",
      "        [ 0.0353,  0.9994],\n",
      "        [-0.9911, -0.1329]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2607],\n",
      "        [2.7755],\n",
      "        [8.0603]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 388, Loss: 0.6370282002174601\n",
      "tensor([[ 0.9815, -0.1915],\n",
      "        [ 0.0363,  0.9993],\n",
      "        [-0.9911, -0.1330]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2655],\n",
      "        [2.7794],\n",
      "        [8.0629]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 389, Loss: 0.6407781124538022\n",
      "tensor([[ 0.9815, -0.1913],\n",
      "        [ 0.0373,  0.9993],\n",
      "        [-0.9911, -0.1331]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2703],\n",
      "        [2.7833],\n",
      "        [8.0657]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 390, Loss: 0.6298218723802866\n",
      "tensor([[ 0.9815, -0.1912],\n",
      "        [ 0.0383,  0.9993],\n",
      "        [-0.9911, -0.1332]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2751],\n",
      "        [2.7872],\n",
      "        [8.0684]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 391, Loss: 0.6312477444526505\n",
      "tensor([[ 0.9815, -0.1912],\n",
      "        [ 0.0393,  0.9992],\n",
      "        [-0.9911, -0.1333]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2800],\n",
      "        [2.7911],\n",
      "        [8.0712]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 392, Loss: 0.6314680953553696\n",
      "tensor([[ 0.9815, -0.1913],\n",
      "        [ 0.0403,  0.9992],\n",
      "        [-0.9911, -0.1333]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2849],\n",
      "        [2.7950],\n",
      "        [8.0739]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 393, Loss: 0.6306752383165016\n",
      "tensor([[ 0.9815, -0.1912],\n",
      "        [ 0.0413,  0.9991],\n",
      "        [-0.9911, -0.1335]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2899],\n",
      "        [2.7988],\n",
      "        [8.0767]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 394, Loss: 0.6349883627153792\n",
      "tensor([[ 0.9815, -0.1913],\n",
      "        [ 0.0424,  0.9991],\n",
      "        [-0.9910, -0.1336]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2948],\n",
      "        [2.8027],\n",
      "        [8.0794]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 395, Loss: 0.6376233256624059\n",
      "tensor([[ 0.9815, -0.1914],\n",
      "        [ 0.0434,  0.9991],\n",
      "        [-0.9910, -0.1337]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2997],\n",
      "        [2.8066],\n",
      "        [8.0821]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 396, Loss: 0.6396547514694206\n",
      "tensor([[ 0.9815, -0.1915],\n",
      "        [ 0.0445,  0.9990],\n",
      "        [-0.9910, -0.1338]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3047],\n",
      "        [2.8105],\n",
      "        [8.0847]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 397, Loss: 0.6334687351265128\n",
      "tensor([[ 0.9815, -0.1916],\n",
      "        [ 0.0455,  0.9990],\n",
      "        [-0.9910, -0.1339]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3096],\n",
      "        [2.8143],\n",
      "        [8.0873]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 398, Loss: 0.645994241934645\n",
      "tensor([[ 0.9814, -0.1917],\n",
      "        [ 0.0465,  0.9989],\n",
      "        [-0.9910, -0.1339]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3145],\n",
      "        [2.8182],\n",
      "        [8.0899]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 399, Loss: 0.6404580880905465\n",
      "tensor([[ 0.9814, -0.1919],\n",
      "        [ 0.0475,  0.9989],\n",
      "        [-0.9910, -0.1340]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3194],\n",
      "        [2.8220],\n",
      "        [8.0925]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 400, Loss: 0.6367713419960978\n",
      "tensor([[ 0.9814, -0.1920],\n",
      "        [ 0.0486,  0.9988],\n",
      "        [-0.9910, -0.1340]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3242],\n",
      "        [2.8259],\n",
      "        [8.0951]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 401, Loss: 0.636698132405498\n",
      "tensor([[ 0.9814, -0.1921],\n",
      "        [ 0.0496,  0.9988],\n",
      "        [-0.9910, -0.1339]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3291],\n",
      "        [2.8298],\n",
      "        [8.0977]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 402, Loss: 0.6384707334626014\n",
      "tensor([[ 0.9813, -0.1923],\n",
      "        [ 0.0506,  0.9987],\n",
      "        [-0.9910, -0.1337]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3340],\n",
      "        [2.8336],\n",
      "        [8.1002]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 403, Loss: 0.6459167249831705\n",
      "tensor([[ 0.9813, -0.1924],\n",
      "        [ 0.0515,  0.9987],\n",
      "        [-0.9910, -0.1336]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3389],\n",
      "        [2.8375],\n",
      "        [8.1027]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 404, Loss: 0.6279449854430394\n",
      "tensor([[ 0.9813, -0.1926],\n",
      "        [ 0.0525,  0.9986],\n",
      "        [-0.9911, -0.1335]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3438],\n",
      "        [2.8414],\n",
      "        [8.1052]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 405, Loss: 0.6431476514095097\n",
      "tensor([[ 0.9813, -0.1926],\n",
      "        [ 0.0535,  0.9986],\n",
      "        [-0.9911, -0.1333]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3487],\n",
      "        [2.8452],\n",
      "        [8.1078]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 406, Loss: 0.6292770681289575\n",
      "tensor([[ 0.9812, -0.1928],\n",
      "        [ 0.0544,  0.9985],\n",
      "        [-0.9911, -0.1333]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3535],\n",
      "        [2.8491],\n",
      "        [8.1105]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 407, Loss: 0.6130827461978025\n",
      "tensor([[ 0.9812, -0.1929],\n",
      "        [ 0.0553,  0.9985],\n",
      "        [-0.9911, -0.1331]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3584],\n",
      "        [2.8529],\n",
      "        [8.1132]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 408, Loss: 0.6291675582194184\n",
      "tensor([[ 0.9812, -0.1928],\n",
      "        [ 0.0562,  0.9984],\n",
      "        [-0.9911, -0.1330]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3634],\n",
      "        [2.8568],\n",
      "        [8.1159]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 409, Loss: 0.6321326464338343\n",
      "tensor([[ 0.9812, -0.1928],\n",
      "        [ 0.0571,  0.9984],\n",
      "        [-0.9911, -0.1329]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3683],\n",
      "        [2.8607],\n",
      "        [8.1186]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 410, Loss: 0.623065036343258\n",
      "tensor([[ 0.9813, -0.1927],\n",
      "        [ 0.0579,  0.9983],\n",
      "        [-0.9911, -0.1328]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3731],\n",
      "        [2.8646],\n",
      "        [8.1213]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 411, Loss: 0.6429673063574392\n",
      "tensor([[ 0.9813, -0.1927],\n",
      "        [ 0.0587,  0.9983],\n",
      "        [-0.9912, -0.1327]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3780],\n",
      "        [2.8684],\n",
      "        [8.1240]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 412, Loss: 0.6242803433823034\n",
      "tensor([[ 0.9813, -0.1927],\n",
      "        [ 0.0596,  0.9982],\n",
      "        [-0.9912, -0.1326]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3828],\n",
      "        [2.8723],\n",
      "        [8.1267]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 413, Loss: 0.631965978951274\n",
      "tensor([[ 0.9813, -0.1927],\n",
      "        [ 0.0604,  0.9982],\n",
      "        [-0.9912, -0.1325]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3876],\n",
      "        [2.8762],\n",
      "        [8.1294]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 414, Loss: 0.6330364300396617\n",
      "tensor([[ 0.9812, -0.1928],\n",
      "        [ 0.0612,  0.9981],\n",
      "        [-0.9912, -0.1325]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3924],\n",
      "        [2.8800],\n",
      "        [8.1321]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 415, Loss: 0.647175383482363\n",
      "tensor([[ 0.9812, -0.1928],\n",
      "        [ 0.0620,  0.9981],\n",
      "        [-0.9912, -0.1325]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3971],\n",
      "        [2.8839],\n",
      "        [8.1347]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 416, Loss: 0.6309556590399855\n",
      "tensor([[ 0.9812, -0.1929],\n",
      "        [ 0.0627,  0.9980],\n",
      "        [-0.9912, -0.1327]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4019],\n",
      "        [2.8877],\n",
      "        [8.1373]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 417, Loss: 0.6261604445923912\n",
      "tensor([[ 0.9812, -0.1929],\n",
      "        [ 0.0635,  0.9980],\n",
      "        [-0.9912, -0.1327]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4067],\n",
      "        [2.8916],\n",
      "        [8.1399]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 418, Loss: 0.6284797505011105\n",
      "tensor([[ 0.9812, -0.1928],\n",
      "        [ 0.0643,  0.9979],\n",
      "        [-0.9911, -0.1328]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4115],\n",
      "        [2.8954],\n",
      "        [8.1425]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 419, Loss: 0.6423746127598688\n",
      "tensor([[ 0.9813, -0.1927],\n",
      "        [ 0.0650,  0.9979],\n",
      "        [-0.9911, -0.1330]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4163],\n",
      "        [2.8993],\n",
      "        [8.1450]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 420, Loss: 0.6270973978762331\n",
      "tensor([[ 0.9813, -0.1925],\n",
      "        [ 0.0658,  0.9978],\n",
      "        [-0.9911, -0.1332]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4212],\n",
      "        [2.9032],\n",
      "        [8.1476]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 421, Loss: 0.6112144982477278\n",
      "tensor([[ 0.9813, -0.1924],\n",
      "        [ 0.0665,  0.9978],\n",
      "        [-0.9911, -0.1332]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4261],\n",
      "        [2.9070],\n",
      "        [8.1501]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 422, Loss: 0.6231654321909136\n",
      "tensor([[ 0.9813, -0.1923],\n",
      "        [ 0.0673,  0.9977],\n",
      "        [-0.9911, -0.1334]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4310],\n",
      "        [2.9109],\n",
      "        [8.1528]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 423, Loss: 0.6234897855154613\n",
      "tensor([[ 0.9813, -0.1922],\n",
      "        [ 0.0680,  0.9977],\n",
      "        [-0.9910, -0.1335]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4359],\n",
      "        [2.9147],\n",
      "        [8.1554]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 424, Loss: 0.6293185601118412\n",
      "tensor([[ 0.9814, -0.1921],\n",
      "        [ 0.0688,  0.9976],\n",
      "        [-0.9910, -0.1337]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4407],\n",
      "        [2.9186],\n",
      "        [8.1580]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 425, Loss: 0.6302852802145932\n",
      "tensor([[ 0.9814, -0.1920],\n",
      "        [ 0.0695,  0.9976],\n",
      "        [-0.9910, -0.1339]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4456],\n",
      "        [2.9225],\n",
      "        [8.1606]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 426, Loss: 0.6267352046851543\n",
      "tensor([[ 0.9814, -0.1919],\n",
      "        [ 0.0702,  0.9975],\n",
      "        [-0.9910, -0.1340]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4505],\n",
      "        [2.9263],\n",
      "        [8.1631]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 427, Loss: 0.6292826662619312\n",
      "tensor([[ 0.9814, -0.1918],\n",
      "        [ 0.0708,  0.9975],\n",
      "        [-0.9910, -0.1340]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4554],\n",
      "        [2.9302],\n",
      "        [8.1657]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 428, Loss: 0.6289880775047968\n",
      "tensor([[ 0.9815, -0.1917],\n",
      "        [ 0.0715,  0.9974],\n",
      "        [-0.9910, -0.1340]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4602],\n",
      "        [2.9340],\n",
      "        [8.1683]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 429, Loss: 0.6228415358136418\n",
      "tensor([[ 0.9815, -0.1917],\n",
      "        [ 0.0722,  0.9974],\n",
      "        [-0.9910, -0.1339]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4651],\n",
      "        [2.9379],\n",
      "        [8.1709]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 430, Loss: 0.6393398072740147\n",
      "tensor([[ 0.9815, -0.1916],\n",
      "        [ 0.0728,  0.9973],\n",
      "        [-0.9910, -0.1340]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4699],\n",
      "        [2.9417],\n",
      "        [8.1735]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 431, Loss: 0.6338055130589199\n",
      "tensor([[ 0.9815, -0.1915],\n",
      "        [ 0.0734,  0.9973],\n",
      "        [-0.9910, -0.1340]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4748],\n",
      "        [2.9456],\n",
      "        [8.1761]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 432, Loss: 0.6251271600207897\n",
      "tensor([[ 0.9815, -0.1914],\n",
      "        [ 0.0740,  0.9973],\n",
      "        [-0.9910, -0.1341]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4797],\n",
      "        [2.9494],\n",
      "        [8.1787]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 433, Loss: 0.6269608269837461\n",
      "tensor([[ 0.9815, -0.1914],\n",
      "        [ 0.0746,  0.9972],\n",
      "        [-0.9910, -0.1342]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4845],\n",
      "        [2.9532],\n",
      "        [8.1813]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 434, Loss: 0.626594575084193\n",
      "tensor([[ 0.9815, -0.1912],\n",
      "        [ 0.0751,  0.9972],\n",
      "        [-0.9910, -0.1342]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4894],\n",
      "        [2.9571],\n",
      "        [8.1839]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 435, Loss: 0.6249401952712702\n",
      "tensor([[ 0.9816, -0.1911],\n",
      "        [ 0.0756,  0.9971],\n",
      "        [-0.9909, -0.1343]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4943],\n",
      "        [2.9609],\n",
      "        [8.1865]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 436, Loss: 0.6166038620573299\n",
      "tensor([[ 0.9816, -0.1909],\n",
      "        [ 0.0760,  0.9971],\n",
      "        [-0.9909, -0.1345]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4992],\n",
      "        [2.9647],\n",
      "        [8.1891]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 437, Loss: 0.6221017231501487\n",
      "tensor([[ 0.9816, -0.1909],\n",
      "        [ 0.0765,  0.9971],\n",
      "        [-0.9909, -0.1347]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5041],\n",
      "        [2.9686],\n",
      "        [8.1918]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 438, Loss: 0.6140749694869893\n",
      "tensor([[ 0.9816, -0.1907],\n",
      "        [ 0.0770,  0.9970],\n",
      "        [-0.9909, -0.1349]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5090],\n",
      "        [2.9724],\n",
      "        [8.1944]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 439, Loss: 0.6324116650674891\n",
      "tensor([[ 0.9817, -0.1905],\n",
      "        [ 0.0775,  0.9970],\n",
      "        [-0.9908, -0.1350]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5139],\n",
      "        [2.9762],\n",
      "        [8.1969]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 440, Loss: 0.6334649421685875\n",
      "tensor([[ 0.9817, -0.1903],\n",
      "        [ 0.0780,  0.9970],\n",
      "        [-0.9908, -0.1352]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5188],\n",
      "        [2.9800],\n",
      "        [8.1995]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 441, Loss: 0.6313822900379937\n",
      "tensor([[ 0.9818, -0.1901],\n",
      "        [ 0.0785,  0.9969],\n",
      "        [-0.9908, -0.1355]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5237],\n",
      "        [2.9838],\n",
      "        [8.2021]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 442, Loss: 0.6106970128559371\n",
      "tensor([[ 0.9818, -0.1900],\n",
      "        [ 0.0790,  0.9969],\n",
      "        [-0.9908, -0.1356]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5286],\n",
      "        [2.9876],\n",
      "        [8.2047]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 443, Loss: 0.6334947010884092\n",
      "tensor([[ 0.9818, -0.1900],\n",
      "        [ 0.0795,  0.9968],\n",
      "        [-0.9907, -0.1357]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5336],\n",
      "        [2.9915],\n",
      "        [8.2074]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 444, Loss: 0.6128037019102208\n",
      "tensor([[ 0.9818, -0.1899],\n",
      "        [ 0.0801,  0.9968],\n",
      "        [-0.9907, -0.1358]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5386],\n",
      "        [2.9953],\n",
      "        [8.2100]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 445, Loss: 0.6335722464314438\n",
      "tensor([[ 0.9818, -0.1898],\n",
      "        [ 0.0806,  0.9967],\n",
      "        [-0.9907, -0.1361]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5434],\n",
      "        [2.9990],\n",
      "        [8.2125]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 446, Loss: 0.6401000530612607\n",
      "tensor([[ 0.9818, -0.1897],\n",
      "        [ 0.0811,  0.9967],\n",
      "        [-0.9907, -0.1363]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5482],\n",
      "        [3.0028],\n",
      "        [8.2151]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 447, Loss: 0.6251650429638793\n",
      "tensor([[ 0.9818, -0.1897],\n",
      "        [ 0.0817,  0.9967],\n",
      "        [-0.9907, -0.1363]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5531],\n",
      "        [3.0066],\n",
      "        [8.2176]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 448, Loss: 0.621776638365278\n",
      "tensor([[ 0.9818, -0.1897],\n",
      "        [ 0.0822,  0.9966],\n",
      "        [-0.9906, -0.1365]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5580],\n",
      "        [3.0104],\n",
      "        [8.2202]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 449, Loss: 0.6112642756023308\n"
     ]
    }
   ],
   "source": [
    "import geotorch\n",
    "\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "\n",
    "class GalleryParams(torch.nn.Module):\n",
    "    def __init__(self, init_mean, init_kappa):\n",
    "        super(GalleryParams, self).__init__()\n",
    "        # self.gallery_means = torch.nn.Parameter(torch.tensor(init_mean))\n",
    "        # self.gallery_kappas =  torch.nn.Parameter(torch.tensor(init_kappa))\n",
    "        self.gallery_means = torch.nn.Parameter(torch.rand(3, 2, dtype=torch.float64))\n",
    "        self.gallery_kappas = torch.nn.Parameter(\n",
    "            torch.rand(3, 1, dtype=torch.float64) * 10\n",
    "        )\n",
    "\n",
    "\n",
    "gallery_params = GalleryParams(init_mean, init_kappa)\n",
    "target_class = torch.tensor(gallery_subject_ids_sorted)\n",
    "T = torch.nn.Parameter(torch.tensor(1.0))\n",
    "geotorch.sphere(gallery_params, \"gallery_means\")\n",
    "\n",
    "\n",
    "# train\n",
    "M = 5\n",
    "mc_prob = MonteCarloPredictiveProb(M=M)\n",
    "\n",
    "# optimizer = torch.optim.Adam([gallery_means, gallery_kappas], lr=10e-3)\n",
    "optimizer = torch.optim.Adam(gallery_params.parameters(), lr=0.01)\n",
    "\n",
    "num_steps = 450\n",
    "\n",
    "for iter in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    # compute nll loss\n",
    "    log_probs = mc_prob(\n",
    "        gallery_features,\n",
    "        gallery_unc,\n",
    "        gallery_params.gallery_means,\n",
    "        gallery_params.gallery_kappas,\n",
    "        T,\n",
    "    )[:, :, :-1]\n",
    "    probs = torch.exp(log_probs)\n",
    "    mean_probs = torch.mean(probs, axis=1)\n",
    "    log_probs_new = torch.log(mean_probs)\n",
    "    loss = nll_loss(log_probs_new, target_class)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(gallery_params.gallery_means)\n",
    "    print(torch.norm(gallery_params.gallery_means, dim=-1))\n",
    "    print(gallery_params.gallery_kappas)\n",
    "    print(f\"Iteration {iter}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fontsize = 20\n",
    "linewidth = 3\n",
    "dot_size = 80\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "draw_circle(ax, linewidth)\n",
    "for class_id, (class_mean, kappa) in enumerate(\n",
    "    zip(gallery_params.gallery_means, gallery_params.gallery_kappas)\n",
    "):\n",
    "    color = colors[class_id]\n",
    "    class_mean = class_mean.detach().numpy()\n",
    "    kappa = kappa.detach().numpy()\n",
    "    class_point_angle = np.angle([class_mean[0] + 1j * class_mean[1]])[0]\n",
    "    draw_vmf_dencity(\n",
    "        class_point_angle,\n",
    "        kappa,\n",
    "        ax,\n",
    "        linewidth=3,\n",
    "        color=color,\n",
    "        range=np.pi / 2,\n",
    "        draw_center=True,\n",
    "        dot_size=dot_size,\n",
    "    )\n",
    "    for position in np.where(gallery_subject_ids_sorted == class_id)[0]:\n",
    "        point_angle = np.angle(\n",
    "            [gallery_features[position][0] + 1j * gallery_features[position][1]]\n",
    "        )[0]\n",
    "        draw_vmf_dencity(\n",
    "            point_angle,\n",
    "            gallery_unc[position],\n",
    "            ax,\n",
    "            linewidth=1,\n",
    "            color=color,\n",
    "            range=np.pi / 2,\n",
    "            scale=0.1,\n",
    "            draw_center=True,\n",
    "            dot_size=20,\n",
    "        )\n",
    "fig.gca().set_aspect(\"equal\")\n",
    "fig.show()\n",
    "plt.savefig(\"/app/outputs/images/trained.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
