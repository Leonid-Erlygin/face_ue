{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "        p(\\textbf{z}|c) &= \\mathcal{C}_d(\\kappa)\\exp\\left(\\kappa\\mu^T_{c}\\textbf{z}\\right)\\\\\n",
    "        \\mathcal{C}_d(\\kappa) &= \\frac{(\\kappa)^{d/2-1}}{(2\\pi)^{d/2}\\mathcal{I}_{d/2-1}(\\kappa)}\n",
    "\\end{align}\n",
    "\n",
    "With d=2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import iv, gamma\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# mpl.style.use('classic')\n",
    "\n",
    "\n",
    "def z_Prob(z, mus, kappa, d=2, beta=0.5):\n",
    "    K = mus.shape[-1]\n",
    "    p_c = (1 - beta) / K\n",
    "    class_probs = np.array([z_vonMises_dencity(z, mu, kappa) for mu in mus.T])\n",
    "    return np.sum(class_probs * p_c) + (1 / (2 * np.pi)) * beta\n",
    "\n",
    "\n",
    "def z_class_prob(class_id, z, mus, kappa, d=2, beta=0.5):\n",
    "    p_z = z_Prob(z, mus, kappa, d, beta)\n",
    "    K = mus.shape[1]\n",
    "    if class_id == K:\n",
    "        p_c = beta\n",
    "        return (1 / (2 * np.pi)) * p_c / p_z\n",
    "    else:\n",
    "        p_c = (1 - beta) / K\n",
    "        return (z_vonMises_dencity(z, mus[:, class_id], kappa) * p_c) / p_z\n",
    "\n",
    "\n",
    "def z_vonMises_dencity(z, mu_c, kappa, d=2):\n",
    "    C_d = kappa ** (d / 2 - 1) / ((2 * np.pi) ** (d / 2) * iv(d / 2 - 1, kappa))\n",
    "    return C_d * np.exp(kappa * np.dot(z, mu_c))\n",
    "\n",
    "\n",
    "def z_power_dencity(z, mu_c, kappa, d=2):\n",
    "    alpha = (d - 1) / 2 + kappa\n",
    "    beta = (d - 1) / 2\n",
    "    M_d = gamma(alpha + beta) / (2 ** (alpha + beta) * np.pi**beta * gamma(alpha))\n",
    "    return M_d * (1 + np.dot(z, mu_c)) ** kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_by_angle(angles):\n",
    "    return np.array([[np.cos(plot_angle), np.sin(plot_angle)] for plot_angle in angles])\n",
    "\n",
    "\n",
    "def compute_class_probs(class_id, zs, mus, kappa, beta):\n",
    "    class_probes = []\n",
    "    for z in zs:\n",
    "        class_prob = z_class_prob(class_id, z, mus, kappa, beta=beta)\n",
    "        class_probes.append(class_prob)\n",
    "    class_probes = np.array(class_probes)\n",
    "    return class_probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_circle(ax, linewidth, zorder=4):\n",
    "    # plot circle\n",
    "    theta = np.linspace(0, 2 * np.pi, 150)\n",
    "    a = np.cos(theta)\n",
    "    b = np.sin(theta)\n",
    "    circle = plt.Circle((0, 0), 1, color=\"blue\", zorder=4, alpha=0.1)\n",
    "    ax.add_patch(circle)\n",
    "    ax.plot(a, b, color=\"tab:gray\", zorder=4, linewidth=linewidth)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "\n",
    "def draw_decity(ax):\n",
    "    pass\n",
    "\n",
    "\n",
    "def draw_example(kappa, gallery_class_angles, text_shift, save_name, beta=0.5):\n",
    "    fontsize = 20\n",
    "    linewidth = 3\n",
    "    dot_size = 80\n",
    "\n",
    "    test_color = \"tab:cyan\"\n",
    "    # gallery_class_angles = [0.4, 0.25]\n",
    "    ident_uncertain_test_point = (\n",
    "        gallery_class_angles[-1] + gallery_class_angles[-2]\n",
    "    ) / 2 - 0.02\n",
    "    test_points_angles = [\n",
    "        ident_uncertain_test_point,\n",
    "        gallery_class_angles[-1]\n",
    "        - (ident_uncertain_test_point - gallery_class_angles[-1]),\n",
    "    ]\n",
    "\n",
    "    gallery_class_angles = np.array(gallery_class_angles) * 2 * np.pi\n",
    "    test_points_angles = np.array(test_points_angles) * 2 * np.pi\n",
    "    theta = np.linspace(0, 2 * np.pi, 150)\n",
    "\n",
    "    colors = list(mcolors.TABLEAU_COLORS)[: len(gallery_class_angles)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    draw_circle(ax, linewidth)\n",
    "\n",
    "    draw_dencity_angles = np.linspace(-np.pi / 3, np.pi / 3, 150)\n",
    "    mus = np.stack([np.cos(gallery_class_angles), np.sin(gallery_class_angles)], axis=0)\n",
    "    class_to_class_probs = []\n",
    "\n",
    "    for i, (angle, color) in enumerate(zip(gallery_class_angles, colors)):\n",
    "        mu_c = mus[:, i]\n",
    "        plot_angles = angle + draw_dencity_angles\n",
    "        zs = get_vectors_by_angle(plot_angles)\n",
    "        class_probes = compute_class_probs(i, zs, mus, kappa, beta)\n",
    "        v = zs.T * (1 + class_probes[np.newaxis, :])\n",
    "        ax.plot(v[0], v[1], c=color, linewidth=linewidth)\n",
    "\n",
    "        ax.scatter([np.cos(angle)], [np.sin(angle)], c=color, s=dot_size, zorder=5)\n",
    "        ax.scatter([0], [0], color=\"black\", s=20)\n",
    "\n",
    "        # plot decity\n",
    "        # ax.scatter(points[:, 0], points[:, 1], color=color, s=3)\n",
    "\n",
    "    # plot_uniform_prob\n",
    "    zs = get_vectors_by_angle(theta)\n",
    "    class_probes = compute_class_probs(mus.shape[1], zs, mus, kappa, beta)\n",
    "    v = zs.T * (1 + class_probes[np.newaxis, :])\n",
    "    # ax.plot(v[0], v[1], color='black')\n",
    "\n",
    "    # plot unc\n",
    "    all_probs = []\n",
    "    for i in range(mus.shape[1] + 1):\n",
    "        class_probes = compute_class_probs(i, zs, mus, kappa, beta)\n",
    "        all_probs.append(class_probes)\n",
    "    all_probs = np.stack(all_probs, axis=0)\n",
    "    unc = -np.sum(all_probs * np.log(all_probs), axis=0)\n",
    "    # unc = -np.max(all_probs, axis=0) + 1\n",
    "    v = zs.T * (1 + unc[np.newaxis, :])\n",
    "    ax.plot(v[0], v[1], color=\"tab:red\", linewidth=linewidth)\n",
    "\n",
    "    # plot test points\n",
    "    for test_angle in test_points_angles:\n",
    "        ax.scatter(\n",
    "            [np.cos(test_angle)],\n",
    "            [np.sin(test_angle)],\n",
    "            c=test_color,\n",
    "            s=dot_size,\n",
    "            zorder=5,\n",
    "        )\n",
    "\n",
    "    test_point_vectors = get_vectors_by_angle(test_points_angles)\n",
    "    # entropy value\n",
    "\n",
    "    probs_at_test_points = []\n",
    "    for i in range(mus.shape[1] + 1):\n",
    "        class_probes = compute_class_probs(i, test_point_vectors, mus, kappa, beta)\n",
    "        probs_at_test_points.append(class_probes)\n",
    "    probs_at_test_points = np.stack(probs_at_test_points, axis=0)\n",
    "\n",
    "    unc_test = -np.sum(probs_at_test_points * np.log(probs_at_test_points), axis=0)\n",
    "    unc_test = np.round(unc_test, 2)\n",
    "    # unc_test = -np.max(probs_at_test_points, axis=0) + 1\n",
    "    # unc_test = np.round(unc_test, 2)\n",
    "    ax.annotate(\n",
    "        f\"${unc_test[0]}$\",\n",
    "        xy=test_point_vectors[0],\n",
    "        xytext=[\n",
    "            test_point_vectors[0][0] + text_shift[0][0],\n",
    "            test_point_vectors[0][1] + text_shift[0][1],\n",
    "        ],\n",
    "        fontsize=fontsize,\n",
    "    )\n",
    "    ax.annotate(\n",
    "        f\"${unc_test[1]}$\",\n",
    "        xy=test_point_vectors[1],\n",
    "        xytext=[\n",
    "            test_point_vectors[1][0] + text_shift[1][0],\n",
    "            test_point_vectors[1][1] + text_shift[1][1],\n",
    "        ],\n",
    "        fontsize=fontsize,\n",
    "    )\n",
    "    fig.gca().set_aspect(\"equal\")\n",
    "    plt.savefig(save_name, dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_example(\n",
    "#     kappa=15,\n",
    "#     gallery_class_angles=[0.4, 0.25],\n",
    "#     text_shift=[[-0.2, -0.3], [-0.2, 0.1]],\n",
    "#     save_name=\"test.png\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Identification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_example(kappa = 15, gallery_class_angles = [0.4, 0.25], text_shift = [[-0.2, -0.3], [-0.2, 0.1]], save_name='/app/paper_assets/images/false_ident_example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False accept/reject example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_example(kappa = 13, gallery_class_angles = [0.48, 0.25], text_shift = [[-0.1, -0.3], [-0.35, 0.2]], save_name='/app/paper_assets/images/false_accept-reject_example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_dencity(\n",
    "    angle,\n",
    "    kappa,\n",
    "    ax,\n",
    "    linewidth,\n",
    "    color,\n",
    "    range=np.pi / 3,\n",
    "    scale=1,\n",
    "    draw_center=False,\n",
    "    dot_size=40,\n",
    "    type=\"vMF\",\n",
    "):\n",
    "    assert type in [\"vMF\", \"power\"]\n",
    "    draw_dencity_angles = np.linspace(-range, range, 150)\n",
    "    plot_angles = angle + draw_dencity_angles\n",
    "    zs = get_vectors_by_angle(plot_angles)\n",
    "    mu = get_vectors_by_angle([angle])[0]\n",
    "    if type == \"vMF\":\n",
    "        dencities = z_vonMises_dencity(zs, mu, kappa)\n",
    "    else:\n",
    "        dencities = z_power_dencity(zs, mu, kappa)\n",
    "    v = zs.T * (1 + dencities * scale)\n",
    "    ax.plot(v[0], v[1], c=color, linewidth=linewidth)\n",
    "    if draw_center:\n",
    "        ax.scatter([np.cos(angle)], [np.sin(angle)], c=color, s=dot_size, zorder=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.samplers import random_VMF, VonMisesFisher\n",
    "\n",
    "gallery_features = []  # N gallery samples X 512\n",
    "gallery_unc_log = []  # N gallery samples X 1\n",
    "gallery_subject_ids_sorted = []  # N gallery samples\n",
    "\n",
    "\n",
    "class_center_angles = np.array([0, np.pi / 2, np.pi])\n",
    "class_z_kappa = np.array([9, 6, 5])\n",
    "colors = list(mcolors.TABLEAU_COLORS)[: len(class_center_angles)]\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(2)\n",
    "\n",
    "for class_id, (angle, kappa) in enumerate(zip(class_center_angles, class_z_kappa)):\n",
    "    num_samples = rng.integers(5, 8)\n",
    "    gallery_subject_ids_sorted.extend([class_id] * num_samples)\n",
    "    gallery_unc_log.extend(4 * rng.random(num_samples) + 2)\n",
    "    samples = random_VMF(\n",
    "        get_vectors_by_angle([angle])[0], kappa=kappa, size=num_samples\n",
    "    )\n",
    "    gallery_features.extend(samples)\n",
    "\n",
    "gallery_features = np.array(gallery_features)\n",
    "gallery_unc_log = np.array(gallery_unc_log).reshape(-1, 1)\n",
    "gallery_subject_ids_sorted = np.array(gallery_subject_ids_sorted)\n",
    "\n",
    "gallery_unc = np.exp(gallery_unc_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19, 2), (19,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gallery_features.shape, gallery_subject_ids_sorted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = VonMisesFisher(3)\n",
    "feature_mean = sampler(np.array([[0, 1], [1, 0]]), np.array([[10], [15]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fontsize = 20\n",
    "linewidth = 3\n",
    "dot_size = 80\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "draw_circle(ax, linewidth)\n",
    "for class_id, (angle, kappa) in enumerate(zip(class_center_angles, class_z_kappa)):\n",
    "    color = colors[class_id]\n",
    "    # draw_dencity(\n",
    "    #     angle,\n",
    "    #     kappa,\n",
    "    #     ax,\n",
    "    #     linewidth=3,\n",
    "    #     color=color,\n",
    "    #     range=np.pi / 2,\n",
    "    #     draw_center=True,\n",
    "    #     dot_size=dot_size,\n",
    "    # )\n",
    "    for position in np.where(gallery_subject_ids_sorted == class_id)[0]:\n",
    "        point_angle = np.angle(\n",
    "            [gallery_features[position][0] + 1j * gallery_features[position][1]]\n",
    "        )[0]\n",
    "        draw_dencity(\n",
    "            point_angle,\n",
    "            gallery_unc[position],\n",
    "            ax,\n",
    "            linewidth=1,\n",
    "            color=color,\n",
    "            range=np.pi / 2,\n",
    "            scale=0.1,\n",
    "            draw_center=True,\n",
    "            dot_size=20,\n",
    "        )\n",
    "fig.gca().set_aspect(\"equal\")\n",
    "fig.show()\n",
    "plt.savefig(\"/app/outputs/images/generation.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "gallery_features.shape\n",
    "init_mean = np.array(\n",
    "    [\n",
    "        np.mean(gallery_features[gallery_subject_ids_sorted == c], axis=0)\n",
    "        for c in range(3)\n",
    "    ]\n",
    ")\n",
    "# init_mean = init_mean / np.linalg.norm(init_mean, axis=1, keepdims=True)\n",
    "\n",
    "init_kappa = np.array(\n",
    "    [np.mean(gallery_unc[gallery_subject_ids_sorted == c], axis=0) for c in range(3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19, 2), (19, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gallery_features.shape, gallery_unc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.open_set_methods.class_prob_models import MonteCarloPredictiveProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4201, -0.9075],\n",
      "        [ 0.9729,  0.2314],\n",
      "        [ 0.9161,  0.4009]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.3080],\n",
      "        [4.2513],\n",
      "        [3.9996]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 0, Loss: 5.511763690956834\n",
      "tensor([[ 0.5295, -0.8483],\n",
      "        [ 0.9414,  0.3373],\n",
      "        [ 0.8607,  0.5091]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.2158],\n",
      "        [4.1533],\n",
      "        [3.8996]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 1, Loss: 5.319564462862632\n",
      "tensor([[ 0.6284, -0.7779],\n",
      "        [ 0.8987,  0.4387],\n",
      "        [ 0.7926,  0.6098]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.1451],\n",
      "        [4.0558],\n",
      "        [3.8004]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 2, Loss: 4.6186182262736075\n",
      "tensor([[ 0.7152, -0.6989],\n",
      "        [ 0.8456,  0.5339],\n",
      "        [ 0.7175,  0.6966]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.1134],\n",
      "        [3.9612],\n",
      "        [3.7014]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 3, Loss: 4.468364403265231\n",
      "tensor([[ 0.7893, -0.6140],\n",
      "        [ 0.7836,  0.6213],\n",
      "        [ 0.6860,  0.7276]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.1161],\n",
      "        [3.8708],\n",
      "        [3.6029]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 4, Loss: 4.214668349210063\n",
      "tensor([[ 0.8510, -0.5251],\n",
      "        [ 0.7141,  0.7001],\n",
      "        [ 0.6294,  0.7771]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.1434],\n",
      "        [3.7847],\n",
      "        [3.5047]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 5, Loss: 3.916654595770572\n",
      "tensor([[ 0.9005, -0.4349],\n",
      "        [ 0.6388,  0.7694],\n",
      "        [ 0.5576,  0.8301]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.1894],\n",
      "        [3.7049],\n",
      "        [3.4074]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 6, Loss: 3.5210782561220753\n",
      "tensor([[ 0.9382, -0.3460],\n",
      "        [ 0.5597,  0.8287],\n",
      "        [ 0.4758,  0.8795]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.2483],\n",
      "        [3.6338],\n",
      "        [3.3119]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 7, Loss: 2.9833142025459893\n",
      "tensor([[ 0.9655, -0.2603],\n",
      "        [ 0.4792,  0.8777],\n",
      "        [ 0.3871,  0.9221]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.3158],\n",
      "        [3.5712],\n",
      "        [3.2187]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 8, Loss: 2.639498036902811\n",
      "tensor([[ 0.9836, -0.1802],\n",
      "        [ 0.3997,  0.9167],\n",
      "        [ 0.2937,  0.9559]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.3898],\n",
      "        [3.5195],\n",
      "        [3.1282]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 9, Loss: 2.323918572184181\n",
      "tensor([[ 0.9943, -0.1070],\n",
      "        [ 0.3224,  0.9466],\n",
      "        [ 0.1974,  0.9803]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.4688],\n",
      "        [3.4788],\n",
      "        [3.0401]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 10, Loss: 2.199548504924706\n",
      "tensor([[ 0.9991, -0.0421],\n",
      "        [ 0.2494,  0.9684],\n",
      "        [ 0.1008,  0.9949]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.5509],\n",
      "        [3.4490],\n",
      "        [2.9555]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 11, Loss: 1.901364604331553\n",
      "tensor([[0.9999, 0.0139],\n",
      "        [0.1824, 0.9832],\n",
      "        [0.0052, 1.0000]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.6353],\n",
      "        [3.4283],\n",
      "        [2.8741]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 12, Loss: 1.7992363989450677\n",
      "tensor([[ 0.9982,  0.0600],\n",
      "        [ 0.1221,  0.9925],\n",
      "        [-0.0878,  0.9961]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.7209],\n",
      "        [3.4158],\n",
      "        [2.7963]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 13, Loss: 1.6272717231835676\n",
      "tensor([[ 0.9954,  0.0960],\n",
      "        [ 0.0688,  0.9976],\n",
      "        [-0.1771,  0.9842]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.8069],\n",
      "        [3.4093],\n",
      "        [2.7223]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 14, Loss: 1.5395712109969406\n",
      "tensor([[ 0.9925,  0.1219],\n",
      "        [ 0.0228,  0.9997],\n",
      "        [-0.2618,  0.9651]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.8933],\n",
      "        [3.4086],\n",
      "        [2.6520]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 15, Loss: 1.440930693307424\n",
      "tensor([[ 0.9904,  0.1385],\n",
      "        [-0.0154,  0.9999],\n",
      "        [-0.3415,  0.9399]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.9793],\n",
      "        [3.4131],\n",
      "        [2.5855]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 16, Loss: 1.3638677432635615\n",
      "tensor([[ 0.9893,  0.1461],\n",
      "        [-0.0455,  0.9990],\n",
      "        [-0.4156,  0.9095]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.0646],\n",
      "        [3.4222],\n",
      "        [2.5229]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 17, Loss: 1.3095136172855144\n",
      "tensor([[ 0.9893,  0.1456],\n",
      "        [-0.0673,  0.9977],\n",
      "        [-0.4838,  0.8752]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.1489],\n",
      "        [3.4350],\n",
      "        [2.4643]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 18, Loss: 1.2305187420545334\n",
      "tensor([[ 0.9905,  0.1378],\n",
      "        [-0.0819,  0.9966],\n",
      "        [-0.5461,  0.8377]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.2327],\n",
      "        [3.4509],\n",
      "        [2.4099]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 19, Loss: 1.1743689414802594\n",
      "tensor([[ 0.9926,  0.1215],\n",
      "        [-0.0899,  0.9960],\n",
      "        [-0.6027,  0.7980]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.3148],\n",
      "        [3.4675],\n",
      "        [2.3591]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 20, Loss: 1.1830240481480747\n",
      "tensor([[ 0.9949,  0.1006],\n",
      "        [-0.0905,  0.9959],\n",
      "        [-0.6538,  0.7567]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.3963],\n",
      "        [3.4844],\n",
      "        [2.3122]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 21, Loss: 1.1393714085315525\n",
      "tensor([[ 0.9972,  0.0746],\n",
      "        [-0.0852,  0.9964],\n",
      "        [-0.6996,  0.7145]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.4771],\n",
      "        [3.5028],\n",
      "        [2.2689]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 22, Loss: 1.0881338235319642\n",
      "tensor([[ 0.9990,  0.0439],\n",
      "        [-0.0753,  0.9972],\n",
      "        [-0.7406,  0.6720]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.5569],\n",
      "        [3.5222],\n",
      "        [2.2289]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 23, Loss: 1.0927519785375928\n",
      "tensor([[ 1.0000,  0.0094],\n",
      "        [-0.0609,  0.9981],\n",
      "        [-0.7769,  0.6296]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.6359],\n",
      "        [3.5425],\n",
      "        [2.1924]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 24, Loss: 1.0464262976061527\n",
      "tensor([[ 0.9996, -0.0278],\n",
      "        [-0.0432,  0.9991],\n",
      "        [-0.8091,  0.5877]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.7142],\n",
      "        [3.5644],\n",
      "        [2.1590]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 25, Loss: 1.0262304433086458\n",
      "tensor([[ 0.9977, -0.0675],\n",
      "        [-0.0216,  0.9998],\n",
      "        [-0.8373,  0.5467]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.7913],\n",
      "        [3.5872],\n",
      "        [2.1288]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 26, Loss: 1.011969173099745\n",
      "tensor([[ 0.9943, -0.1064],\n",
      "        [ 0.0024,  1.0000],\n",
      "        [-0.8620,  0.5069]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.8683],\n",
      "        [3.6104],\n",
      "        [2.1015]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 27, Loss: 0.9890160197279447\n",
      "tensor([[ 0.9895, -0.1446],\n",
      "        [ 0.0276,  0.9996],\n",
      "        [-0.8836,  0.4683]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.9449],\n",
      "        [3.6367],\n",
      "        [2.0771]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 28, Loss: 0.9449228579820349\n",
      "tensor([[ 0.9835, -0.1810],\n",
      "        [ 0.0542,  0.9985],\n",
      "        [-0.9023,  0.4310]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.0205],\n",
      "        [3.6641],\n",
      "        [2.0552]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 29, Loss: 0.9697032388738475\n",
      "tensor([[ 0.9769, -0.2137],\n",
      "        [ 0.0809,  0.9967],\n",
      "        [-0.9186,  0.3953]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.0955],\n",
      "        [3.6929],\n",
      "        [2.0358]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 30, Loss: 0.9493902704425866\n",
      "tensor([[ 0.9699, -0.2434],\n",
      "        [ 0.1078,  0.9942],\n",
      "        [-0.9325,  0.3611]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.1695],\n",
      "        [3.7227],\n",
      "        [2.0188]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 31, Loss: 0.9297904236919217\n",
      "tensor([[ 0.9631, -0.2692],\n",
      "        [ 0.1353,  0.9908],\n",
      "        [-0.9445,  0.3286]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.2423],\n",
      "        [3.7531],\n",
      "        [2.0041]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 32, Loss: 0.925568648217228\n",
      "tensor([[ 0.9566, -0.2915],\n",
      "        [ 0.1616,  0.9869],\n",
      "        [-0.9547,  0.2976]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.3143],\n",
      "        [3.7849],\n",
      "        [1.9914]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 33, Loss: 0.908301698257986\n",
      "tensor([[ 0.9516, -0.3073],\n",
      "        [ 0.1854,  0.9827],\n",
      "        [-0.9633,  0.2683]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.3854],\n",
      "        [3.8186],\n",
      "        [1.9807]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 34, Loss: 0.9059040327065567\n",
      "tensor([[ 0.9484, -0.3170],\n",
      "        [ 0.2081,  0.9781],\n",
      "        [-0.9706,  0.2405]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.4548],\n",
      "        [3.8523],\n",
      "        [1.9719]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 35, Loss: 0.9232933484068774\n",
      "tensor([[ 0.9474, -0.3200],\n",
      "        [ 0.2284,  0.9736],\n",
      "        [-0.9768,  0.2142]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.5228],\n",
      "        [3.8867],\n",
      "        [1.9648]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 36, Loss: 0.915182942338916\n",
      "tensor([[ 0.9485, -0.3168],\n",
      "        [ 0.2455,  0.9694],\n",
      "        [-0.9819,  0.1894]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.5896],\n",
      "        [3.9222],\n",
      "        [1.9593]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 37, Loss: 0.9116442791170184\n",
      "tensor([[ 0.9514, -0.3079],\n",
      "        [ 0.2594,  0.9658],\n",
      "        [-0.9861,  0.1661]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.6552],\n",
      "        [3.9586],\n",
      "        [1.9553]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 38, Loss: 0.9044283742724917\n",
      "tensor([[ 0.9555, -0.2949],\n",
      "        [ 0.2708,  0.9626],\n",
      "        [-0.9895,  0.1442]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.7191],\n",
      "        [3.9940],\n",
      "        [1.9526]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 39, Loss: 0.920986669675916\n",
      "tensor([[ 0.9608, -0.2774],\n",
      "        [ 0.2790,  0.9603],\n",
      "        [-0.9923,  0.1236]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.7825],\n",
      "        [4.0302],\n",
      "        [1.9513]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 40, Loss: 0.8873366159110212\n",
      "tensor([[ 0.9658, -0.2592],\n",
      "        [ 0.2850,  0.9585],\n",
      "        [-0.9946,  0.1042]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.8446],\n",
      "        [4.0651],\n",
      "        [1.9511]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 41, Loss: 0.9131310382609797\n",
      "tensor([[ 0.9711, -0.2387],\n",
      "        [ 0.2872,  0.9579],\n",
      "        [-0.9963,  0.0860]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.9064],\n",
      "        [4.1003],\n",
      "        [1.9519]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 42, Loss: 0.8840274187461754\n",
      "tensor([[ 0.9760, -0.2179],\n",
      "        [ 0.2866,  0.9581],\n",
      "        [-0.9976,  0.0689]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.9676],\n",
      "        [4.1351],\n",
      "        [1.9538]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 43, Loss: 0.8863494822956177\n",
      "tensor([[ 0.9805, -0.1963],\n",
      "        [ 0.2841,  0.9588],\n",
      "        [-0.9986,  0.0530]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.0279],\n",
      "        [4.1691],\n",
      "        [1.9566]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 44, Loss: 0.9211123268877593\n",
      "tensor([[ 0.9845, -0.1752],\n",
      "        [ 0.2803,  0.9599],\n",
      "        [-0.9993,  0.0381]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.0876],\n",
      "        [4.2025],\n",
      "        [1.9603]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 45, Loss: 0.8861406970866404\n",
      "tensor([[ 0.9875, -0.1574],\n",
      "        [ 0.2756,  0.9613],\n",
      "        [-0.9997,  0.0242]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.1468],\n",
      "        [4.2361],\n",
      "        [1.9649]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 46, Loss: 0.8801318568261454\n",
      "tensor([[ 0.9901, -0.1405],\n",
      "        [ 0.2703,  0.9628],\n",
      "        [-0.9999,  0.0113]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.2054],\n",
      "        [4.2691],\n",
      "        [1.9701]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 47, Loss: 0.8904148359167067\n",
      "tensor([[ 9.9229e-01, -1.2396e-01],\n",
      "        [ 2.6252e-01,  9.6493e-01],\n",
      "        [-1.0000e+00, -7.2078e-04]], dtype=torch.float64,\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.2639],\n",
      "        [4.3016],\n",
      "        [1.9761]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 48, Loss: 0.8850189937602693\n",
      "tensor([[ 0.9938, -0.1114],\n",
      "        [ 0.2537,  0.9673],\n",
      "        [-0.9999, -0.0119]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.3213],\n",
      "        [4.3321],\n",
      "        [1.9827]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 49, Loss: 0.9111805494022804\n",
      "tensor([[ 0.9946, -0.1041],\n",
      "        [ 0.2447,  0.9696],\n",
      "        [-0.9998, -0.0224]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.3784],\n",
      "        [4.3634],\n",
      "        [1.9899]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 50, Loss: 0.8638484543961846\n",
      "tensor([[ 0.9947, -0.1024],\n",
      "        [ 0.2353,  0.9719],\n",
      "        [-0.9995, -0.0321]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.4350],\n",
      "        [4.3951],\n",
      "        [1.9976]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 51, Loss: 0.8697105440396031\n",
      "tensor([[ 0.9945, -0.1043],\n",
      "        [ 0.2256,  0.9742],\n",
      "        [-0.9992, -0.0410]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.4913],\n",
      "        [4.4262],\n",
      "        [2.0058]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 52, Loss: 0.8774106438149151\n",
      "tensor([[ 0.9943, -0.1071],\n",
      "        [ 0.2161,  0.9764],\n",
      "        [-0.9988, -0.0493]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.5473],\n",
      "        [4.4570],\n",
      "        [2.0144]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 53, Loss: 0.8758735564295721\n",
      "tensor([[ 0.9937, -0.1124],\n",
      "        [ 0.2068,  0.9784],\n",
      "        [-0.9984, -0.0570]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.6030],\n",
      "        [4.4881],\n",
      "        [2.0234]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 54, Loss: 0.864815029752368\n",
      "tensor([[ 0.9928, -0.1199],\n",
      "        [ 0.1988,  0.9800],\n",
      "        [-0.9980, -0.0639]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.6583],\n",
      "        [4.5191],\n",
      "        [2.0328]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 55, Loss: 0.8648575843914733\n",
      "tensor([[ 0.9916, -0.1290],\n",
      "        [ 0.1915,  0.9815],\n",
      "        [-0.9975, -0.0703]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.7134],\n",
      "        [4.5509],\n",
      "        [2.0425]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 56, Loss: 0.8685590418287455\n",
      "tensor([[ 0.9904, -0.1381],\n",
      "        [ 0.1867,  0.9824],\n",
      "        [-0.9971, -0.0762]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.7680],\n",
      "        [4.5821],\n",
      "        [2.0524]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 57, Loss: 0.8688765582243969\n",
      "tensor([[ 0.9893, -0.1462],\n",
      "        [ 0.1833,  0.9831],\n",
      "        [-0.9967, -0.0815]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.8222],\n",
      "        [4.6142],\n",
      "        [2.0627]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 58, Loss: 0.8481570303810685\n",
      "tensor([[ 0.9884, -0.1522],\n",
      "        [ 0.1818,  0.9833],\n",
      "        [-0.9963, -0.0865]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.8756],\n",
      "        [4.6449],\n",
      "        [2.0732]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 59, Loss: 0.8639841593524299\n",
      "tensor([[ 0.9872, -0.1593],\n",
      "        [ 0.1832,  0.9831],\n",
      "        [-0.9959, -0.0910]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.9280],\n",
      "        [4.6729],\n",
      "        [2.0839]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 60, Loss: 0.8762589556582353\n",
      "tensor([[ 0.9861, -0.1662],\n",
      "        [ 0.1854,  0.9827],\n",
      "        [-0.9955, -0.0950]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.9802],\n",
      "        [4.7016],\n",
      "        [2.0949]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 61, Loss: 0.8425620425706336\n",
      "tensor([[ 0.9850, -0.1728],\n",
      "        [ 0.1873,  0.9823],\n",
      "        [-0.9951, -0.0988]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.0323],\n",
      "        [4.7301],\n",
      "        [2.1060]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 62, Loss: 0.8385306526079537\n",
      "tensor([[ 0.9841, -0.1775],\n",
      "        [ 0.1881,  0.9822],\n",
      "        [-0.9948, -0.1022]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.0838],\n",
      "        [4.7577],\n",
      "        [2.1173]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 63, Loss: 0.8590744558109384\n",
      "tensor([[ 0.9835, -0.1808],\n",
      "        [ 0.1907,  0.9816],\n",
      "        [-0.9944, -0.1054]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.1350],\n",
      "        [4.7844],\n",
      "        [2.1287]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 64, Loss: 0.8569865256846964\n",
      "tensor([[ 0.9833, -0.1819],\n",
      "        [ 0.1936,  0.9811],\n",
      "        [-0.9941, -0.1082]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.1854],\n",
      "        [4.8084],\n",
      "        [2.1402]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 65, Loss: 0.8811183802678366\n",
      "tensor([[ 0.9835, -0.1808],\n",
      "        [ 0.1957,  0.9807],\n",
      "        [-0.9939, -0.1106]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.2354],\n",
      "        [4.8309],\n",
      "        [2.1517]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 66, Loss: 0.8728924451330938\n",
      "tensor([[ 0.9839, -0.1786],\n",
      "        [ 0.1973,  0.9803],\n",
      "        [-0.9936, -0.1129]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.2853],\n",
      "        [4.8547],\n",
      "        [2.1634]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 67, Loss: 0.8352724333626147\n",
      "tensor([[ 0.9846, -0.1747],\n",
      "        [ 0.1993,  0.9799],\n",
      "        [-0.9934, -0.1149]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.3342],\n",
      "        [4.8767],\n",
      "        [2.1751]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 68, Loss: 0.8702853710369493\n",
      "tensor([[ 0.9855, -0.1696],\n",
      "        [ 0.1999,  0.9798],\n",
      "        [-0.9932, -0.1167]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.3829],\n",
      "        [4.8974],\n",
      "        [2.1869]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 69, Loss: 0.8727406029844819\n",
      "tensor([[ 0.9860, -0.1668],\n",
      "        [ 0.2004,  0.9797],\n",
      "        [-0.9930, -0.1182]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.4313],\n",
      "        [4.9180],\n",
      "        [2.1988]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 70, Loss: 0.8410550861581072\n",
      "tensor([[ 0.9867, -0.1625],\n",
      "        [ 0.1994,  0.9799],\n",
      "        [-0.9928, -0.1196]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.4791],\n",
      "        [4.9387],\n",
      "        [2.2108]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 71, Loss: 0.8550272441756899\n",
      "tensor([[ 0.9880, -0.1546],\n",
      "        [ 0.1989,  0.9800],\n",
      "        [-0.9927, -0.1209]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.5267],\n",
      "        [4.9599],\n",
      "        [2.2228]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 72, Loss: 0.8514882388890683\n",
      "tensor([[ 0.9888, -0.1492],\n",
      "        [ 0.1994,  0.9799],\n",
      "        [-0.9926, -0.1218]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.5741],\n",
      "        [4.9804],\n",
      "        [2.2348]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 73, Loss: 0.8436504532331097\n",
      "tensor([[ 0.9895, -0.1448],\n",
      "        [ 0.2004,  0.9797],\n",
      "        [-0.9925, -0.1225]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.6214],\n",
      "        [5.0030],\n",
      "        [2.2468]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 74, Loss: 0.8226188024668935\n",
      "tensor([[ 0.9898, -0.1428],\n",
      "        [ 0.2006,  0.9797],\n",
      "        [-0.9924, -0.1231]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.6686],\n",
      "        [5.0269],\n",
      "        [2.2588]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 75, Loss: 0.8123031228380635\n",
      "tensor([[ 0.9896, -0.1437],\n",
      "        [ 0.1989,  0.9800],\n",
      "        [-0.9923, -0.1235]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.7153],\n",
      "        [5.0510],\n",
      "        [2.2708]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 76, Loss: 0.8296636073430879\n",
      "tensor([[ 0.9893, -0.1460],\n",
      "        [ 0.1976,  0.9803],\n",
      "        [-0.9923, -0.1238]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.7614],\n",
      "        [5.0744],\n",
      "        [2.2829]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 77, Loss: 0.8502758140726978\n",
      "tensor([[ 0.9885, -0.1514],\n",
      "        [ 0.1963,  0.9805],\n",
      "        [-0.9923, -0.1240]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.8068],\n",
      "        [5.0984],\n",
      "        [2.2950]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 78, Loss: 0.8295736594690528\n",
      "tensor([[ 0.9874, -0.1580],\n",
      "        [ 0.1949,  0.9808],\n",
      "        [-0.9923, -0.1240]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.8524],\n",
      "        [5.1226],\n",
      "        [2.3070]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 79, Loss: 0.8210675549048456\n",
      "tensor([[ 0.9864, -0.1643],\n",
      "        [ 0.1955,  0.9807],\n",
      "        [-0.9923, -0.1238]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.8973],\n",
      "        [5.1442],\n",
      "        [2.3191]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 80, Loss: 0.8661101307682153\n",
      "tensor([[ 0.9856, -0.1688],\n",
      "        [ 0.1956,  0.9807],\n",
      "        [-0.9923, -0.1235]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.9422],\n",
      "        [5.1667],\n",
      "        [2.3312]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 81, Loss: 0.8242809428475943\n",
      "tensor([[ 0.9843, -0.1763],\n",
      "        [ 0.1976,  0.9803],\n",
      "        [-0.9924, -0.1232]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.9860],\n",
      "        [5.1849],\n",
      "        [2.3432]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 82, Loss: 0.8791190215589755\n",
      "tensor([[ 0.9836, -0.1803],\n",
      "        [ 0.2008,  0.9796],\n",
      "        [-0.9924, -0.1229]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.0295],\n",
      "        [5.2028],\n",
      "        [2.3552]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 83, Loss: 0.8296301036634359\n",
      "tensor([[ 0.9832, -0.1826],\n",
      "        [ 0.2038,  0.9790],\n",
      "        [-0.9925, -0.1224]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.0730],\n",
      "        [5.2208],\n",
      "        [2.3672]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 84, Loss: 0.8163304876067636\n",
      "tensor([[ 0.9835, -0.1809],\n",
      "        [ 0.2071,  0.9783],\n",
      "        [-0.9925, -0.1219]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1161],\n",
      "        [5.2376],\n",
      "        [2.3793]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 85, Loss: 0.8439785227121559\n",
      "tensor([[ 0.9842, -0.1768],\n",
      "        [ 0.2093,  0.9778],\n",
      "        [-0.9926, -0.1214]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1592],\n",
      "        [5.2550],\n",
      "        [2.3914]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 86, Loss: 0.8213788530261157\n",
      "tensor([[ 0.9848, -0.1735],\n",
      "        [ 0.2124,  0.9772],\n",
      "        [-0.9927, -0.1207]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2015],\n",
      "        [5.2727],\n",
      "        [2.4035]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 87, Loss: 0.8432985275912241\n",
      "tensor([[ 0.9862, -0.1655],\n",
      "        [ 0.2123,  0.9772],\n",
      "        [-0.9928, -0.1200]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2437],\n",
      "        [5.2888],\n",
      "        [2.4156]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 88, Loss: 0.8433238787762195\n",
      "tensor([[ 0.9883, -0.1525],\n",
      "        [ 0.2099,  0.9777],\n",
      "        [-0.9929, -0.1193]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2858],\n",
      "        [5.3074],\n",
      "        [2.4276]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 89, Loss: 0.7990068214880485\n",
      "tensor([[ 0.9896, -0.1436],\n",
      "        [ 0.2091,  0.9779],\n",
      "        [-0.9930, -0.1185]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3278],\n",
      "        [5.3251],\n",
      "        [2.4396]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 90, Loss: 0.8285323016905664\n",
      "tensor([[ 0.9905, -0.1375],\n",
      "        [ 0.2052,  0.9787],\n",
      "        [-0.9931, -0.1177]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3688],\n",
      "        [5.3443],\n",
      "        [2.4517]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 91, Loss: 0.8200192606719522\n",
      "tensor([[ 0.9908, -0.1353],\n",
      "        [ 0.2011,  0.9796],\n",
      "        [-0.9932, -0.1168]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4096],\n",
      "        [5.3635],\n",
      "        [2.4637]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 92, Loss: 0.8256584424853385\n",
      "tensor([[ 0.9907, -0.1357],\n",
      "        [ 0.1969,  0.9804],\n",
      "        [-0.9933, -0.1159]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4504],\n",
      "        [5.3818],\n",
      "        [2.4757]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 93, Loss: 0.8227982372950944\n",
      "tensor([[ 0.9906, -0.1370],\n",
      "        [ 0.1928,  0.9812],\n",
      "        [-0.9934, -0.1149]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4911],\n",
      "        [5.4023],\n",
      "        [2.4876]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 94, Loss: 0.7936894610024593\n",
      "tensor([[ 0.9906, -0.1365],\n",
      "        [ 0.1901,  0.9818],\n",
      "        [-0.9935, -0.1138]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5320],\n",
      "        [5.4222],\n",
      "        [2.4995]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 95, Loss: 0.8333702669121785\n",
      "tensor([[ 0.9903, -0.1387],\n",
      "        [ 0.1899,  0.9818],\n",
      "        [-0.9936, -0.1126]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5722],\n",
      "        [5.4413],\n",
      "        [2.5113]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 96, Loss: 0.8305084373906614\n",
      "tensor([[ 0.9901, -0.1401],\n",
      "        [ 0.1907,  0.9816],\n",
      "        [-0.9938, -0.1112]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6128],\n",
      "        [5.4612],\n",
      "        [2.5231]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 97, Loss: 0.8044564695768512\n",
      "tensor([[ 0.9897, -0.1429],\n",
      "        [ 0.1954,  0.9807],\n",
      "        [-0.9940, -0.1097]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6528],\n",
      "        [5.4784],\n",
      "        [2.5348]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 98, Loss: 0.844156497037842\n",
      "tensor([[ 0.9889, -0.1483],\n",
      "        [ 0.2005,  0.9797],\n",
      "        [-0.9941, -0.1081]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6924],\n",
      "        [5.4956],\n",
      "        [2.5466]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 99, Loss: 0.8119434358038037\n",
      "tensor([[ 0.9886, -0.1504],\n",
      "        [ 0.2018,  0.9794],\n",
      "        [-0.9943, -0.1065]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7321],\n",
      "        [5.5143],\n",
      "        [2.5583]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 100, Loss: 0.7932894103496524\n",
      "tensor([[ 0.9872, -0.1596],\n",
      "        [ 0.2056,  0.9786],\n",
      "        [-0.9945, -0.1050]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7715],\n",
      "        [5.5299],\n",
      "        [2.5700]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 101, Loss: 0.8284806709571869\n",
      "tensor([[ 0.9858, -0.1682],\n",
      "        [ 0.2080,  0.9781],\n",
      "        [-0.9946, -0.1036]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8102],\n",
      "        [5.5446],\n",
      "        [2.5817]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 102, Loss: 0.8407824486023494\n",
      "tensor([[ 0.9848, -0.1739],\n",
      "        [ 0.2104,  0.9776],\n",
      "        [-0.9947, -0.1023]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8484],\n",
      "        [5.5554],\n",
      "        [2.5933]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 103, Loss: 0.8504443553255964\n",
      "tensor([[ 0.9838, -0.1792],\n",
      "        [ 0.2104,  0.9776],\n",
      "        [-0.9949, -0.1012]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8866],\n",
      "        [5.5652],\n",
      "        [2.6049]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 104, Loss: 0.8178625704295993\n",
      "tensor([[ 0.9834, -0.1817],\n",
      "        [ 0.2086,  0.9780],\n",
      "        [-0.9950, -0.1001]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9243],\n",
      "        [5.5750],\n",
      "        [2.6164]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 105, Loss: 0.8386443370752132\n",
      "tensor([[ 0.9839, -0.1788],\n",
      "        [ 0.2100,  0.9777],\n",
      "        [-0.9951, -0.0990]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9612],\n",
      "        [5.5827],\n",
      "        [2.6279]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 106, Loss: 0.8421068797210195\n",
      "tensor([[ 0.9845, -0.1755],\n",
      "        [ 0.2108,  0.9775],\n",
      "        [-0.9952, -0.0977]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9980],\n",
      "        [5.5894],\n",
      "        [2.6392]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 107, Loss: 0.8457477011996565\n",
      "tensor([[ 0.9858, -0.1682],\n",
      "        [ 0.2110,  0.9775],\n",
      "        [-0.9953, -0.0966]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0348],\n",
      "        [5.5944],\n",
      "        [2.6506]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 108, Loss: 0.8373938252675146\n",
      "tensor([[ 0.9873, -0.1588],\n",
      "        [ 0.2100,  0.9777],\n",
      "        [-0.9954, -0.0956]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0714],\n",
      "        [5.6007],\n",
      "        [2.6620]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 109, Loss: 0.8016413485998823\n",
      "tensor([[ 0.9891, -0.1473],\n",
      "        [ 0.2080,  0.9781],\n",
      "        [-0.9955, -0.0948]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1083],\n",
      "        [5.6076],\n",
      "        [2.6733]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 110, Loss: 0.7976252240408229\n",
      "tensor([[ 0.9902, -0.1400],\n",
      "        [ 0.2077,  0.9782],\n",
      "        [-0.9956, -0.0941]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1454],\n",
      "        [5.6162],\n",
      "        [2.6847]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 111, Loss: 0.798759785366364\n",
      "tensor([[ 0.9910, -0.1339],\n",
      "        [ 0.2055,  0.9787],\n",
      "        [-0.9956, -0.0934]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1827],\n",
      "        [5.6255],\n",
      "        [2.6959]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 112, Loss: 0.794876366728436\n",
      "tensor([[ 0.9915, -0.1303],\n",
      "        [ 0.2050,  0.9788],\n",
      "        [-0.9957, -0.0927]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2199],\n",
      "        [5.6338],\n",
      "        [2.7072]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 113, Loss: 0.8206282598618589\n",
      "tensor([[ 0.9916, -0.1291],\n",
      "        [ 0.2062,  0.9785],\n",
      "        [-0.9958, -0.0920]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2565],\n",
      "        [5.6393],\n",
      "        [2.7184]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 114, Loss: 0.8427934926096544\n",
      "tensor([[ 0.9913, -0.1318],\n",
      "        [ 0.2059,  0.9786],\n",
      "        [-0.9958, -0.0914]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2929],\n",
      "        [5.6445],\n",
      "        [2.7296]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 115, Loss: 0.8156601584692488\n",
      "tensor([[ 0.9901, -0.1405],\n",
      "        [ 0.2054,  0.9787],\n",
      "        [-0.9959, -0.0908]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3291],\n",
      "        [5.6473],\n",
      "        [2.7407]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 116, Loss: 0.8366707156502923\n",
      "tensor([[ 0.9881, -0.1537],\n",
      "        [ 0.2047,  0.9788],\n",
      "        [-0.9959, -0.0901]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3648],\n",
      "        [5.6522],\n",
      "        [2.7519]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 117, Loss: 0.8081575454991013\n",
      "tensor([[ 0.9866, -0.1634],\n",
      "        [ 0.2072,  0.9783],\n",
      "        [-0.9960, -0.0893]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4003],\n",
      "        [5.6548],\n",
      "        [2.7630]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 118, Loss: 0.8462853280328348\n",
      "tensor([[ 0.9847, -0.1740],\n",
      "        [ 0.2104,  0.9776],\n",
      "        [-0.9961, -0.0886]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4356],\n",
      "        [5.6578],\n",
      "        [2.7741]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 119, Loss: 0.8054980498591563\n",
      "tensor([[ 0.9834, -0.1817],\n",
      "        [ 0.2115,  0.9774],\n",
      "        [-0.9961, -0.0880]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4709],\n",
      "        [5.6599],\n",
      "        [2.7851]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 120, Loss: 0.8116149653323027\n",
      "tensor([[ 0.9817, -0.1902],\n",
      "        [ 0.2151,  0.9766],\n",
      "        [-0.9962, -0.0872]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5059],\n",
      "        [5.6599],\n",
      "        [2.7962]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 121, Loss: 0.8283731392212473\n",
      "tensor([[ 0.9805, -0.1966],\n",
      "        [ 0.2187,  0.9758],\n",
      "        [-0.9963, -0.0865]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5412],\n",
      "        [5.6611],\n",
      "        [2.8072]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 122, Loss: 0.8013360859004671\n",
      "tensor([[ 0.9806, -0.1958],\n",
      "        [ 0.2192,  0.9757],\n",
      "        [-0.9963, -0.0857]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5759],\n",
      "        [5.6622],\n",
      "        [2.8183]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 123, Loss: 0.8240411547616024\n",
      "tensor([[ 0.9813, -0.1924],\n",
      "        [ 0.2230,  0.9748],\n",
      "        [-0.9964, -0.0848]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.6095],\n",
      "        [5.6607],\n",
      "        [2.8293]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 124, Loss: 0.8496084613705235\n",
      "tensor([[ 0.9831, -0.1832],\n",
      "        [ 0.2251,  0.9743],\n",
      "        [-0.9965, -0.0837]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.6426],\n",
      "        [5.6573],\n",
      "        [2.8402]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 125, Loss: 0.8447150922075863\n",
      "tensor([[ 0.9839, -0.1786],\n",
      "        [ 0.2299,  0.9732],\n",
      "        [-0.9966, -0.0827]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.6750],\n",
      "        [5.6527],\n",
      "        [2.8512]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 126, Loss: 0.8289690582646067\n",
      "tensor([[ 0.9854, -0.1702],\n",
      "        [ 0.2330,  0.9725],\n",
      "        [-0.9967, -0.0818]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.7075],\n",
      "        [5.6498],\n",
      "        [2.8620]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 127, Loss: 0.8120997747732588\n",
      "tensor([[ 0.9877, -0.1561],\n",
      "        [ 0.2328,  0.9725],\n",
      "        [-0.9967, -0.0810]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.7400],\n",
      "        [5.6491],\n",
      "        [2.8728]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 128, Loss: 0.8112497646789599\n",
      "tensor([[ 0.9896, -0.1440],\n",
      "        [ 0.2303,  0.9731],\n",
      "        [-0.9968, -0.0804]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.7727],\n",
      "        [5.6484],\n",
      "        [2.8836]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 129, Loss: 0.807561156234342\n",
      "tensor([[ 0.9908, -0.1352],\n",
      "        [ 0.2299,  0.9732],\n",
      "        [-0.9968, -0.0796]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.8043],\n",
      "        [5.6468],\n",
      "        [2.8943]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 130, Loss: 0.848401323327384\n",
      "tensor([[ 0.9921, -0.1251],\n",
      "        [ 0.2274,  0.9738],\n",
      "        [-0.9969, -0.0789]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.8364],\n",
      "        [5.6484],\n",
      "        [2.9050]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 131, Loss: 0.774719206309066\n",
      "tensor([[ 0.9927, -0.1205],\n",
      "        [ 0.2231,  0.9748],\n",
      "        [-0.9969, -0.0781]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.8692],\n",
      "        [5.6505],\n",
      "        [2.9156]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 132, Loss: 0.793130453827015\n",
      "tensor([[ 0.9927, -0.1208],\n",
      "        [ 0.2213,  0.9752],\n",
      "        [-0.9970, -0.0774]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.9007],\n",
      "        [5.6501],\n",
      "        [2.9262]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 133, Loss: 0.8572630855100996\n",
      "tensor([[ 0.9919, -0.1269],\n",
      "        [ 0.2183,  0.9759],\n",
      "        [-0.9971, -0.0767]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.9322],\n",
      "        [5.6505],\n",
      "        [2.9368]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 134, Loss: 0.8061520285541511\n",
      "tensor([[ 0.9913, -0.1316],\n",
      "        [ 0.2114,  0.9774],\n",
      "        [-0.9971, -0.0760]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.9635],\n",
      "        [5.6507],\n",
      "        [2.9472]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 135, Loss: 0.8272735198208698\n",
      "tensor([[ 0.9909, -0.1346],\n",
      "        [ 0.2049,  0.9788],\n",
      "        [-0.9971, -0.0755]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.9953],\n",
      "        [5.6546],\n",
      "        [2.9576]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 136, Loss: 0.7758434741284499\n",
      "tensor([[ 0.9905, -0.1374],\n",
      "        [ 0.1995,  0.9799],\n",
      "        [-0.9972, -0.0750]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.0271],\n",
      "        [5.6605],\n",
      "        [2.9681]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 137, Loss: 0.8156908784984329\n",
      "tensor([[ 0.9900, -0.1412],\n",
      "        [ 0.1958,  0.9806],\n",
      "        [-0.9972, -0.0744]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.0589],\n",
      "        [5.6623],\n",
      "        [2.9784]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 138, Loss: 0.8374734647536116\n",
      "tensor([[ 0.9887, -0.1498],\n",
      "        [ 0.1973,  0.9803],\n",
      "        [-0.9973, -0.0739]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.0900],\n",
      "        [5.6588],\n",
      "        [2.9888]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 139, Loss: 0.8540978812188432\n",
      "tensor([[ 0.9877, -0.1563],\n",
      "        [ 0.1980,  0.9802],\n",
      "        [-0.9973, -0.0736]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.1214],\n",
      "        [5.6562],\n",
      "        [2.9991]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 140, Loss: 0.7973002892061135\n",
      "tensor([[ 0.9872, -0.1597],\n",
      "        [ 0.1993,  0.9799],\n",
      "        [-0.9973, -0.0732]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.1530],\n",
      "        [5.6562],\n",
      "        [3.0095]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 141, Loss: 0.7892776298288046\n",
      "tensor([[ 0.9880, -0.1546],\n",
      "        [ 0.1977,  0.9803],\n",
      "        [-0.9974, -0.0726]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.1840],\n",
      "        [5.6561],\n",
      "        [3.0199]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 142, Loss: 0.8212556406445519\n",
      "tensor([[ 0.9895, -0.1442],\n",
      "        [ 0.1965,  0.9805],\n",
      "        [-0.9974, -0.0719]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.2144],\n",
      "        [5.6547],\n",
      "        [3.0301]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 143, Loss: 0.8349526846854861\n",
      "tensor([[ 0.9907, -0.1358],\n",
      "        [ 0.1956,  0.9807],\n",
      "        [-0.9975, -0.0711]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.2451],\n",
      "        [5.6537],\n",
      "        [3.0404]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 144, Loss: 0.7962456787450295\n",
      "tensor([[ 0.9915, -0.1304],\n",
      "        [ 0.1939,  0.9810],\n",
      "        [-0.9975, -0.0706]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.2761],\n",
      "        [5.6528],\n",
      "        [3.0506]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 145, Loss: 0.8080859545019607\n",
      "tensor([[ 0.9915, -0.1300],\n",
      "        [ 0.1953,  0.9808],\n",
      "        [-0.9976, -0.0698]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.3073],\n",
      "        [5.6511],\n",
      "        [3.0607]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 146, Loss: 0.8247668671109593\n",
      "tensor([[ 0.9907, -0.1361],\n",
      "        [ 0.2003,  0.9797],\n",
      "        [-0.9976, -0.0694]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.3382],\n",
      "        [5.6481],\n",
      "        [3.0709]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 147, Loss: 0.8061789030221613\n",
      "tensor([[ 0.9897, -0.1431],\n",
      "        [ 0.2036,  0.9791],\n",
      "        [-0.9976, -0.0689]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.3693],\n",
      "        [5.6474],\n",
      "        [3.0811]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 148, Loss: 0.7705531291836302\n",
      "tensor([[ 0.9885, -0.1512],\n",
      "        [ 0.2113,  0.9774],\n",
      "        [-0.9977, -0.0685]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.4004],\n",
      "        [5.6428],\n",
      "        [3.0912]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 149, Loss: 0.8333358152339955\n",
      "tensor([[ 0.9870, -0.1608],\n",
      "        [ 0.2199,  0.9755],\n",
      "        [-0.9977, -0.0682]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.4309],\n",
      "        [5.6383],\n",
      "        [3.1014]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 150, Loss: 0.819405335444744\n",
      "tensor([[ 0.9847, -0.1744],\n",
      "        [ 0.2248,  0.9744],\n",
      "        [-0.9977, -0.0680]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.4609],\n",
      "        [5.6336],\n",
      "        [3.1115]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 151, Loss: 0.8095063951346254\n",
      "tensor([[ 0.9837, -0.1800],\n",
      "        [ 0.2272,  0.9739],\n",
      "        [-0.9977, -0.0676]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.4909],\n",
      "        [5.6318],\n",
      "        [3.1216]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 152, Loss: 0.7818896108228689\n",
      "tensor([[ 0.9840, -0.1783],\n",
      "        [ 0.2256,  0.9742],\n",
      "        [-0.9977, -0.0675]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.5203],\n",
      "        [5.6327],\n",
      "        [3.1318]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 153, Loss: 0.797719363504345\n",
      "tensor([[ 0.9841, -0.1774],\n",
      "        [ 0.2248,  0.9744],\n",
      "        [-0.9977, -0.0675]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.5500],\n",
      "        [5.6318],\n",
      "        [3.1419]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 154, Loss: 0.8069807125940666\n",
      "tensor([[ 0.9847, -0.1741],\n",
      "        [ 0.2222,  0.9750],\n",
      "        [-0.9977, -0.0674]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.5792],\n",
      "        [5.6336],\n",
      "        [3.1519]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 155, Loss: 0.8007210417687245\n",
      "tensor([[ 0.9859, -0.1672],\n",
      "        [ 0.2218,  0.9751],\n",
      "        [-0.9977, -0.0673]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.6084],\n",
      "        [5.6349],\n",
      "        [3.1619]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 156, Loss: 0.7956742499696181\n",
      "tensor([[ 0.9873, -0.1592],\n",
      "        [ 0.2195,  0.9756],\n",
      "        [-0.9977, -0.0673]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.6373],\n",
      "        [5.6326],\n",
      "        [3.1717]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 157, Loss: 0.8513280498379268\n",
      "tensor([[ 0.9892, -0.1468],\n",
      "        [ 0.2149,  0.9766],\n",
      "        [-0.9977, -0.0673]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.6661],\n",
      "        [5.6314],\n",
      "        [3.1816]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 158, Loss: 0.7954136283857255\n",
      "tensor([[ 0.9907, -0.1358],\n",
      "        [ 0.2151,  0.9766],\n",
      "        [-0.9977, -0.0674]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.6946],\n",
      "        [5.6289],\n",
      "        [3.1914]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 159, Loss: 0.8146323483447896\n",
      "tensor([[ 0.9917, -0.1282],\n",
      "        [ 0.2156,  0.9765],\n",
      "        [-0.9977, -0.0676]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.7232],\n",
      "        [5.6284],\n",
      "        [3.2012]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 160, Loss: 0.7900438761507993\n",
      "tensor([[ 0.9930, -0.1184],\n",
      "        [ 0.2182,  0.9759],\n",
      "        [-0.9977, -0.0677]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.7514],\n",
      "        [5.6281],\n",
      "        [3.2110]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 161, Loss: 0.8218649301972905\n",
      "tensor([[ 0.9930, -0.1184],\n",
      "        [ 0.2206,  0.9754],\n",
      "        [-0.9977, -0.0677]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.7799],\n",
      "        [5.6284],\n",
      "        [3.2208]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 162, Loss: 0.7886275749690814\n",
      "tensor([[ 0.9931, -0.1173],\n",
      "        [ 0.2260,  0.9741],\n",
      "        [-0.9977, -0.0677]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.8079],\n",
      "        [5.6274],\n",
      "        [3.2305]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 163, Loss: 0.8266301667497526\n",
      "tensor([[ 0.9937, -0.1121],\n",
      "        [ 0.2240,  0.9746],\n",
      "        [-0.9977, -0.0678]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.8363],\n",
      "        [5.6280],\n",
      "        [3.2404]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 164, Loss: 0.785971467073907\n",
      "tensor([[ 0.9944, -0.1055],\n",
      "        [ 0.2188,  0.9758],\n",
      "        [-0.9977, -0.0680]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.8652],\n",
      "        [5.6299],\n",
      "        [3.2501]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 165, Loss: 0.7797086488672209\n",
      "tensor([[ 0.9949, -0.1011],\n",
      "        [ 0.2119,  0.9773],\n",
      "        [-0.9977, -0.0682]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.8941],\n",
      "        [5.6309],\n",
      "        [3.2598]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 166, Loss: 0.8149284576811509\n",
      "tensor([[ 0.9948, -0.1014],\n",
      "        [ 0.2028,  0.9792],\n",
      "        [-0.9977, -0.0684]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.9233],\n",
      "        [5.6342],\n",
      "        [3.2695]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 167, Loss: 0.7755219423264027\n",
      "tensor([[ 0.9931, -0.1169],\n",
      "        [ 0.1950,  0.9808],\n",
      "        [-0.9976, -0.0687]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.9522],\n",
      "        [5.6381],\n",
      "        [3.2792]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 168, Loss: 0.7882921025871488\n",
      "tensor([[ 0.9907, -0.1363],\n",
      "        [ 0.1875,  0.9823],\n",
      "        [-0.9976, -0.0692]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.9803],\n",
      "        [5.6382],\n",
      "        [3.2889]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 169, Loss: 0.8341753232680243\n",
      "tensor([[ 0.9872, -0.1596],\n",
      "        [ 0.1802,  0.9836],\n",
      "        [-0.9976, -0.0698]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.0088],\n",
      "        [ 5.6383],\n",
      "        [ 3.2985]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 170, Loss: 0.7802267746224616\n",
      "tensor([[ 0.9834, -0.1817],\n",
      "        [ 0.1736,  0.9848],\n",
      "        [-0.9975, -0.0704]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.0373],\n",
      "        [ 5.6353],\n",
      "        [ 3.3081]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 171, Loss: 0.8191830046821377\n",
      "tensor([[ 0.9803, -0.1977],\n",
      "        [ 0.1708,  0.9853],\n",
      "        [-0.9975, -0.0709]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.0661],\n",
      "        [ 5.6358],\n",
      "        [ 3.3177]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 172, Loss: 0.7576665224161939\n",
      "tensor([[ 0.9792, -0.2030],\n",
      "        [ 0.1701,  0.9854],\n",
      "        [-0.9975, -0.0711]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.0947],\n",
      "        [ 5.6378],\n",
      "        [ 3.3272]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 173, Loss: 0.7909664327651408\n",
      "tensor([[ 0.9803, -0.1975],\n",
      "        [ 0.1759,  0.9844],\n",
      "        [-0.9975, -0.0712]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.1223],\n",
      "        [ 5.6374],\n",
      "        [ 3.3365]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 174, Loss: 0.8350358639892334\n",
      "tensor([[ 0.9822, -0.1878],\n",
      "        [ 0.1850,  0.9827],\n",
      "        [-0.9975, -0.0713]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.1501],\n",
      "        [ 5.6363],\n",
      "        [ 3.3458]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 175, Loss: 0.7953709467837767\n",
      "tensor([[ 0.9842, -0.1772],\n",
      "        [ 0.1933,  0.9811],\n",
      "        [-0.9975, -0.0713]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.1784],\n",
      "        [ 5.6360],\n",
      "        [ 3.3551]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 176, Loss: 0.7854465543517801\n",
      "tensor([[ 0.9861, -0.1659],\n",
      "        [ 0.2034,  0.9791],\n",
      "        [-0.9975, -0.0711]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.2058],\n",
      "        [ 5.6357],\n",
      "        [ 3.3644]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 177, Loss: 0.8007958838077593\n",
      "tensor([[ 0.9889, -0.1484],\n",
      "        [ 0.2116,  0.9774],\n",
      "        [-0.9975, -0.0713]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.2331],\n",
      "        [ 5.6349],\n",
      "        [ 3.3736]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 178, Loss: 0.7975000566826993\n",
      "tensor([[ 0.9916, -0.1291],\n",
      "        [ 0.2188,  0.9758],\n",
      "        [-0.9975, -0.0713]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.2600],\n",
      "        [ 5.6313],\n",
      "        [ 3.3828]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 179, Loss: 0.8449080033717712\n",
      "tensor([[ 0.9930, -0.1178],\n",
      "        [ 0.2248,  0.9744],\n",
      "        [-0.9975, -0.0713]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.2870],\n",
      "        [ 5.6282],\n",
      "        [ 3.3920]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 180, Loss: 0.7879753861117897\n",
      "tensor([[ 0.9942, -0.1076],\n",
      "        [ 0.2316,  0.9728],\n",
      "        [-0.9975, -0.0709]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.3135],\n",
      "        [ 5.6215],\n",
      "        [ 3.4012]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 181, Loss: 0.83825929486973\n",
      "tensor([[ 0.9943, -0.1071],\n",
      "        [ 0.2371,  0.9715],\n",
      "        [-0.9975, -0.0707]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.3401],\n",
      "        [ 5.6165],\n",
      "        [ 3.4102]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 182, Loss: 0.792807865979224\n",
      "tensor([[ 0.9931, -0.1172],\n",
      "        [ 0.2410,  0.9705],\n",
      "        [-0.9975, -0.0706]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.3670],\n",
      "        [ 5.6129],\n",
      "        [ 3.4193]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 183, Loss: 0.7773210861694304\n",
      "tensor([[ 0.9922, -0.1246],\n",
      "        [ 0.2441,  0.9697],\n",
      "        [-0.9975, -0.0705]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.3942],\n",
      "        [ 5.6108],\n",
      "        [ 3.4285]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 184, Loss: 0.7909684892441066\n",
      "tensor([[ 0.9904, -0.1381],\n",
      "        [ 0.2475,  0.9689],\n",
      "        [-0.9975, -0.0702]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.4212],\n",
      "        [ 5.6074],\n",
      "        [ 3.4377]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 185, Loss: 0.8111217480280958\n",
      "tensor([[ 0.9874, -0.1583],\n",
      "        [ 0.2481,  0.9687],\n",
      "        [-0.9975, -0.0700]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.4473],\n",
      "        [ 5.5973],\n",
      "        [ 3.4468]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 186, Loss: 0.860562892083981\n",
      "tensor([[ 0.9854, -0.1700],\n",
      "        [ 0.2516,  0.9678],\n",
      "        [-0.9976, -0.0699]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.4732],\n",
      "        [ 5.5852],\n",
      "        [ 3.4559]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 187, Loss: 0.8166052316470622\n",
      "tensor([[ 0.9838, -0.1793],\n",
      "        [ 0.2507,  0.9681],\n",
      "        [-0.9976, -0.0697]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.4986],\n",
      "        [ 5.5713],\n",
      "        [ 3.4648]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 188, Loss: 0.844904745987649\n",
      "tensor([[ 0.9834, -0.1813],\n",
      "        [ 0.2515,  0.9679],\n",
      "        [-0.9976, -0.0694]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.5240],\n",
      "        [ 5.5562],\n",
      "        [ 3.4737]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 189, Loss: 0.8196275849306653\n",
      "tensor([[ 0.9851, -0.1717],\n",
      "        [ 0.2501,  0.9682],\n",
      "        [-0.9976, -0.0689]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.5498],\n",
      "        [ 5.5447],\n",
      "        [ 3.4826]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 190, Loss: 0.7789523365660586\n",
      "tensor([[ 0.9872, -0.1595],\n",
      "        [ 0.2460,  0.9693],\n",
      "        [-0.9976, -0.0686]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.5757],\n",
      "        [ 5.5361],\n",
      "        [ 3.4914]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 191, Loss: 0.7916324936628419\n",
      "tensor([[ 0.9909, -0.1346],\n",
      "        [ 0.2426,  0.9701],\n",
      "        [-0.9977, -0.0683]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.6015],\n",
      "        [ 5.5282],\n",
      "        [ 3.5002]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 192, Loss: 0.7983152134189083\n",
      "tensor([[ 0.9934, -0.1148],\n",
      "        [ 0.2351,  0.9720],\n",
      "        [-0.9977, -0.0678]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.6275],\n",
      "        [ 5.5235],\n",
      "        [ 3.5089]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 193, Loss: 0.7839601865045783\n",
      "tensor([[ 0.9957, -0.0930],\n",
      "        [ 0.2252,  0.9743],\n",
      "        [-0.9977, -0.0672]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.6532],\n",
      "        [ 5.5188],\n",
      "        [ 3.5176]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 194, Loss: 0.8121010452936771\n",
      "tensor([[ 0.9974, -0.0727],\n",
      "        [ 0.2103,  0.9776],\n",
      "        [-0.9978, -0.0664]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.6789],\n",
      "        [ 5.5165],\n",
      "        [ 3.5262]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 195, Loss: 0.7836216759532625\n",
      "tensor([[ 0.9976, -0.0698],\n",
      "        [ 0.1956,  0.9807],\n",
      "        [-0.9978, -0.0657]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.7044],\n",
      "        [ 5.5152],\n",
      "        [ 3.5349]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 196, Loss: 0.7962778523064208\n",
      "tensor([[ 0.9972, -0.0747],\n",
      "        [ 0.1872,  0.9823],\n",
      "        [-0.9979, -0.0647]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.7305],\n",
      "        [ 5.5134],\n",
      "        [ 3.5436]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 197, Loss: 0.7872964686220248\n",
      "tensor([[ 0.9961, -0.0879],\n",
      "        [ 0.1788,  0.9839],\n",
      "        [-0.9980, -0.0636]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.7571],\n",
      "        [ 5.5138],\n",
      "        [ 3.5522]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 198, Loss: 0.7781040846226202\n",
      "tensor([[ 0.9945, -0.1049],\n",
      "        [ 0.1711,  0.9853],\n",
      "        [-0.9980, -0.0627]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.7840],\n",
      "        [ 5.5164],\n",
      "        [ 3.5608]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 199, Loss: 0.7804369190414837\n",
      "tensor([[ 0.9917, -0.1284],\n",
      "        [ 0.1688,  0.9857],\n",
      "        [-0.9981, -0.0618]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.8110],\n",
      "        [ 5.5183],\n",
      "        [ 3.5695]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 200, Loss: 0.7958866814916474\n",
      "tensor([[ 0.9886, -0.1509],\n",
      "        [ 0.1664,  0.9861],\n",
      "        [-0.9981, -0.0609]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.8386],\n",
      "        [ 5.5222],\n",
      "        [ 3.5781]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 201, Loss: 0.7691159219411847\n",
      "tensor([[ 0.9839, -0.1789],\n",
      "        [ 0.1731,  0.9849],\n",
      "        [-0.9982, -0.0597]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.8660],\n",
      "        [ 5.5246],\n",
      "        [ 3.5868]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 202, Loss: 0.8100502012745557\n",
      "tensor([[ 0.9803, -0.1977],\n",
      "        [ 0.1834,  0.9830],\n",
      "        [-0.9983, -0.0586]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.8932],\n",
      "        [ 5.5304],\n",
      "        [ 3.5954]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 203, Loss: 0.7629448571182234\n",
      "tensor([[ 0.9788, -0.2050],\n",
      "        [ 0.1936,  0.9811],\n",
      "        [-0.9984, -0.0573]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.9200],\n",
      "        [ 5.5383],\n",
      "        [ 3.6039]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 204, Loss: 0.7850697417905694\n",
      "tensor([[ 0.9800, -0.1989],\n",
      "        [ 0.2042,  0.9789],\n",
      "        [-0.9984, -0.0560]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.9463],\n",
      "        [ 5.5474],\n",
      "        [ 3.6124]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 205, Loss: 0.7883628840838142\n",
      "tensor([[ 0.9829, -0.1839],\n",
      "        [ 0.2110,  0.9775],\n",
      "        [-0.9985, -0.0549]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.9716],\n",
      "        [ 5.5564],\n",
      "        [ 3.6209]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 206, Loss: 0.8129219940557196\n",
      "tensor([[ 0.9873, -0.1588],\n",
      "        [ 0.2165,  0.9763],\n",
      "        [-0.9985, -0.0539]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.9968],\n",
      "        [ 5.5658],\n",
      "        [ 3.6294]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 207, Loss: 0.7915833569286893\n",
      "tensor([[ 0.9901, -0.1401],\n",
      "        [ 0.2219,  0.9751],\n",
      "        [-0.9986, -0.0533]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.0223],\n",
      "        [ 5.5747],\n",
      "        [ 3.6377]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 208, Loss: 0.7873718980921252\n",
      "tensor([[ 0.9930, -0.1178],\n",
      "        [ 0.2226,  0.9749],\n",
      "        [-0.9986, -0.0528]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.0478],\n",
      "        [ 5.5858],\n",
      "        [ 3.6461]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 209, Loss: 0.7796166576695864\n",
      "tensor([[ 0.9945, -0.1050],\n",
      "        [ 0.2255,  0.9742],\n",
      "        [-0.9986, -0.0526]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.0734],\n",
      "        [ 5.5947],\n",
      "        [ 3.6545]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 210, Loss: 0.7944506638340209\n",
      "tensor([[ 0.9940, -0.1090],\n",
      "        [ 0.2253,  0.9743],\n",
      "        [-0.9986, -0.0524]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.0991],\n",
      "        [ 5.6032],\n",
      "        [ 3.6630]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 211, Loss: 0.776533119045759\n",
      "tensor([[ 0.9933, -0.1152],\n",
      "        [ 0.2217,  0.9751],\n",
      "        [-0.9986, -0.0524]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.1243],\n",
      "        [ 5.6107],\n",
      "        [ 3.6714]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 212, Loss: 0.7929746386753059\n",
      "tensor([[ 0.9925, -0.1225],\n",
      "        [ 0.2143,  0.9768],\n",
      "        [-0.9986, -0.0524]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.1499],\n",
      "        [ 5.6189],\n",
      "        [ 3.6798]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 213, Loss: 0.7757544590831146\n",
      "tensor([[ 0.9905, -0.1375],\n",
      "        [ 0.2084,  0.9780],\n",
      "        [-0.9986, -0.0521]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.1753],\n",
      "        [ 5.6268],\n",
      "        [ 3.6881]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 214, Loss: 0.8022514549195833\n",
      "tensor([[ 0.9889, -0.1487],\n",
      "        [ 0.1994,  0.9799],\n",
      "        [-0.9987, -0.0516]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.2007],\n",
      "        [ 5.6364],\n",
      "        [ 3.6965]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 215, Loss: 0.7672556851992083\n",
      "tensor([[ 0.9872, -0.1598],\n",
      "        [ 0.1901,  0.9818],\n",
      "        [-0.9987, -0.0511]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.2258],\n",
      "        [ 5.6472],\n",
      "        [ 3.7047]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 216, Loss: 0.7821100043385051\n",
      "tensor([[ 0.9859, -0.1673],\n",
      "        [ 0.1813,  0.9834],\n",
      "        [-0.9987, -0.0507]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.2510],\n",
      "        [ 5.6571],\n",
      "        [ 3.7128]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 217, Loss: 0.8016364957427061\n",
      "tensor([[ 0.9863, -0.1647],\n",
      "        [ 0.1725,  0.9850],\n",
      "        [-0.9987, -0.0502]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.2766],\n",
      "        [ 5.6666],\n",
      "        [ 3.7208]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 218, Loss: 0.771967898653224\n",
      "tensor([[ 0.9872, -0.1597],\n",
      "        [ 0.1671,  0.9859],\n",
      "        [-0.9988, -0.0497]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.3023],\n",
      "        [ 5.6749],\n",
      "        [ 3.7288]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 219, Loss: 0.8025803164618133\n",
      "tensor([[ 0.9883, -0.1526],\n",
      "        [ 0.1670,  0.9860],\n",
      "        [-0.9988, -0.0488]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.3283],\n",
      "        [ 5.6852],\n",
      "        [ 3.7366]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 220, Loss: 0.7654220550968208\n",
      "tensor([[ 0.9895, -0.1446],\n",
      "        [ 0.1681,  0.9858],\n",
      "        [-0.9988, -0.0480]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.3543],\n",
      "        [ 5.6967],\n",
      "        [ 3.7445]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 221, Loss: 0.7564904305291185\n",
      "tensor([[ 0.9899, -0.1419],\n",
      "        [ 0.1685,  0.9857],\n",
      "        [-0.9989, -0.0475]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.3795],\n",
      "        [ 5.7052],\n",
      "        [ 3.7526]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 222, Loss: 0.7969959170720435\n",
      "tensor([[ 0.9894, -0.1453],\n",
      "        [ 0.1745,  0.9847],\n",
      "        [-0.9989, -0.0469]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.4044],\n",
      "        [ 5.7126],\n",
      "        [ 3.7606]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 223, Loss: 0.7998026264000044\n",
      "tensor([[ 0.9898, -0.1423],\n",
      "        [ 0.1779,  0.9840],\n",
      "        [-0.9989, -0.0460]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.4286],\n",
      "        [ 5.7202],\n",
      "        [ 3.7686]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 224, Loss: 0.8129737960918918\n",
      "tensor([[ 0.9908, -0.1355],\n",
      "        [ 0.1775,  0.9841],\n",
      "        [-0.9990, -0.0454]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.4529],\n",
      "        [ 5.7256],\n",
      "        [ 3.7765]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 225, Loss: 0.7920286587431901\n",
      "tensor([[ 0.9912, -0.1323],\n",
      "        [ 0.1799,  0.9837],\n",
      "        [-0.9990, -0.0447]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.4770],\n",
      "        [ 5.7304],\n",
      "        [ 3.7844]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 226, Loss: 0.7877053510133599\n",
      "tensor([[ 0.9913, -0.1317],\n",
      "        [ 0.1858,  0.9826],\n",
      "        [-0.9990, -0.0443]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.5014],\n",
      "        [ 5.7319],\n",
      "        [ 3.7922]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 227, Loss: 0.7980109978783624\n",
      "tensor([[ 0.9919, -0.1273],\n",
      "        [ 0.1901,  0.9818],\n",
      "        [-0.9990, -0.0439]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.5255],\n",
      "        [ 5.7305],\n",
      "        [ 3.8001]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 228, Loss: 0.8151854335332098\n",
      "tensor([[ 0.9932, -0.1165],\n",
      "        [ 0.1891,  0.9820],\n",
      "        [-0.9990, -0.0440]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.5498],\n",
      "        [ 5.7309],\n",
      "        [ 3.8079]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 229, Loss: 0.772478250882115\n",
      "tensor([[ 0.9932, -0.1162],\n",
      "        [ 0.1925,  0.9813],\n",
      "        [-0.9990, -0.0437]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.5741],\n",
      "        [ 5.7315],\n",
      "        [ 3.8157]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 230, Loss: 0.8012262414364509\n",
      "tensor([[ 0.9924, -0.1229],\n",
      "        [ 0.1960,  0.9806],\n",
      "        [-0.9991, -0.0432]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.5990],\n",
      "        [ 5.7315],\n",
      "        [ 3.8234]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 231, Loss: 0.7754825772943642\n",
      "tensor([[ 0.9909, -0.1348],\n",
      "        [ 0.2023,  0.9793],\n",
      "        [-0.9991, -0.0425]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.6231],\n",
      "        [ 5.7315],\n",
      "        [ 3.8312]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 232, Loss: 0.8002216417710294\n",
      "tensor([[ 0.9881, -0.1536],\n",
      "        [ 0.2106,  0.9776],\n",
      "        [-0.9991, -0.0419]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.6468],\n",
      "        [ 5.7309],\n",
      "        [ 3.8390]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 233, Loss: 0.7973330678049629\n",
      "tensor([[ 0.9857, -0.1683],\n",
      "        [ 0.2181,  0.9759],\n",
      "        [-0.9991, -0.0414]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.6712],\n",
      "        [ 5.7306],\n",
      "        [ 3.8466]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 234, Loss: 0.7701567007549448\n",
      "tensor([[ 0.9836, -0.1805],\n",
      "        [ 0.2252,  0.9743],\n",
      "        [-0.9992, -0.0407]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.6956],\n",
      "        [ 5.7315],\n",
      "        [ 3.8542]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 235, Loss: 0.7745426976016493\n",
      "tensor([[ 0.9842, -0.1771],\n",
      "        [ 0.2317,  0.9728],\n",
      "        [-0.9992, -0.0401]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.7193],\n",
      "        [ 5.7330],\n",
      "        [ 3.8618]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 236, Loss: 0.7992722668185476\n",
      "tensor([[ 0.9861, -0.1664],\n",
      "        [ 0.2383,  0.9712],\n",
      "        [-0.9992, -0.0398]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.7422],\n",
      "        [ 5.7357],\n",
      "        [ 3.8694]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 237, Loss: 0.782783855325107\n",
      "tensor([[ 0.9880, -0.1546],\n",
      "        [ 0.2407,  0.9706],\n",
      "        [-0.9992, -0.0396]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.7642],\n",
      "        [ 5.7421],\n",
      "        [ 3.8770]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 238, Loss: 0.7724975487939565\n",
      "tensor([[ 0.9900, -0.1408],\n",
      "        [ 0.2438,  0.9698],\n",
      "        [-0.9992, -0.0394]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.7862],\n",
      "        [ 5.7479],\n",
      "        [ 3.8845]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 239, Loss: 0.7925815083137033\n",
      "tensor([[ 0.9910, -0.1342],\n",
      "        [ 0.2457,  0.9694],\n",
      "        [-0.9992, -0.0394]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.8074],\n",
      "        [ 5.7528],\n",
      "        [ 3.8919]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 240, Loss: 0.8091610235996356\n",
      "tensor([[ 0.9916, -0.1293],\n",
      "        [ 0.2425,  0.9702],\n",
      "        [-0.9992, -0.0397]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.8291],\n",
      "        [ 5.7565],\n",
      "        [ 3.8993]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 241, Loss: 0.7846810718791154\n",
      "tensor([[ 0.9930, -0.1180],\n",
      "        [ 0.2390,  0.9710],\n",
      "        [-0.9992, -0.0397]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.8503],\n",
      "        [ 5.7618],\n",
      "        [ 3.9067]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 242, Loss: 0.7902178597547787\n",
      "tensor([[ 0.9951, -0.0987],\n",
      "        [ 0.2371,  0.9715],\n",
      "        [-0.9992, -0.0397]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.8713],\n",
      "        [ 5.7685],\n",
      "        [ 3.9141]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 243, Loss: 0.7783830322366324\n",
      "tensor([[ 0.9954, -0.0962],\n",
      "        [ 0.2390,  0.9710],\n",
      "        [-0.9992, -0.0400]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.8917],\n",
      "        [ 5.7701],\n",
      "        [ 3.9215]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 244, Loss: 0.8198412711351466\n",
      "tensor([[ 0.9941, -0.1081],\n",
      "        [ 0.2411,  0.9705],\n",
      "        [-0.9992, -0.0403]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.9125],\n",
      "        [ 5.7673],\n",
      "        [ 3.9288]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 245, Loss: 0.8197390095803826\n",
      "tensor([[ 0.9927, -0.1203],\n",
      "        [ 0.2384,  0.9712],\n",
      "        [-0.9992, -0.0406]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.9328],\n",
      "        [ 5.7669],\n",
      "        [ 3.9362]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 246, Loss: 0.7742015858031065\n",
      "tensor([[ 0.9910, -0.1338],\n",
      "        [ 0.2374,  0.9714],\n",
      "        [-0.9992, -0.0412]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.9531],\n",
      "        [ 5.7656],\n",
      "        [ 3.9437]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 247, Loss: 0.7898246609220537\n",
      "tensor([[ 0.9901, -0.1407],\n",
      "        [ 0.2317,  0.9728],\n",
      "        [-0.9991, -0.0420]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.9737],\n",
      "        [ 5.7668],\n",
      "        [ 3.9513]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 248, Loss: 0.7603879913393881\n",
      "tensor([[ 0.9892, -0.1467],\n",
      "        [ 0.2292,  0.9734],\n",
      "        [-0.9991, -0.0428]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.9939],\n",
      "        [ 5.7662],\n",
      "        [ 3.9589]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 249, Loss: 0.8043407883371039\n",
      "tensor([[ 0.9887, -0.1501],\n",
      "        [ 0.2252,  0.9743],\n",
      "        [-0.9990, -0.0436]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.0146],\n",
      "        [ 5.7649],\n",
      "        [ 3.9665]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 250, Loss: 0.7774585589050406\n",
      "tensor([[ 0.9884, -0.1521],\n",
      "        [ 0.2259,  0.9742],\n",
      "        [-0.9990, -0.0443]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.0353],\n",
      "        [ 5.7567],\n",
      "        [ 3.9742]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 251, Loss: 0.8245790809952487\n",
      "tensor([[ 0.9893, -0.1462],\n",
      "        [ 0.2255,  0.9742],\n",
      "        [-0.9990, -0.0445]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.0559],\n",
      "        [ 5.7507],\n",
      "        [ 3.9818]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 252, Loss: 0.7893151097913791\n",
      "tensor([[ 0.9904, -0.1384],\n",
      "        [ 0.2221,  0.9750],\n",
      "        [-0.9990, -0.0447]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.0768],\n",
      "        [ 5.7457],\n",
      "        [ 3.9892]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 253, Loss: 0.7871962949611945\n",
      "tensor([[ 0.9922, -0.1249],\n",
      "        [ 0.2156,  0.9765],\n",
      "        [-0.9990, -0.0450]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.0976],\n",
      "        [ 5.7370],\n",
      "        [ 3.9965]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 254, Loss: 0.8173766196396822\n",
      "tensor([[ 0.9927, -0.1209],\n",
      "        [ 0.2172,  0.9761],\n",
      "        [-0.9990, -0.0449]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.1184],\n",
      "        [ 5.7237],\n",
      "        [ 4.0039]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 255, Loss: 0.8129055880870826\n",
      "tensor([[ 0.9932, -0.1162],\n",
      "        [ 0.2178,  0.9760],\n",
      "        [-0.9990, -0.0450]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.1388],\n",
      "        [ 5.7081],\n",
      "        [ 4.0111]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 256, Loss: 0.8516475578639554\n",
      "tensor([[ 0.9942, -0.1079],\n",
      "        [ 0.2104,  0.9776],\n",
      "        [-0.9990, -0.0452]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.1592],\n",
      "        [ 5.6963],\n",
      "        [ 4.0183]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 257, Loss: 0.7646537377751476\n",
      "tensor([[ 0.9940, -0.1094],\n",
      "        [ 0.2040,  0.9790],\n",
      "        [-0.9990, -0.0454]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.1793],\n",
      "        [ 5.6821],\n",
      "        [ 4.0254]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 258, Loss: 0.8179745584797541\n",
      "tensor([[ 0.9932, -0.1164],\n",
      "        [ 0.2016,  0.9795],\n",
      "        [-0.9989, -0.0459]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.1993],\n",
      "        [ 5.6645],\n",
      "        [ 4.0326]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 259, Loss: 0.8178833058745552\n",
      "tensor([[ 0.9920, -0.1266],\n",
      "        [ 0.2039,  0.9790],\n",
      "        [-0.9989, -0.0463]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.2195],\n",
      "        [ 5.6453],\n",
      "        [ 4.0397]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 260, Loss: 0.7989148558284169\n",
      "tensor([[ 0.9901, -0.1407],\n",
      "        [ 0.2028,  0.9792],\n",
      "        [-0.9989, -0.0470]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.2407],\n",
      "        [ 5.6281],\n",
      "        [ 4.0469]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 261, Loss: 0.7586186222271001\n",
      "tensor([[ 0.9875, -0.1576],\n",
      "        [ 0.2032,  0.9791],\n",
      "        [-0.9989, -0.0477]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.2615],\n",
      "        [ 5.6149],\n",
      "        [ 4.0540]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 262, Loss: 0.7840976240283223\n",
      "tensor([[ 0.9862, -0.1655],\n",
      "        [ 0.2055,  0.9787],\n",
      "        [-0.9988, -0.0483]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.2832],\n",
      "        [ 5.6038],\n",
      "        [ 4.0611]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 263, Loss: 0.7715029860990705\n",
      "tensor([[ 0.9853, -0.1707],\n",
      "        [ 0.2065,  0.9784],\n",
      "        [-0.9988, -0.0490]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.3041],\n",
      "        [ 5.5975],\n",
      "        [ 4.0682]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 264, Loss: 0.767728771308963\n",
      "tensor([[ 0.9856, -0.1692],\n",
      "        [ 0.2093,  0.9779],\n",
      "        [-0.9988, -0.0495]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.3248],\n",
      "        [ 5.5964],\n",
      "        [ 4.0752]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 265, Loss: 0.7574994299883514\n",
      "tensor([[ 0.9863, -0.1651],\n",
      "        [ 0.2156,  0.9765],\n",
      "        [-0.9988, -0.0497]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.3456],\n",
      "        [ 5.5899],\n",
      "        [ 4.0822]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 266, Loss: 0.8259623925544255\n",
      "tensor([[ 0.9871, -0.1599],\n",
      "        [ 0.2214,  0.9752],\n",
      "        [-0.9988, -0.0499]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.3674],\n",
      "        [ 5.5835],\n",
      "        [ 4.0890]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 267, Loss: 0.7724509333287747\n",
      "tensor([[ 0.9884, -0.1516],\n",
      "        [ 0.2306,  0.9730],\n",
      "        [-0.9987, -0.0502]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.3886],\n",
      "        [ 5.5766],\n",
      "        [ 4.0960]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 268, Loss: 0.7921774712352461\n",
      "tensor([[ 0.9904, -0.1385],\n",
      "        [ 0.2356,  0.9718],\n",
      "        [-0.9987, -0.0502]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.4097],\n",
      "        [ 5.5692],\n",
      "        [ 4.1029]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 269, Loss: 0.8024102188832428\n",
      "tensor([[ 0.9917, -0.1288],\n",
      "        [ 0.2362,  0.9717],\n",
      "        [-0.9987, -0.0503]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.4304],\n",
      "        [ 5.5632],\n",
      "        [ 4.1098]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 270, Loss: 0.7812742997134164\n",
      "tensor([[ 0.9934, -0.1149],\n",
      "        [ 0.2342,  0.9722],\n",
      "        [-0.9987, -0.0504]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.4506],\n",
      "        [ 5.5557],\n",
      "        [ 4.1166]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 271, Loss: 0.8245597480927948\n",
      "tensor([[ 0.9942, -0.1074],\n",
      "        [ 0.2281,  0.9736],\n",
      "        [-0.9987, -0.0506]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.4714],\n",
      "        [ 5.5491],\n",
      "        [ 4.1233]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 272, Loss: 0.779954117711315\n",
      "tensor([[ 0.9941, -0.1081],\n",
      "        [ 0.2167,  0.9762],\n",
      "        [-0.9987, -0.0506]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.4923],\n",
      "        [ 5.5459],\n",
      "        [ 4.1300]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 273, Loss: 0.7661079465297835\n",
      "tensor([[ 0.9941, -0.1084],\n",
      "        [ 0.2019,  0.9794],\n",
      "        [-0.9987, -0.0507]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.5128],\n",
      "        [ 5.5458],\n",
      "        [ 4.1366]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 274, Loss: 0.7893557940247391\n",
      "tensor([[ 0.9920, -0.1259],\n",
      "        [ 0.1887,  0.9820],\n",
      "        [-0.9987, -0.0508]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.5332],\n",
      "        [ 5.5494],\n",
      "        [ 4.1434]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 275, Loss: 0.7507450884297758\n",
      "tensor([[ 0.9888, -0.1493],\n",
      "        [ 0.1839,  0.9829],\n",
      "        [-0.9987, -0.0504]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.5536],\n",
      "        [ 5.5514],\n",
      "        [ 4.1500]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 276, Loss: 0.7862382328832695\n",
      "tensor([[ 0.9858, -0.1677],\n",
      "        [ 0.1866,  0.9824],\n",
      "        [-0.9987, -0.0500]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.5731],\n",
      "        [ 5.5514],\n",
      "        [ 4.1566]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 277, Loss: 0.8130198809283463\n",
      "tensor([[ 0.9856, -0.1694],\n",
      "        [ 0.1880,  0.9822],\n",
      "        [-0.9988, -0.0499]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.5929],\n",
      "        [ 5.5544],\n",
      "        [ 4.1633]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 278, Loss: 0.7699450300646152\n",
      "tensor([[ 0.9857, -0.1685],\n",
      "        [ 0.1887,  0.9820],\n",
      "        [-0.9988, -0.0496]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.6127],\n",
      "        [ 5.5571],\n",
      "        [ 4.1697]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 279, Loss: 0.7935050071689013\n",
      "tensor([[ 0.9868, -0.1618],\n",
      "        [ 0.1867,  0.9824],\n",
      "        [-0.9988, -0.0495]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.6312],\n",
      "        [ 5.5575],\n",
      "        [ 4.1763]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 280, Loss: 0.8082287850622021\n",
      "tensor([[ 0.9889, -0.1484],\n",
      "        [ 0.1866,  0.9824],\n",
      "        [-0.9988, -0.0492]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.6502],\n",
      "        [ 5.5569],\n",
      "        [ 4.1828]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 281, Loss: 0.7897778776200596\n",
      "tensor([[ 0.9906, -0.1369],\n",
      "        [ 0.1916,  0.9815],\n",
      "        [-0.9988, -0.0490]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.6684],\n",
      "        [ 5.5578],\n",
      "        [ 4.1895]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 282, Loss: 0.778020209548933\n",
      "tensor([[ 0.9927, -0.1204],\n",
      "        [ 0.1914,  0.9815],\n",
      "        [-0.9988, -0.0490]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.6867],\n",
      "        [ 5.5604],\n",
      "        [ 4.1961]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 283, Loss: 0.7730579765868043\n",
      "tensor([[ 0.9949, -0.1006],\n",
      "        [ 0.1924,  0.9813],\n",
      "        [-0.9988, -0.0484]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.7056],\n",
      "        [ 5.5650],\n",
      "        [ 4.2026]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 284, Loss: 0.78358143372407\n",
      "tensor([[ 0.9952, -0.0982],\n",
      "        [ 0.1991,  0.9800],\n",
      "        [-0.9989, -0.0476]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.7245],\n",
      "        [ 5.5650],\n",
      "        [ 4.2090]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 285, Loss: 0.8140795874256791\n",
      "tensor([[ 0.9946, -0.1041],\n",
      "        [ 0.2054,  0.9787],\n",
      "        [-0.9989, -0.0467]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.7436],\n",
      "        [ 5.5677],\n",
      "        [ 4.2155]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 286, Loss: 0.7584162399411349\n",
      "tensor([[ 0.9924, -0.1227],\n",
      "        [ 0.2163,  0.9763],\n",
      "        [-0.9990, -0.0455]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.7618],\n",
      "        [ 5.5628],\n",
      "        [ 4.2219]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 287, Loss: 0.8485249178108487\n",
      "tensor([[ 0.9891, -0.1475],\n",
      "        [ 0.2260,  0.9741],\n",
      "        [-0.9990, -0.0447]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.7804],\n",
      "        [ 5.5578],\n",
      "        [ 4.2284]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 288, Loss: 0.7842276915035673\n",
      "tensor([[ 0.9856, -0.1692],\n",
      "        [ 0.2368,  0.9715],\n",
      "        [-0.9990, -0.0439]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.7992],\n",
      "        [ 5.5520],\n",
      "        [ 4.2348]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 289, Loss: 0.7857472514636256\n",
      "tensor([[ 0.9819, -0.1896],\n",
      "        [ 0.2462,  0.9692],\n",
      "        [-0.9991, -0.0432]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.8177],\n",
      "        [ 5.5515],\n",
      "        [ 4.2414]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 290, Loss: 0.7502107217305032\n",
      "tensor([[ 0.9810, -0.1942],\n",
      "        [ 0.2485,  0.9686],\n",
      "        [-0.9991, -0.0428]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.8354],\n",
      "        [ 5.5481],\n",
      "        [ 4.2478]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 291, Loss: 0.8192990882386708\n",
      "tensor([[ 0.9831, -0.1830],\n",
      "        [ 0.2446,  0.9696],\n",
      "        [-0.9991, -0.0420]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.8531],\n",
      "        [ 5.5470],\n",
      "        [ 4.2542]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 292, Loss: 0.7704535884268073\n",
      "tensor([[ 0.9861, -0.1662],\n",
      "        [ 0.2382,  0.9712],\n",
      "        [-0.9992, -0.0409]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.8715],\n",
      "        [ 5.5502],\n",
      "        [ 4.2606]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 293, Loss: 0.7522377498107415\n",
      "tensor([[ 0.9894, -0.1452],\n",
      "        [ 0.2317,  0.9728],\n",
      "        [-0.9992, -0.0399]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.8898],\n",
      "        [ 5.5549],\n",
      "        [ 4.2670]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 294, Loss: 0.7741227744448558\n",
      "tensor([[ 0.9926, -0.1210],\n",
      "        [ 0.2211,  0.9753],\n",
      "        [-0.9992, -0.0390]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.9084],\n",
      "        [ 5.5632],\n",
      "        [ 4.2734]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 295, Loss: 0.7572748438751137\n",
      "tensor([[ 0.9958, -0.0911],\n",
      "        [ 0.2088,  0.9780],\n",
      "        [-0.9993, -0.0382]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.9272],\n",
      "        [ 5.5754],\n",
      "        [ 4.2799]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 296, Loss: 0.746695100919489\n",
      "tensor([[ 0.9976, -0.0698],\n",
      "        [ 0.2021,  0.9794],\n",
      "        [-0.9993, -0.0379]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.9457],\n",
      "        [ 5.5856],\n",
      "        [ 4.2863]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 297, Loss: 0.7905295153070045\n",
      "tensor([[ 0.9975, -0.0708],\n",
      "        [ 0.1945,  0.9809],\n",
      "        [-0.9993, -0.0380]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.9648],\n",
      "        [ 5.5940],\n",
      "        [ 4.2926]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 298, Loss: 0.7898458491992483\n",
      "tensor([[ 0.9963, -0.0856],\n",
      "        [ 0.1847,  0.9828],\n",
      "        [-0.9993, -0.0376]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.9840],\n",
      "        [ 5.6045],\n",
      "        [ 4.2988]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 299, Loss: 0.7812340482892498\n",
      "tensor([[ 0.9937, -0.1119],\n",
      "        [ 0.1813,  0.9834],\n",
      "        [-0.9993, -0.0373]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.0025],\n",
      "        [ 5.6107],\n",
      "        [ 4.3049]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 300, Loss: 0.810400178247366\n",
      "tensor([[ 0.9909, -0.1345],\n",
      "        [ 0.1779,  0.9841],\n",
      "        [-0.9993, -0.0375]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.0207],\n",
      "        [ 5.6158],\n",
      "        [ 4.3109]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 301, Loss: 0.8030891221416413\n",
      "tensor([[ 0.9867, -0.1628],\n",
      "        [ 0.1839,  0.9830],\n",
      "        [-0.9993, -0.0373]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.0395],\n",
      "        [ 5.6207],\n",
      "        [ 4.3170]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 302, Loss: 0.7570849902197414\n",
      "tensor([[ 0.9826, -0.1855],\n",
      "        [ 0.1933,  0.9811],\n",
      "        [-0.9993, -0.0373]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.0573],\n",
      "        [ 5.6271],\n",
      "        [ 4.3231]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 303, Loss: 0.7758379377977742\n",
      "tensor([[ 0.9815, -0.1914],\n",
      "        [ 0.2005,  0.9797],\n",
      "        [-0.9993, -0.0374]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.0749],\n",
      "        [ 5.6318],\n",
      "        [ 4.3292]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 304, Loss: 0.7919897770577484\n",
      "tensor([[ 0.9834, -0.1814],\n",
      "        [ 0.2050,  0.9788],\n",
      "        [-0.9993, -0.0379]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.0924],\n",
      "        [ 5.6338],\n",
      "        [ 4.3354]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 305, Loss: 0.794766095252401\n",
      "tensor([[ 0.9878, -0.1557],\n",
      "        [ 0.2057,  0.9786],\n",
      "        [-0.9993, -0.0382]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.1097],\n",
      "        [ 5.6353],\n",
      "        [ 4.3415]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 306, Loss: 0.7905141007728279\n",
      "tensor([[ 0.9921, -0.1252],\n",
      "        [ 0.2058,  0.9786],\n",
      "        [-0.9992, -0.0389]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.1274],\n",
      "        [ 5.6383],\n",
      "        [ 4.3476]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 307, Loss: 0.7746428851393743\n",
      "tensor([[ 0.9944, -0.1057],\n",
      "        [ 0.2118,  0.9773],\n",
      "        [-0.9992, -0.0392]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.1462],\n",
      "        [ 5.6434],\n",
      "        [ 4.3537]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 308, Loss: 0.7450503135904522\n",
      "tensor([[ 0.9956, -0.0932],\n",
      "        [ 0.2208,  0.9753],\n",
      "        [-0.9992, -0.0397]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.1648],\n",
      "        [ 5.6456],\n",
      "        [ 4.3598]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 309, Loss: 0.8017444862230078\n",
      "tensor([[ 0.9961, -0.0878],\n",
      "        [ 0.2252,  0.9743],\n",
      "        [-0.9992, -0.0406]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.1830],\n",
      "        [ 5.6497],\n",
      "        [ 4.3658]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 310, Loss: 0.7797469970416896\n",
      "tensor([[ 0.9952, -0.0977],\n",
      "        [ 0.2358,  0.9718],\n",
      "        [-0.9992, -0.0408]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.2012],\n",
      "        [ 5.6484],\n",
      "        [ 4.3719]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 311, Loss: 0.8179943163632042\n",
      "tensor([[ 0.9922, -0.1250],\n",
      "        [ 0.2496,  0.9684],\n",
      "        [-0.9992, -0.0410]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.2184],\n",
      "        [ 5.6441],\n",
      "        [ 4.3779]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 312, Loss: 0.8174655931495906\n",
      "tensor([[ 0.9892, -0.1463],\n",
      "        [ 0.2557,  0.9668],\n",
      "        [-0.9992, -0.0410]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.2349],\n",
      "        [ 5.6383],\n",
      "        [ 4.3840]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 313, Loss: 0.8190636812662713\n",
      "tensor([[ 0.9865, -0.1640],\n",
      "        [ 0.2561,  0.9666],\n",
      "        [-0.9992, -0.0409]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.2518],\n",
      "        [ 5.6373],\n",
      "        [ 4.3902]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 314, Loss: 0.7548919010980096\n",
      "tensor([[ 0.9858, -0.1678],\n",
      "        [ 0.2508,  0.9680],\n",
      "        [-0.9992, -0.0409]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.2694],\n",
      "        [ 5.6316],\n",
      "        [ 4.3965]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 315, Loss: 0.8041352681125802\n",
      "tensor([[ 0.9871, -0.1603],\n",
      "        [ 0.2417,  0.9704],\n",
      "        [-0.9992, -0.0404]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.2868],\n",
      "        [ 5.6233],\n",
      "        [ 4.4027]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 316, Loss: 0.813554025385267\n",
      "tensor([[ 0.9903, -0.1392],\n",
      "        [ 0.2264,  0.9740],\n",
      "        [-0.9992, -0.0404]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.3040],\n",
      "        [ 5.6181],\n",
      "        [ 4.4090]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 317, Loss: 0.7490199424922143\n",
      "tensor([[ 0.9941, -0.1088],\n",
      "        [ 0.2129,  0.9771],\n",
      "        [-0.9992, -0.0403]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.3218],\n",
      "        [ 5.6131],\n",
      "        [ 4.4154]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 318, Loss: 0.7781877684690574\n",
      "tensor([[ 0.9960, -0.0889],\n",
      "        [ 0.1952,  0.9808],\n",
      "        [-0.9992, -0.0404]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.3387],\n",
      "        [ 5.6101],\n",
      "        [ 4.4216]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 319, Loss: 0.7794006988754668\n",
      "tensor([[ 0.9971, -0.0758],\n",
      "        [ 0.1838,  0.9830],\n",
      "        [-0.9992, -0.0410]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.3560],\n",
      "        [ 5.6034],\n",
      "        [ 4.4279]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 320, Loss: 0.8002410195992015\n",
      "tensor([[ 0.9975, -0.0707],\n",
      "        [ 0.1743,  0.9847],\n",
      "        [-0.9991, -0.0414]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.3738],\n",
      "        [ 5.5965],\n",
      "        [ 4.4339]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 321, Loss: 0.7888214661765308\n",
      "tensor([[ 0.9968, -0.0800],\n",
      "        [ 0.1696,  0.9855],\n",
      "        [-0.9991, -0.0418]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.3917],\n",
      "        [ 5.5912],\n",
      "        [ 4.4398]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 322, Loss: 0.7786874945136116\n",
      "tensor([[ 0.9950, -0.0996],\n",
      "        [ 0.1664,  0.9861],\n",
      "        [-0.9991, -0.0421]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.4093],\n",
      "        [ 5.5899],\n",
      "        [ 4.4457]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 323, Loss: 0.7750764741961484\n",
      "tensor([[ 0.9919, -0.1273],\n",
      "        [ 0.1697,  0.9855],\n",
      "        [-0.9991, -0.0425]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.4269],\n",
      "        [ 5.5864],\n",
      "        [ 4.4516]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 324, Loss: 0.7896602769031628\n",
      "tensor([[ 0.9906, -0.1366],\n",
      "        [ 0.1733,  0.9849],\n",
      "        [-0.9991, -0.0429]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.4447],\n",
      "        [ 5.5868],\n",
      "        [ 4.4575]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 325, Loss: 0.7577553489415211\n",
      "tensor([[ 0.9904, -0.1381],\n",
      "        [ 0.1769,  0.9842],\n",
      "        [-0.9991, -0.0431]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.4619],\n",
      "        [ 5.5888],\n",
      "        [ 4.4633]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 326, Loss: 0.7848723112667215\n",
      "tensor([[ 0.9904, -0.1384],\n",
      "        [ 0.1767,  0.9843],\n",
      "        [-0.9991, -0.0432]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.4789],\n",
      "        [ 5.5925],\n",
      "        [ 4.4692]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 327, Loss: 0.7731791213178625\n",
      "tensor([[ 0.9909, -0.1344],\n",
      "        [ 0.1756,  0.9845],\n",
      "        [-0.9991, -0.0429]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.4962],\n",
      "        [ 5.5990],\n",
      "        [ 4.4751]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 328, Loss: 0.7585955806976563\n",
      "tensor([[ 0.9908, -0.1352],\n",
      "        [ 0.1816,  0.9834],\n",
      "        [-0.9991, -0.0427]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.5139],\n",
      "        [ 5.6056],\n",
      "        [ 4.4811]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 329, Loss: 0.7640269864033236\n",
      "tensor([[ 0.9905, -0.1378],\n",
      "        [ 0.1854,  0.9827],\n",
      "        [-0.9991, -0.0430]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.5318],\n",
      "        [ 5.6135],\n",
      "        [ 4.4868]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 330, Loss: 0.7878990501469936\n",
      "tensor([[ 0.9912, -0.1326],\n",
      "        [ 0.1879,  0.9822],\n",
      "        [-0.9991, -0.0433]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.5501],\n",
      "        [ 5.6236],\n",
      "        [ 4.4924]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 331, Loss: 0.770472577963175\n",
      "tensor([[ 0.9912, -0.1324],\n",
      "        [ 0.1968,  0.9804],\n",
      "        [-0.9991, -0.0432]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.5680],\n",
      "        [ 5.6299],\n",
      "        [ 4.4981]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 332, Loss: 0.8067668235294686\n",
      "tensor([[ 0.9926, -0.1214],\n",
      "        [ 0.2043,  0.9789],\n",
      "        [-0.9991, -0.0427]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.5856],\n",
      "        [ 5.6351],\n",
      "        [ 4.5036]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 333, Loss: 0.821305084821319\n",
      "tensor([[ 0.9947, -0.1030],\n",
      "        [ 0.2137,  0.9769],\n",
      "        [-0.9991, -0.0421]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.6034],\n",
      "        [ 5.6408],\n",
      "        [ 4.5090]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 334, Loss: 0.7752586019478092\n",
      "tensor([[ 0.9950, -0.0998],\n",
      "        [ 0.2274,  0.9738],\n",
      "        [-0.9992, -0.0412]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.6210],\n",
      "        [ 5.6440],\n",
      "        [ 4.5145]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 335, Loss: 0.7958844414878318\n",
      "tensor([[ 0.9943, -0.1069],\n",
      "        [ 0.2431,  0.9700],\n",
      "        [-0.9992, -0.0404]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.6392],\n",
      "        [ 5.6437],\n",
      "        [ 4.5201]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 336, Loss: 0.7805761699977499\n",
      "tensor([[ 0.9916, -0.1292],\n",
      "        [ 0.2632,  0.9647],\n",
      "        [-0.9992, -0.0397]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.6565],\n",
      "        [ 5.6374],\n",
      "        [ 4.5258]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 337, Loss: 0.8101269863353324\n",
      "tensor([[ 0.9880, -0.1547],\n",
      "        [ 0.2834,  0.9590],\n",
      "        [-0.9992, -0.0397]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.6730],\n",
      "        [ 5.6300],\n",
      "        [ 4.5316]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 338, Loss: 0.7860049008615927\n",
      "tensor([[ 0.9869, -0.1614],\n",
      "        [ 0.2944,  0.9557],\n",
      "        [-0.9992, -0.0396]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.6902],\n",
      "        [ 5.6234],\n",
      "        [ 4.5374]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 339, Loss: 0.7796612993704609\n",
      "tensor([[ 0.9884, -0.1516],\n",
      "        [ 0.2979,  0.9546],\n",
      "        [-0.9992, -0.0397]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.7061],\n",
      "        [ 5.6182],\n",
      "        [ 4.5433]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 340, Loss: 0.8001635391875282\n",
      "tensor([[ 0.9906, -0.1366],\n",
      "        [ 0.2940,  0.9558],\n",
      "        [-0.9992, -0.0401]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.7228],\n",
      "        [ 5.6124],\n",
      "        [ 4.5492]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 341, Loss: 0.7804728065552332\n",
      "tensor([[ 0.9917, -0.1286],\n",
      "        [ 0.2848,  0.9586],\n",
      "        [-0.9992, -0.0407]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.7388],\n",
      "        [ 5.6082],\n",
      "        [ 4.5550]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 342, Loss: 0.792648477493049\n",
      "tensor([[ 0.9936, -0.1127],\n",
      "        [ 0.2702,  0.9628],\n",
      "        [-0.9992, -0.0411]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.7545],\n",
      "        [ 5.6053],\n",
      "        [ 4.5608]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 343, Loss: 0.7884085463897221\n",
      "tensor([[ 0.9947, -0.1031],\n",
      "        [ 0.2534,  0.9674],\n",
      "        [-0.9991, -0.0415]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.7707],\n",
      "        [ 5.6017],\n",
      "        [ 4.5665]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 344, Loss: 0.785807208537692\n",
      "tensor([[ 0.9955, -0.0950],\n",
      "        [ 0.2309,  0.9730],\n",
      "        [-0.9991, -0.0416]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.7881],\n",
      "        [ 5.6007],\n",
      "        [ 4.5721]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 345, Loss: 0.750673913295758\n",
      "tensor([[ 0.9950, -0.0995],\n",
      "        [ 0.2110,  0.9775],\n",
      "        [-0.9991, -0.0416]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.8054],\n",
      "        [ 5.5977],\n",
      "        [ 4.5777]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 346, Loss: 0.7973392546694009\n",
      "tensor([[ 0.9945, -0.1051],\n",
      "        [ 0.1918,  0.9814],\n",
      "        [-0.9991, -0.0415]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.8230],\n",
      "        [ 5.5982],\n",
      "        [ 4.5833]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 347, Loss: 0.752749635009409\n",
      "tensor([[ 0.9938, -0.1111],\n",
      "        [ 0.1755,  0.9845],\n",
      "        [-0.9992, -0.0409]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.8400],\n",
      "        [ 5.6006],\n",
      "        [ 4.5889]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 348, Loss: 0.7769058523569471\n",
      "tensor([[ 0.9925, -0.1220],\n",
      "        [ 0.1643,  0.9864],\n",
      "        [-0.9992, -0.0406]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.8567],\n",
      "        [ 5.6027],\n",
      "        [ 4.5944]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 349, Loss: 0.7981470225516077\n",
      "tensor([[ 0.9900, -0.1408],\n",
      "        [ 0.1544,  0.9880],\n",
      "        [-0.9992, -0.0404]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.8732],\n",
      "        [ 5.6014],\n",
      "        [ 4.5999]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 350, Loss: 0.8066195038834278\n",
      "tensor([[ 0.9878, -0.1555],\n",
      "        [ 0.1552,  0.9879],\n",
      "        [-0.9992, -0.0399]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.8899],\n",
      "        [ 5.5957],\n",
      "        [ 4.6054]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 351, Loss: 0.8107585478640541\n",
      "tensor([[ 0.9874, -0.1585],\n",
      "        [ 0.1622,  0.9868],\n",
      "        [-0.9992, -0.0396]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.9069],\n",
      "        [ 5.5912],\n",
      "        [ 4.6109]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 352, Loss: 0.772510709786299\n",
      "tensor([[ 0.9878, -0.1558],\n",
      "        [ 0.1739,  0.9848],\n",
      "        [-0.9992, -0.0389]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.9243],\n",
      "        [ 5.5891],\n",
      "        [ 4.6165]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 353, Loss: 0.7644787341681942\n",
      "tensor([[ 0.9912, -0.1325],\n",
      "        [ 0.1794,  0.9838],\n",
      "        [-0.9993, -0.0385]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.9412],\n",
      "        [ 5.5893],\n",
      "        [ 4.6219]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 354, Loss: 0.7792843492939937\n",
      "tensor([[ 0.9938, -0.1114],\n",
      "        [ 0.1841,  0.9829],\n",
      "        [-0.9993, -0.0381]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.9591],\n",
      "        [ 5.5861],\n",
      "        [ 4.6272]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 355, Loss: 0.7826330999831734\n",
      "tensor([[ 0.9960, -0.0897],\n",
      "        [ 0.1888,  0.9820],\n",
      "        [-0.9993, -0.0373]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.9773],\n",
      "        [ 5.5832],\n",
      "        [ 4.6326]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 356, Loss: 0.7729057793286491\n",
      "tensor([[ 0.9959, -0.0905],\n",
      "        [ 0.1985,  0.9801],\n",
      "        [-0.9993, -0.0363]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.9942],\n",
      "        [ 5.5768],\n",
      "        [ 4.6380]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 357, Loss: 0.8155727425606571\n",
      "tensor([[ 0.9956, -0.0941],\n",
      "        [ 0.2115,  0.9774],\n",
      "        [-0.9994, -0.0355]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.0108],\n",
      "        [ 5.5684],\n",
      "        [ 4.6432]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 358, Loss: 0.8110737249389747\n",
      "tensor([[ 0.9941, -0.1083],\n",
      "        [ 0.2225,  0.9749],\n",
      "        [-0.9994, -0.0345]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.0283],\n",
      "        [ 5.5626],\n",
      "        [ 4.6485]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 359, Loss: 0.7665071987300789\n",
      "tensor([[ 0.9927, -0.1209],\n",
      "        [ 0.2262,  0.9741],\n",
      "        [-0.9994, -0.0341]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.0458],\n",
      "        [ 5.5582],\n",
      "        [ 4.6537]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 360, Loss: 0.7874579931417\n",
      "tensor([[ 0.9900, -0.1408],\n",
      "        [ 0.2281,  0.9736],\n",
      "        [-0.9994, -0.0333]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.0636],\n",
      "        [ 5.5579],\n",
      "        [ 4.6587]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 361, Loss: 0.7517681310280506\n",
      "tensor([[ 0.9871, -0.1599],\n",
      "        [ 0.2261,  0.9741],\n",
      "        [-0.9995, -0.0330]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.0816],\n",
      "        [ 5.5596],\n",
      "        [ 4.6639]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 362, Loss: 0.7553403838379206\n",
      "tensor([[ 0.9863, -0.1651],\n",
      "        [ 0.2244,  0.9745],\n",
      "        [-0.9995, -0.0330]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.1004],\n",
      "        [ 5.5611],\n",
      "        [ 4.6692]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 363, Loss: 0.7677456468909717\n",
      "tensor([[ 0.9857, -0.1683],\n",
      "        [ 0.2229,  0.9748],\n",
      "        [-0.9994, -0.0335]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.1186],\n",
      "        [ 5.5615],\n",
      "        [ 4.6745]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 364, Loss: 0.7962851021647923\n",
      "tensor([[ 0.9861, -0.1661],\n",
      "        [ 0.2220,  0.9750],\n",
      "        [-0.9994, -0.0339]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.1359],\n",
      "        [ 5.5634],\n",
      "        [ 4.6798]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 365, Loss: 0.7838184950850211\n",
      "tensor([[ 0.9880, -0.1543],\n",
      "        [ 0.2193,  0.9756],\n",
      "        [-0.9994, -0.0342]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.1520],\n",
      "        [ 5.5658],\n",
      "        [ 4.6852]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 366, Loss: 0.7875067085532622\n",
      "tensor([[ 0.9913, -0.1318],\n",
      "        [ 0.2151,  0.9766],\n",
      "        [-0.9994, -0.0349]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.1682],\n",
      "        [ 5.5705],\n",
      "        [ 4.6905]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 367, Loss: 0.7634142915009697\n",
      "tensor([[ 0.9950, -0.0998],\n",
      "        [ 0.2095,  0.9778],\n",
      "        [-0.9994, -0.0355]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.1843],\n",
      "        [ 5.5769],\n",
      "        [ 4.6959]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 368, Loss: 0.7676977041728161\n",
      "tensor([[ 0.9974, -0.0725],\n",
      "        [ 0.1982,  0.9802],\n",
      "        [-0.9993, -0.0361]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.2006],\n",
      "        [ 5.5859],\n",
      "        [ 4.7012]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 369, Loss: 0.7589007551198538\n",
      "tensor([[ 0.9986, -0.0538],\n",
      "        [ 0.1902,  0.9817],\n",
      "        [-0.9993, -0.0361]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.2174],\n",
      "        [ 5.5967],\n",
      "        [ 4.7066]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 370, Loss: 0.7664269204067915\n",
      "tensor([[ 0.9984, -0.0565],\n",
      "        [ 0.1861,  0.9825],\n",
      "        [-0.9994, -0.0359]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.2350],\n",
      "        [ 5.6083],\n",
      "        [ 4.7119]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 371, Loss: 0.7656125049073338\n",
      "tensor([[ 0.9960, -0.0890],\n",
      "        [ 0.1827,  0.9832],\n",
      "        [-0.9994, -0.0359]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.2526],\n",
      "        [ 5.6225],\n",
      "        [ 4.7173]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 372, Loss: 0.7488996124046078\n",
      "tensor([[ 0.9913, -0.1315],\n",
      "        [ 0.1830,  0.9831],\n",
      "        [-0.9993, -0.0364]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.2707],\n",
      "        [ 5.6313],\n",
      "        [ 4.7227]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 373, Loss: 0.777611435500851\n",
      "tensor([[ 0.9862, -0.1657],\n",
      "        [ 0.1777,  0.9841],\n",
      "        [-0.9993, -0.0373]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.2879],\n",
      "        [ 5.6429],\n",
      "        [ 4.7281]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 374, Loss: 0.7637136305485231\n",
      "tensor([[ 0.9838, -0.1795],\n",
      "        [ 0.1710,  0.9853],\n",
      "        [-0.9993, -0.0382]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.3035],\n",
      "        [ 5.6523],\n",
      "        [ 4.7335]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 375, Loss: 0.8053660319939782\n",
      "tensor([[ 0.9818, -0.1897],\n",
      "        [ 0.1727,  0.9850],\n",
      "        [-0.9992, -0.0391]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.3192],\n",
      "        [ 5.6593],\n",
      "        [ 4.7388]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 376, Loss: 0.7833506313541649\n",
      "tensor([[ 0.9808, -0.1950],\n",
      "        [ 0.1828,  0.9832],\n",
      "        [-0.9992, -0.0403]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.3345],\n",
      "        [ 5.6657],\n",
      "        [ 4.7441]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 377, Loss: 0.7732706661471951\n",
      "tensor([[ 0.9841, -0.1778],\n",
      "        [ 0.1962,  0.9806],\n",
      "        [-0.9992, -0.0409]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.3489],\n",
      "        [ 5.6723],\n",
      "        [ 4.7492]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 378, Loss: 0.8001086442759614\n",
      "tensor([[ 0.9901, -0.1405],\n",
      "        [ 0.2062,  0.9785],\n",
      "        [-0.9991, -0.0415]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.3629],\n",
      "        [ 5.6832],\n",
      "        [ 4.7544]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 379, Loss: 0.7528343375240013\n",
      "tensor([[ 0.9954, -0.0962],\n",
      "        [ 0.2143,  0.9768],\n",
      "        [-0.9991, -0.0417]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.3776],\n",
      "        [ 5.6909],\n",
      "        [ 4.7594]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 380, Loss: 0.7851745013783024\n",
      "tensor([[ 0.9981, -0.0611],\n",
      "        [ 0.2164,  0.9763],\n",
      "        [-0.9991, -0.0422]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.3929],\n",
      "        [ 5.6937],\n",
      "        [ 4.7644]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 381, Loss: 0.7880184881951524\n",
      "tensor([[ 0.9991, -0.0434],\n",
      "        [ 0.2133,  0.9770],\n",
      "        [-0.9991, -0.0426]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.4081],\n",
      "        [ 5.6953],\n",
      "        [ 4.7693]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 382, Loss: 0.795862896923921\n",
      "tensor([[ 0.9993, -0.0385],\n",
      "        [ 0.2068,  0.9784],\n",
      "        [-0.9991, -0.0432]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.4244],\n",
      "        [ 5.6977],\n",
      "        [ 4.7743]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 383, Loss: 0.7550246519715306\n",
      "tensor([[ 0.9980, -0.0630],\n",
      "        [ 0.2041,  0.9789],\n",
      "        [-0.9991, -0.0434]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.4410],\n",
      "        [ 5.7028],\n",
      "        [ 4.7792]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 384, Loss: 0.7647571505493183\n",
      "tensor([[ 0.9952, -0.0974],\n",
      "        [ 0.2114,  0.9774],\n",
      "        [-0.9991, -0.0434]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.4582],\n",
      "        [ 5.7033],\n",
      "        [ 4.7841]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 385, Loss: 0.7829921175841568\n",
      "tensor([[ 0.9914, -0.1311],\n",
      "        [ 0.2230,  0.9748],\n",
      "        [-0.9991, -0.0426]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.4769],\n",
      "        [ 5.7074],\n",
      "        [ 4.7891]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 386, Loss: 0.732636301546432\n",
      "tensor([[ 0.9860, -0.1667],\n",
      "        [ 0.2352,  0.9719],\n",
      "        [-0.9991, -0.0418]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.4959],\n",
      "        [ 5.7109],\n",
      "        [ 4.7941]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 387, Loss: 0.7621924121427202\n",
      "tensor([[ 0.9828, -0.1845],\n",
      "        [ 0.2455,  0.9694],\n",
      "        [-0.9992, -0.0409]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.5150],\n",
      "        [ 5.7168],\n",
      "        [ 4.7991]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 388, Loss: 0.7485186592534254\n",
      "tensor([[ 0.9837, -0.1797],\n",
      "        [ 0.2495,  0.9684],\n",
      "        [-0.9992, -0.0402]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.5331],\n",
      "        [ 5.7214],\n",
      "        [ 4.8040]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 389, Loss: 0.7938101278429335\n",
      "tensor([[ 0.9860, -0.1669],\n",
      "        [ 0.2497,  0.9683],\n",
      "        [-0.9992, -0.0395]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.5508],\n",
      "        [ 5.7266],\n",
      "        [ 4.8090]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 390, Loss: 0.7651504835468348\n",
      "tensor([[ 0.9898, -0.1427],\n",
      "        [ 0.2488,  0.9686],\n",
      "        [-0.9993, -0.0382]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.5684],\n",
      "        [ 5.7366],\n",
      "        [ 4.8139]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 391, Loss: 0.7460247984423377\n",
      "tensor([[ 0.9929, -0.1187],\n",
      "        [ 0.2426,  0.9701],\n",
      "        [-0.9993, -0.0371]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.5863],\n",
      "        [ 5.7478],\n",
      "        [ 4.8188]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 392, Loss: 0.7528191653941271\n",
      "tensor([[ 0.9938, -0.1110],\n",
      "        [ 0.2367,  0.9716],\n",
      "        [-0.9994, -0.0360]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.6040],\n",
      "        [ 5.7554],\n",
      "        [ 4.8236]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 393, Loss: 0.7937697850658846\n",
      "tensor([[ 0.9949, -0.1009],\n",
      "        [ 0.2304,  0.9731],\n",
      "        [-0.9994, -0.0352]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.6218],\n",
      "        [ 5.7631],\n",
      "        [ 4.8284]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 394, Loss: 0.7730691388261807\n",
      "tensor([[ 0.9963, -0.0864],\n",
      "        [ 0.2187,  0.9758],\n",
      "        [-0.9994, -0.0340]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.6402],\n",
      "        [ 5.7737],\n",
      "        [ 4.8333]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 395, Loss: 0.7470016240163888\n",
      "tensor([[ 0.9964, -0.0853],\n",
      "        [ 0.2111,  0.9775],\n",
      "        [-0.9994, -0.0332]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.6581],\n",
      "        [ 5.7758],\n",
      "        [ 4.8381]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 396, Loss: 0.8245810038279356\n",
      "tensor([[ 0.9948, -0.1017],\n",
      "        [ 0.2137,  0.9769],\n",
      "        [-0.9995, -0.0323]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.6753],\n",
      "        [ 5.7758],\n",
      "        [ 4.8431]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 397, Loss: 0.7808120332970757\n",
      "tensor([[ 0.9920, -0.1261],\n",
      "        [ 0.2183,  0.9759],\n",
      "        [-0.9995, -0.0322]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.6926],\n",
      "        [ 5.7742],\n",
      "        [ 4.8481]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 398, Loss: 0.7774258210843442\n",
      "tensor([[ 0.9885, -0.1511],\n",
      "        [ 0.2144,  0.9768],\n",
      "        [-0.9995, -0.0323]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.7086],\n",
      "        [ 5.7729],\n",
      "        [ 4.8530]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 399, Loss: 0.7960516499124562\n",
      "tensor([[ 0.9865, -0.1640],\n",
      "        [ 0.2115,  0.9774],\n",
      "        [-0.9995, -0.0320]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.7251],\n",
      "        [ 5.7699],\n",
      "        [ 4.8578]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 400, Loss: 0.7794266170065343\n",
      "tensor([[ 0.9846, -0.1746],\n",
      "        [ 0.2153,  0.9766],\n",
      "        [-0.9995, -0.0316]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.7414],\n",
      "        [ 5.7681],\n",
      "        [ 4.8626]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 401, Loss: 0.7785424882049338\n",
      "tensor([[ 0.9857, -0.1686],\n",
      "        [ 0.2137,  0.9769],\n",
      "        [-0.9995, -0.0317]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.7566],\n",
      "        [ 5.7671],\n",
      "        [ 4.8673]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 402, Loss: 0.7886546157196852\n",
      "tensor([[ 0.9866, -0.1634],\n",
      "        [ 0.2149,  0.9766],\n",
      "        [-0.9995, -0.0318]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.7720],\n",
      "        [ 5.7650],\n",
      "        [ 4.8719]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 403, Loss: 0.7720744198933882\n",
      "tensor([[ 0.9881, -0.1537],\n",
      "        [ 0.2208,  0.9753],\n",
      "        [-0.9995, -0.0323]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.7880],\n",
      "        [ 5.7635],\n",
      "        [ 4.8767]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 404, Loss: 0.7531775676769026\n",
      "tensor([[ 0.9918, -0.1279],\n",
      "        [ 0.2231,  0.9748],\n",
      "        [-0.9995, -0.0325]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.8034],\n",
      "        [ 5.7647],\n",
      "        [ 4.8815]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 405, Loss: 0.7684035910801175\n",
      "tensor([[ 0.9946, -0.1039],\n",
      "        [ 0.2260,  0.9741],\n",
      "        [-0.9995, -0.0325]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.8196],\n",
      "        [ 5.7658],\n",
      "        [ 4.8863]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 406, Loss: 0.7690172930645814\n",
      "tensor([[ 0.9963, -0.0865],\n",
      "        [ 0.2256,  0.9742],\n",
      "        [-0.9995, -0.0326]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.8364],\n",
      "        [ 5.7668],\n",
      "        [ 4.8910]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 407, Loss: 0.7790752762897706\n",
      "tensor([[ 0.9960, -0.0897],\n",
      "        [ 0.2253,  0.9743],\n",
      "        [-0.9995, -0.0328]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.8533],\n",
      "        [ 5.7654],\n",
      "        [ 4.8957]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 408, Loss: 0.7761499963056284\n",
      "tensor([[ 0.9946, -0.1036],\n",
      "        [ 0.2222,  0.9750],\n",
      "        [-0.9995, -0.0329]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.8702],\n",
      "        [ 5.7676],\n",
      "        [ 4.9004]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 409, Loss: 0.7545247768632892\n",
      "tensor([[ 0.9920, -0.1266],\n",
      "        [ 0.2237,  0.9747],\n",
      "        [-0.9994, -0.0332]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.8861],\n",
      "        [ 5.7622],\n",
      "        [ 4.9052]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 410, Loss: 0.8316481207232671\n",
      "tensor([[ 0.9887, -0.1497],\n",
      "        [ 0.2251,  0.9743],\n",
      "        [-0.9994, -0.0336]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.9020],\n",
      "        [ 5.7522],\n",
      "        [ 4.9098]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 411, Loss: 0.8173105035965392\n",
      "tensor([[ 0.9858, -0.1677],\n",
      "        [ 0.2229,  0.9748],\n",
      "        [-0.9994, -0.0340]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.9179],\n",
      "        [ 5.7398],\n",
      "        [ 4.9145]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 412, Loss: 0.7864786491395579\n",
      "tensor([[ 0.9833, -0.1819],\n",
      "        [ 0.2231,  0.9748],\n",
      "        [-0.9994, -0.0341]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.9332],\n",
      "        [ 5.7262],\n",
      "        [ 4.9191]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 413, Loss: 0.7969972423025302\n",
      "tensor([[ 0.9811, -0.1933],\n",
      "        [ 0.2247,  0.9744],\n",
      "        [-0.9994, -0.0340]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.9490],\n",
      "        [ 5.7127],\n",
      "        [ 4.9239]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 414, Loss: 0.7607881117853954\n",
      "tensor([[ 0.9828, -0.1849],\n",
      "        [ 0.2270,  0.9739],\n",
      "        [-0.9994, -0.0342]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.9639],\n",
      "        [ 5.6963],\n",
      "        [ 4.9285]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 415, Loss: 0.808371371602328\n",
      "tensor([[ 0.9873, -0.1589],\n",
      "        [ 0.2223,  0.9750],\n",
      "        [-0.9994, -0.0342]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.9790],\n",
      "        [ 5.6805],\n",
      "        [ 4.9331]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 416, Loss: 0.7793738303797987\n",
      "tensor([[ 0.9916, -0.1295],\n",
      "        [ 0.2143,  0.9768],\n",
      "        [-0.9994, -0.0339]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.9945],\n",
      "        [ 5.6691],\n",
      "        [ 4.9375]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 417, Loss: 0.7645487919740781\n",
      "tensor([[ 0.9945, -0.1049],\n",
      "        [ 0.2058,  0.9786],\n",
      "        [-0.9994, -0.0336]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.0095],\n",
      "        [ 5.6570],\n",
      "        [ 4.9420]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 418, Loss: 0.7974496281964535\n",
      "tensor([[ 0.9959, -0.0909],\n",
      "        [ 0.2009,  0.9796],\n",
      "        [-0.9994, -0.0332]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.0254],\n",
      "        [ 5.6485],\n",
      "        [ 4.9465]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 419, Loss: 0.7433036543210917\n",
      "tensor([[ 0.9954, -0.0961],\n",
      "        [ 0.2051,  0.9787],\n",
      "        [-0.9995, -0.0327]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.0406],\n",
      "        [ 5.6368],\n",
      "        [ 4.9509]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 420, Loss: 0.8106770275577799\n",
      "tensor([[ 0.9934, -0.1144],\n",
      "        [ 0.2103,  0.9776],\n",
      "        [-0.9995, -0.0314]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.0561],\n",
      "        [ 5.6297],\n",
      "        [ 4.9554]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 421, Loss: 0.7458231916962641\n",
      "tensor([[ 0.9907, -0.1359],\n",
      "        [ 0.2131,  0.9770],\n",
      "        [-0.9995, -0.0300]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.0723],\n",
      "        [ 5.6236],\n",
      "        [ 4.9598]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 422, Loss: 0.7648276077838951\n",
      "tensor([[ 0.9871, -0.1599],\n",
      "        [ 0.2167,  0.9762],\n",
      "        [-0.9996, -0.0290]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.0880],\n",
      "        [ 5.6117],\n",
      "        [ 4.9642]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 423, Loss: 0.8102601808950286\n",
      "tensor([[ 0.9855, -0.1695],\n",
      "        [ 0.2185,  0.9758],\n",
      "        [-0.9996, -0.0279]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.1044],\n",
      "        [ 5.6022],\n",
      "        [ 4.9687]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 424, Loss: 0.7502603110138236\n",
      "tensor([[ 0.9872, -0.1594],\n",
      "        [ 0.2203,  0.9754],\n",
      "        [-0.9996, -0.0266]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.1207],\n",
      "        [ 5.5982],\n",
      "        [ 4.9732]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 425, Loss: 0.7626788411280542\n",
      "tensor([[ 0.9904, -0.1381],\n",
      "        [ 0.2205,  0.9754],\n",
      "        [-0.9997, -0.0252]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.1364],\n",
      "        [ 5.5946],\n",
      "        [ 4.9777]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 426, Loss: 0.7848014320211626\n",
      "tensor([[ 0.9930, -0.1184],\n",
      "        [ 0.2209,  0.9753],\n",
      "        [-0.9997, -0.0242]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.1526],\n",
      "        [ 5.5927],\n",
      "        [ 4.9822]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 427, Loss: 0.7522278106299036\n",
      "tensor([[ 0.9950, -0.0995],\n",
      "        [ 0.2157,  0.9765],\n",
      "        [-0.9997, -0.0227]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.1683],\n",
      "        [ 5.5960],\n",
      "        [ 4.9868]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 428, Loss: 0.7486605415741802\n",
      "tensor([[ 0.9960, -0.0898],\n",
      "        [ 0.2101,  0.9777],\n",
      "        [-0.9998, -0.0211]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.1837],\n",
      "        [ 5.5996],\n",
      "        [ 4.9913]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 429, Loss: 0.7753802307289848\n",
      "tensor([[ 0.9950, -0.1000],\n",
      "        [ 0.2034,  0.9791],\n",
      "        [-0.9998, -0.0203]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.1988],\n",
      "        [ 5.6042],\n",
      "        [ 4.9959]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 430, Loss: 0.7641286505873012\n",
      "tensor([[ 0.9934, -0.1151],\n",
      "        [ 0.1983,  0.9802],\n",
      "        [-0.9998, -0.0198]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.2144],\n",
      "        [ 5.6111],\n",
      "        [ 5.0004]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 431, Loss: 0.7608518376005368\n",
      "tensor([[ 0.9913, -0.1315],\n",
      "        [ 0.1950,  0.9808],\n",
      "        [-0.9998, -0.0197]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.2305],\n",
      "        [ 5.6207],\n",
      "        [ 5.0051]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 432, Loss: 0.7471784283324288\n",
      "tensor([[ 0.9888, -0.1489],\n",
      "        [ 0.2060,  0.9786],\n",
      "        [-0.9998, -0.0202]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.2460],\n",
      "        [ 5.6207],\n",
      "        [ 5.0099]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 433, Loss: 0.8066310766911277\n",
      "tensor([[ 0.9880, -0.1547],\n",
      "        [ 0.2185,  0.9758],\n",
      "        [-0.9998, -0.0206]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.2618],\n",
      "        [ 5.6210],\n",
      "        [ 5.0145]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 434, Loss: 0.7758532671945496\n",
      "tensor([[ 0.9881, -0.1537],\n",
      "        [ 0.2268,  0.9739],\n",
      "        [-0.9998, -0.0213]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.2774],\n",
      "        [ 5.6240],\n",
      "        [ 5.0192]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 435, Loss: 0.7704067821750944\n",
      "tensor([[ 0.9887, -0.1496],\n",
      "        [ 0.2369,  0.9715],\n",
      "        [-0.9998, -0.0216]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.2927],\n",
      "        [ 5.6316],\n",
      "        [ 5.0238]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 436, Loss: 0.7545830374880883\n",
      "tensor([[ 0.9912, -0.1320],\n",
      "        [ 0.2371,  0.9715],\n",
      "        [-0.9998, -0.0220]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.3093],\n",
      "        [ 5.6417],\n",
      "        [ 5.0283]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 437, Loss: 0.749243686595468\n",
      "tensor([[ 0.9927, -0.1208],\n",
      "        [ 0.2363,  0.9717],\n",
      "        [-0.9997, -0.0228]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.3255],\n",
      "        [ 5.6513],\n",
      "        [ 5.0328]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 438, Loss: 0.7727700547913968\n",
      "tensor([[ 0.9928, -0.1199],\n",
      "        [ 0.2272,  0.9739],\n",
      "        [-0.9997, -0.0234]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.3414],\n",
      "        [ 5.6588],\n",
      "        [ 5.0371]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 439, Loss: 0.7907829008651239\n",
      "tensor([[ 0.9918, -0.1278],\n",
      "        [ 0.2133,  0.9770],\n",
      "        [-0.9997, -0.0242]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.3574],\n",
      "        [ 5.6661],\n",
      "        [ 5.0416]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 440, Loss: 0.7598256946053867\n",
      "tensor([[ 0.9915, -0.1305],\n",
      "        [ 0.2016,  0.9795],\n",
      "        [-0.9997, -0.0255]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.3727],\n",
      "        [ 5.6704],\n",
      "        [ 5.0460]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 441, Loss: 0.797645204182605\n",
      "tensor([[ 0.9892, -0.1463],\n",
      "        [ 0.1957,  0.9807],\n",
      "        [-0.9997, -0.0262]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.3877],\n",
      "        [ 5.6725],\n",
      "        [ 5.0503]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 442, Loss: 0.7881754129880597\n",
      "tensor([[ 0.9892, -0.1466],\n",
      "        [ 0.1926,  0.9813],\n",
      "        [-0.9996, -0.0267]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.4025],\n",
      "        [ 5.6758],\n",
      "        [ 5.0546]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 443, Loss: 0.7777305489533854\n",
      "tensor([[ 0.9891, -0.1475],\n",
      "        [ 0.1967,  0.9805],\n",
      "        [-0.9996, -0.0279]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.4178],\n",
      "        [ 5.6762],\n",
      "        [ 5.0589]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 444, Loss: 0.7649557592078124\n",
      "tensor([[ 0.9875, -0.1575],\n",
      "        [ 0.2083,  0.9781],\n",
      "        [-0.9996, -0.0294]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.4337],\n",
      "        [ 5.6701],\n",
      "        [ 5.0632]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 445, Loss: 0.7902217088691662\n",
      "tensor([[ 0.9861, -0.1664],\n",
      "        [ 0.2218,  0.9751],\n",
      "        [-0.9995, -0.0306]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.4501],\n",
      "        [ 5.6621],\n",
      "        [ 5.0673]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 446, Loss: 0.7793888752710462\n",
      "tensor([[ 0.9853, -0.1711],\n",
      "        [ 0.2419,  0.9703],\n",
      "        [-0.9995, -0.0315]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.4667],\n",
      "        [ 5.6557],\n",
      "        [ 5.0714]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 447, Loss: 0.7665951113230411\n",
      "tensor([[ 0.9874, -0.1581],\n",
      "        [ 0.2535,  0.9673],\n",
      "        [-0.9995, -0.0324]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.4834],\n",
      "        [ 5.6474],\n",
      "        [ 5.0756]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 448, Loss: 0.7836863515364402\n",
      "tensor([[ 0.9901, -0.1400],\n",
      "        [ 0.2553,  0.9669],\n",
      "        [-0.9994, -0.0332]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.5001],\n",
      "        [ 5.6325],\n",
      "        [ 5.0795]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 449, Loss: 0.8050283601700462\n"
     ]
    }
   ],
   "source": [
    "import geotorch\n",
    "\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "\n",
    "class GalleryParams(torch.nn.Module):\n",
    "    def __init__(self, init_mean, init_kappa):\n",
    "        super(GalleryParams, self).__init__()\n",
    "        self.gallery_means = torch.nn.Parameter(torch.tensor(init_mean))\n",
    "        # self.gallery_kappas =  torch.nn.Parameter(torch.tensor(init_kappa))\n",
    "        # self.gallery_means = torch.nn.Parameter(torch.rand(3, 2, dtype=torch.float64))\n",
    "        self.gallery_kappas = torch.nn.Parameter(\n",
    "            torch.rand(3, 1, dtype=torch.float64) * 10\n",
    "        )\n",
    "\n",
    "\n",
    "gallery_params = GalleryParams(init_mean, init_kappa)\n",
    "target_class = torch.tensor(gallery_subject_ids_sorted)\n",
    "T = torch.nn.Parameter(torch.tensor(1.0))\n",
    "geotorch.sphere(gallery_params, \"gallery_means\")\n",
    "\n",
    "\n",
    "# train\n",
    "M = 10\n",
    "mc_prob = MonteCarloPredictiveProb(M=M)\n",
    "\n",
    "# optimizer = torch.optim.Adam([gallery_means, gallery_kappas], lr=10e-3)\n",
    "optimizer = torch.optim.Adam(gallery_params.parameters(), lr=0.1)\n",
    "\n",
    "num_steps = 450\n",
    "\n",
    "for iter in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    # compute nll loss\n",
    "    log_probs = mc_prob(\n",
    "        gallery_features,\n",
    "        gallery_unc,\n",
    "        gallery_params.gallery_means,\n",
    "        gallery_params.gallery_kappas,\n",
    "        T,\n",
    "    )[:, :, :-1]\n",
    "    probs = torch.exp(log_probs)\n",
    "    mean_probs = torch.mean(probs, axis=1)\n",
    "    log_probs_new = torch.log(mean_probs)\n",
    "    loss = nll_loss(log_probs_new, target_class)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(gallery_params.gallery_means)\n",
    "    print(torch.norm(gallery_params.gallery_means, dim=-1))\n",
    "    print(gallery_params.gallery_kappas)\n",
    "    print(f\"Iteration {iter}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fontsize = 20\n",
    "linewidth = 3\n",
    "dot_size = 80\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "draw_circle(ax, linewidth)\n",
    "for class_id, (class_mean, kappa) in enumerate(\n",
    "    zip(gallery_params.gallery_means, gallery_params.gallery_kappas)\n",
    "):\n",
    "    color = colors[class_id]\n",
    "    class_mean = class_mean.detach().numpy()\n",
    "    kappa = kappa.detach().numpy()\n",
    "    class_point_angle = np.angle([class_mean[0] + 1j * class_mean[1]])[0]\n",
    "    draw_dencity(\n",
    "        class_point_angle,\n",
    "        kappa,\n",
    "        ax,\n",
    "        linewidth=3,\n",
    "        color=color,\n",
    "        range=np.pi / 2,\n",
    "        draw_center=True,\n",
    "        dot_size=dot_size,\n",
    "        type=\"power\",\n",
    "    )\n",
    "    for position in np.where(gallery_subject_ids_sorted == class_id)[0]:\n",
    "        point_angle = np.angle(\n",
    "            [gallery_features[position][0] + 1j * gallery_features[position][1]]\n",
    "        )[0]\n",
    "        draw_dencity(\n",
    "            point_angle,\n",
    "            gallery_unc[position],\n",
    "            ax,\n",
    "            linewidth=1,\n",
    "            color=color,\n",
    "            range=np.pi / 2,\n",
    "            scale=0.1,\n",
    "            draw_center=True,\n",
    "            dot_size=20,\n",
    "        )\n",
    "fig.gca().set_aspect(\"equal\")\n",
    "fig.show()\n",
    "plt.savefig(\"/app/outputs/images/trained.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Probability estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_angles = np.array([np.pi / 3])\n",
    "test_vectors = get_vectors_by_angle(test_angles)\n",
    "test_kappa = np.array([[10]])\n",
    "M = 100\n",
    "mc_prob = MonteCarloPredictiveProb(M=M)\n",
    "\n",
    "log_probs = mc_prob(\n",
    "    test_vectors,\n",
    "    test_kappa,\n",
    "    gallery_params.gallery_means,\n",
    "    gallery_params.gallery_kappas,\n",
    "    T,\n",
    ")  # [:, :, :-1]\n",
    "probs = torch.exp(log_probs)\n",
    "mean_probs = torch.mean(probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0491, 0.4923, 0.0034, 0.4552]], dtype=torch.float64,\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fontsize = 20\n",
    "linewidth = 3\n",
    "dot_size = 80\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "draw_circle(ax, linewidth)\n",
    "draw_dencity(\n",
    "    test_angles[0],\n",
    "    test_kappa[0],\n",
    "    ax,\n",
    "    linewidth=1,\n",
    "    color=\"tab:cyan\",\n",
    "    range=np.pi / 2,\n",
    "    scale=1,\n",
    "    draw_center=True,\n",
    "    dot_size=20,\n",
    ")\n",
    "for class_id, (class_mean, kappa) in enumerate(\n",
    "    zip(gallery_params.gallery_means, gallery_params.gallery_kappas)\n",
    "):\n",
    "    color = colors[class_id]\n",
    "    class_mean = class_mean.detach().numpy()\n",
    "    kappa = kappa.detach().numpy()\n",
    "    class_point_angle = np.angle([class_mean[0] + 1j * class_mean[1]])[0]\n",
    "    draw_dencity(\n",
    "        class_point_angle,\n",
    "        kappa,\n",
    "        ax,\n",
    "        linewidth=3,\n",
    "        color=color,\n",
    "        range=np.pi / 2,\n",
    "        draw_center=True,\n",
    "        dot_size=dot_size,\n",
    "        type=\"power\",\n",
    "    )\n",
    "fig.gca().set_aspect(\"equal\")\n",
    "fig.show()\n",
    "plt.savefig(\"/app/outputs/images/test.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tab:blue', 'tab:orange', 'tab:green']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
