{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "        p(\\textbf{z}|c) &= \\mathcal{C}_d(\\kappa)\\exp\\left(\\kappa\\mu^T_{c}\\textbf{z}\\right)\\\\\n",
    "        \\mathcal{C}_d(\\kappa) &= \\frac{(\\kappa)^{d/2-1}}{(2\\pi)^{d/2}\\mathcal{I}_{d/2-1}(\\kappa)}\n",
    "\\end{align}\n",
    "\n",
    "With d=2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import iv, gamma\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# mpl.style.use('classic')\n",
    "\n",
    "\n",
    "def z_Prob(z, mus, kappa, d=2, beta=0.5):\n",
    "    K = mus.shape[-1]\n",
    "    p_c = (1 - beta) / K\n",
    "    class_probs = np.array([z_vonMises_dencity(z, mu, kappa) for mu in mus.T])\n",
    "    return np.sum(class_probs * p_c) + (1 / (2 * np.pi)) * beta\n",
    "\n",
    "\n",
    "def z_class_prob(class_id, z, mus, kappa, d=2, beta=0.5):\n",
    "    p_z = z_Prob(z, mus, kappa, d, beta)\n",
    "    K = mus.shape[1]\n",
    "    if class_id == K:\n",
    "        p_c = beta\n",
    "        return (1 / (2 * np.pi)) * p_c / p_z\n",
    "    else:\n",
    "        p_c = (1 - beta) / K\n",
    "        return (z_vonMises_dencity(z, mus[:, class_id], kappa) * p_c) / p_z\n",
    "\n",
    "\n",
    "def z_vonMises_dencity(z, mu_c, kappa, d=2):\n",
    "    C_d = kappa ** (d / 2 - 1) / ((2 * np.pi) ** (d / 2) * iv(d / 2 - 1, kappa))\n",
    "    return C_d * np.exp(kappa * np.dot(z, mu_c))\n",
    "\n",
    "\n",
    "def z_power_dencity(z, mu_c, kappa, d=2):\n",
    "    alpha = (d - 1) / 2 + kappa\n",
    "    beta = (d - 1) / 2\n",
    "    M_d = gamma(alpha + beta) / (2 ** (alpha + beta) * np.pi**beta * gamma(alpha))\n",
    "    return M_d * (1 + np.dot(z, mu_c)) ** kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_by_angle(angles):\n",
    "    return np.array([[np.cos(plot_angle), np.sin(plot_angle)] for plot_angle in angles])\n",
    "\n",
    "\n",
    "def compute_class_probs(class_id, zs, mus, kappa, beta):\n",
    "    class_probes = []\n",
    "    for z in zs:\n",
    "        class_prob = z_class_prob(class_id, z, mus, kappa, beta=beta)\n",
    "        class_probes.append(class_prob)\n",
    "    class_probes = np.array(class_probes)\n",
    "    return class_probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_circle(ax, linewidth, zorder=4):\n",
    "    # plot circle\n",
    "    theta = np.linspace(0, 2 * np.pi, 150)\n",
    "    a = np.cos(theta)\n",
    "    b = np.sin(theta)\n",
    "    circle = plt.Circle((0, 0), 1, color=\"blue\", zorder=4, alpha=0.1)\n",
    "    ax.add_patch(circle)\n",
    "    ax.plot(a, b, color=\"tab:gray\", zorder=4, linewidth=linewidth)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "\n",
    "def draw_decity(ax):\n",
    "    pass\n",
    "\n",
    "\n",
    "def draw_example(kappa, gallery_class_angles, text_shift, save_name, beta=0.5):\n",
    "    fontsize = 20\n",
    "    linewidth = 3\n",
    "    dot_size = 80\n",
    "\n",
    "    test_color = \"tab:cyan\"\n",
    "    # gallery_class_angles = [0.4, 0.25]\n",
    "    ident_uncertain_test_point = (\n",
    "        gallery_class_angles[-1] + gallery_class_angles[-2]\n",
    "    ) / 2 - 0.02\n",
    "    test_points_angles = [\n",
    "        ident_uncertain_test_point,\n",
    "        gallery_class_angles[-1]\n",
    "        - (ident_uncertain_test_point - gallery_class_angles[-1]),\n",
    "    ]\n",
    "\n",
    "    gallery_class_angles = np.array(gallery_class_angles) * 2 * np.pi\n",
    "    test_points_angles = np.array(test_points_angles) * 2 * np.pi\n",
    "    theta = np.linspace(0, 2 * np.pi, 150)\n",
    "\n",
    "    colors = list(mcolors.TABLEAU_COLORS)[: len(gallery_class_angles)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    draw_circle(ax, linewidth)\n",
    "\n",
    "    draw_dencity_angles = np.linspace(-np.pi / 3, np.pi / 3, 150)\n",
    "    mus = np.stack([np.cos(gallery_class_angles), np.sin(gallery_class_angles)], axis=0)\n",
    "    class_to_class_probs = []\n",
    "\n",
    "    for i, (angle, color) in enumerate(zip(gallery_class_angles, colors)):\n",
    "        mu_c = mus[:, i]\n",
    "        plot_angles = angle + draw_dencity_angles\n",
    "        zs = get_vectors_by_angle(plot_angles)\n",
    "        class_probes = compute_class_probs(i, zs, mus, kappa, beta)\n",
    "        v = zs.T * (1 + class_probes[np.newaxis, :])\n",
    "        ax.plot(v[0], v[1], c=color, linewidth=linewidth)\n",
    "\n",
    "        ax.scatter([np.cos(angle)], [np.sin(angle)], c=color, s=dot_size, zorder=5)\n",
    "        ax.scatter([0], [0], color=\"black\", s=20)\n",
    "\n",
    "        # plot decity\n",
    "        # ax.scatter(points[:, 0], points[:, 1], color=color, s=3)\n",
    "\n",
    "    # plot_uniform_prob\n",
    "    zs = get_vectors_by_angle(theta)\n",
    "    class_probes = compute_class_probs(mus.shape[1], zs, mus, kappa, beta)\n",
    "    v = zs.T * (1 + class_probes[np.newaxis, :])\n",
    "    # ax.plot(v[0], v[1], color='black')\n",
    "\n",
    "    # plot unc\n",
    "    all_probs = []\n",
    "    for i in range(mus.shape[1] + 1):\n",
    "        class_probes = compute_class_probs(i, zs, mus, kappa, beta)\n",
    "        all_probs.append(class_probes)\n",
    "    all_probs = np.stack(all_probs, axis=0)\n",
    "    unc = -np.sum(all_probs * np.log(all_probs), axis=0)\n",
    "    # unc = -np.max(all_probs, axis=0) + 1\n",
    "    v = zs.T * (1 + unc[np.newaxis, :])\n",
    "    ax.plot(v[0], v[1], color=\"tab:red\", linewidth=linewidth)\n",
    "\n",
    "    # plot test points\n",
    "    for test_angle in test_points_angles:\n",
    "        ax.scatter(\n",
    "            [np.cos(test_angle)],\n",
    "            [np.sin(test_angle)],\n",
    "            c=test_color,\n",
    "            s=dot_size,\n",
    "            zorder=5,\n",
    "        )\n",
    "\n",
    "    test_point_vectors = get_vectors_by_angle(test_points_angles)\n",
    "    # entropy value\n",
    "\n",
    "    probs_at_test_points = []\n",
    "    for i in range(mus.shape[1] + 1):\n",
    "        class_probes = compute_class_probs(i, test_point_vectors, mus, kappa, beta)\n",
    "        probs_at_test_points.append(class_probes)\n",
    "    probs_at_test_points = np.stack(probs_at_test_points, axis=0)\n",
    "\n",
    "    unc_test = -np.sum(probs_at_test_points * np.log(probs_at_test_points), axis=0)\n",
    "    unc_test = np.round(unc_test, 2)\n",
    "    # unc_test = -np.max(probs_at_test_points, axis=0) + 1\n",
    "    # unc_test = np.round(unc_test, 2)\n",
    "    ax.annotate(\n",
    "        f\"${unc_test[0]}$\",\n",
    "        xy=test_point_vectors[0],\n",
    "        xytext=[\n",
    "            test_point_vectors[0][0] + text_shift[0][0],\n",
    "            test_point_vectors[0][1] + text_shift[0][1],\n",
    "        ],\n",
    "        fontsize=fontsize,\n",
    "    )\n",
    "    ax.annotate(\n",
    "        f\"${unc_test[1]}$\",\n",
    "        xy=test_point_vectors[1],\n",
    "        xytext=[\n",
    "            test_point_vectors[1][0] + text_shift[1][0],\n",
    "            test_point_vectors[1][1] + text_shift[1][1],\n",
    "        ],\n",
    "        fontsize=fontsize,\n",
    "    )\n",
    "    fig.gca().set_aspect(\"equal\")\n",
    "    plt.savefig(save_name, dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_example(\n",
    "#     kappa=15,\n",
    "#     gallery_class_angles=[0.4, 0.25],\n",
    "#     text_shift=[[-0.2, -0.3], [-0.2, 0.1]],\n",
    "#     save_name=\"test.png\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Identification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_example(kappa = 15, gallery_class_angles = [0.4, 0.25], text_shift = [[-0.2, -0.3], [-0.2, 0.1]], save_name='/app/paper_assets/images/false_ident_example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False accept/reject example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_example(kappa = 13, gallery_class_angles = [0.48, 0.25], text_shift = [[-0.1, -0.3], [-0.35, 0.2]], save_name='/app/paper_assets/images/false_accept-reject_example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_dencity(\n",
    "    angle,\n",
    "    kappa,\n",
    "    ax,\n",
    "    linewidth,\n",
    "    color,\n",
    "    range=np.pi / 3,\n",
    "    scale=1,\n",
    "    draw_center=False,\n",
    "    dot_size=40,\n",
    "    type=\"vMF\",\n",
    "):\n",
    "    assert type in [\"vMF\", \"power\"]\n",
    "    draw_dencity_angles = np.linspace(-range, range, 150)\n",
    "    plot_angles = angle + draw_dencity_angles\n",
    "    zs = get_vectors_by_angle(plot_angles)\n",
    "    mu = get_vectors_by_angle([angle])[0]\n",
    "    if type == \"vMF\":\n",
    "        dencities = z_vonMises_dencity(zs, mu, kappa)\n",
    "    else:\n",
    "        dencities = z_power_dencity(zs, mu, kappa)\n",
    "    v = zs.T * (1 + dencities * scale)\n",
    "    ax.plot(v[0], v[1], c=color, linewidth=linewidth)\n",
    "    if draw_center:\n",
    "        ax.scatter([np.cos(angle)], [np.sin(angle)], c=color, s=dot_size, zorder=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.samplers import random_VMF, VonMisesFisher\n",
    "\n",
    "gallery_features = []  # N gallery samples X 512\n",
    "gallery_unc_log = []  # N gallery samples X 1\n",
    "gallery_subject_ids_sorted = []  # N gallery samples\n",
    "\n",
    "\n",
    "class_center_angles = np.array([0, np.pi / 2, np.pi])\n",
    "class_z_kappa = np.array([9, 6, 5])\n",
    "colors = list(mcolors.TABLEAU_COLORS)[: len(class_center_angles)]\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(2)\n",
    "\n",
    "for class_id, (angle, kappa) in enumerate(zip(class_center_angles, class_z_kappa)):\n",
    "    num_samples = rng.integers(5, 8)\n",
    "    gallery_subject_ids_sorted.extend([class_id] * num_samples)\n",
    "    gallery_unc_log.extend(4 * rng.random(num_samples) + 2)\n",
    "    samples = random_VMF(\n",
    "        get_vectors_by_angle([angle])[0], kappa=kappa, size=num_samples\n",
    "    )\n",
    "    gallery_features.extend(samples)\n",
    "\n",
    "gallery_features = np.array(gallery_features)\n",
    "gallery_unc_log = np.array(gallery_unc_log).reshape(-1, 1)\n",
    "gallery_subject_ids_sorted = np.array(gallery_subject_ids_sorted)\n",
    "\n",
    "gallery_unc = np.exp(gallery_unc_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19, 2), (19,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gallery_features.shape, gallery_subject_ids_sorted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = VonMisesFisher(3)\n",
    "feature_mean = sampler(np.array([[0, 1], [1, 0]]), np.array([[10], [15]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8937/1737226771.py:34: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/app/outputs/images/generation.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m fig\u001b[38;5;241m.\u001b[39mgca()\u001b[38;5;241m.\u001b[39mset_aspect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m fig\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 35\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/app/outputs/images/generation.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_inches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/matplotlib/pyplot.py:859\u001b[0m, in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Figure\u001b[38;5;241m.\u001b[39msavefig)\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msavefig\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    858\u001b[0m     fig \u001b[38;5;241m=\u001b[39m gcf()\n\u001b[0;32m--> 859\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m     fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mdraw_idle()   \u001b[38;5;66;03m# need this if 'transparent=True' to reset colors\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/matplotlib/figure.py:2311\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2308\u001b[0m         patch\u001b[38;5;241m.\u001b[39mset_facecolor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2309\u001b[0m         patch\u001b[38;5;241m.\u001b[39mset_edgecolor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2311\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transparent:\n\u001b[1;32m   2314\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax, cc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes, original_axes_colors):\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/matplotlib/backend_bases.py:2210\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     _bbox_inches_restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2210\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2214\u001b[0m \u001b[43m        \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2215\u001b[0m \u001b[43m        \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/matplotlib/backend_bases.py:1639\u001b[0m, in \u001b[0;36m_check_savefig_extra_args.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1631\u001b[0m     cbook\u001b[38;5;241m.\u001b[39mwarn_deprecated(\n\u001b[1;32m   1632\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.3\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   1633\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m() got unexpected keyword argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1634\u001b[0m                 \u001b[38;5;241m+\u001b[39m arg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m which is no longer supported as of \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1635\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m and will become an error \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1636\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1637\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(arg)\n\u001b[0;32m-> 1639\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:510\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03mWrite the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;124;03m    *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    509\u001b[0m FigureCanvasAgg\u001b[38;5;241m.\u001b[39mdraw(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 510\u001b[0m \u001b[43mmpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimsave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_rgba\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/matplotlib/image.py:1611\u001b[0m, in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1610\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (dpi, dpi))\n\u001b[0;32m-> 1611\u001b[0m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/PIL/Image.py:2410\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2408\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2409\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2410\u001b[0m         fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw+b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2413\u001b[0m     save_handler(\u001b[38;5;28mself\u001b[39m, fp, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/app/outputs/images/generation.png'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fontsize = 20\n",
    "linewidth = 3\n",
    "dot_size = 80\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "draw_circle(ax, linewidth)\n",
    "for class_id, (angle, kappa) in enumerate(zip(class_center_angles, class_z_kappa)):\n",
    "    color = colors[class_id]\n",
    "    # draw_dencity(\n",
    "    #     angle,\n",
    "    #     kappa,\n",
    "    #     ax,\n",
    "    #     linewidth=3,\n",
    "    #     color=color,\n",
    "    #     range=np.pi / 2,\n",
    "    #     draw_center=True,\n",
    "    #     dot_size=dot_size,\n",
    "    # )\n",
    "    for position in np.where(gallery_subject_ids_sorted == class_id)[0]:\n",
    "        point_angle = np.angle(\n",
    "            [gallery_features[position][0] + 1j * gallery_features[position][1]]\n",
    "        )[0]\n",
    "        draw_dencity(\n",
    "            point_angle,\n",
    "            gallery_unc[position],\n",
    "            ax,\n",
    "            linewidth=1,\n",
    "            color=color,\n",
    "            range=np.pi / 2,\n",
    "            scale=0.1,\n",
    "            draw_center=True,\n",
    "            dot_size=20,\n",
    "        )\n",
    "fig.gca().set_aspect(\"equal\")\n",
    "fig.show()\n",
    "plt.savefig(\"/app/outputs/images/generation.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "gallery_features.shape\n",
    "init_mean = np.array(\n",
    "    [\n",
    "        np.mean(gallery_features[gallery_subject_ids_sorted == c], axis=0)\n",
    "        for c in range(3)\n",
    "    ]\n",
    ")\n",
    "# init_mean = init_mean / np.linalg.norm(init_mean, axis=1, keepdims=True)\n",
    "\n",
    "init_kappa = np.array(\n",
    "    [np.mean(gallery_unc[gallery_subject_ids_sorted == c], axis=0) for c in range(3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19, 2), (19, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gallery_features.shape, gallery_unc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.open_set_methods.class_prob_models import MonteCarloPredictiveProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4201, -0.9075],\n",
      "        [ 0.9729,  0.2314],\n",
      "        [ 0.9161,  0.4009]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.3080],\n",
      "        [4.2513],\n",
      "        [3.9996]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 0, Loss: 5.511763690956834\n",
      "tensor([[ 0.5295, -0.8483],\n",
      "        [ 0.9414,  0.3373],\n",
      "        [ 0.8607,  0.5091]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.2158],\n",
      "        [4.1533],\n",
      "        [3.8996]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 1, Loss: 5.319564462862632\n",
      "tensor([[ 0.6284, -0.7779],\n",
      "        [ 0.8987,  0.4387],\n",
      "        [ 0.7926,  0.6098]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.1451],\n",
      "        [4.0558],\n",
      "        [3.8004]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 2, Loss: 4.6186182262736075\n",
      "tensor([[ 0.7152, -0.6989],\n",
      "        [ 0.8456,  0.5339],\n",
      "        [ 0.7175,  0.6966]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.1134],\n",
      "        [3.9612],\n",
      "        [3.7014]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 3, Loss: 4.468364403265231\n",
      "tensor([[ 0.7893, -0.6140],\n",
      "        [ 0.7836,  0.6213],\n",
      "        [ 0.6860,  0.7276]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.1161],\n",
      "        [3.8708],\n",
      "        [3.6029]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 4, Loss: 4.214668349210063\n",
      "tensor([[ 0.8510, -0.5251],\n",
      "        [ 0.7141,  0.7001],\n",
      "        [ 0.6294,  0.7771]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.1434],\n",
      "        [3.7847],\n",
      "        [3.5047]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 5, Loss: 3.916654595770572\n",
      "tensor([[ 0.9005, -0.4349],\n",
      "        [ 0.6388,  0.7694],\n",
      "        [ 0.5576,  0.8301]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.1894],\n",
      "        [3.7049],\n",
      "        [3.4074]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 6, Loss: 3.5210782561220753\n",
      "tensor([[ 0.9382, -0.3460],\n",
      "        [ 0.5597,  0.8287],\n",
      "        [ 0.4758,  0.8795]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.2483],\n",
      "        [3.6338],\n",
      "        [3.3119]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 7, Loss: 2.9833142025459893\n",
      "tensor([[ 0.9655, -0.2603],\n",
      "        [ 0.4792,  0.8777],\n",
      "        [ 0.3871,  0.9221]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.3158],\n",
      "        [3.5712],\n",
      "        [3.2187]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 8, Loss: 2.639498036902811\n",
      "tensor([[ 0.9836, -0.1802],\n",
      "        [ 0.3997,  0.9167],\n",
      "        [ 0.2937,  0.9559]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.3898],\n",
      "        [3.5195],\n",
      "        [3.1282]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 9, Loss: 2.323918572184181\n",
      "tensor([[ 0.9943, -0.1070],\n",
      "        [ 0.3224,  0.9466],\n",
      "        [ 0.1974,  0.9803]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.4688],\n",
      "        [3.4788],\n",
      "        [3.0401]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 10, Loss: 2.199548504924706\n",
      "tensor([[ 0.9991, -0.0421],\n",
      "        [ 0.2494,  0.9684],\n",
      "        [ 0.1008,  0.9949]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.5509],\n",
      "        [3.4490],\n",
      "        [2.9555]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 11, Loss: 1.901364604331553\n",
      "tensor([[0.9999, 0.0139],\n",
      "        [0.1824, 0.9832],\n",
      "        [0.0052, 1.0000]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.6353],\n",
      "        [3.4283],\n",
      "        [2.8741]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 12, Loss: 1.7992363989450677\n",
      "tensor([[ 0.9982,  0.0600],\n",
      "        [ 0.1221,  0.9925],\n",
      "        [-0.0878,  0.9961]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.7209],\n",
      "        [3.4158],\n",
      "        [2.7963]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 13, Loss: 1.6272717231835676\n",
      "tensor([[ 0.9954,  0.0960],\n",
      "        [ 0.0688,  0.9976],\n",
      "        [-0.1771,  0.9842]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.8069],\n",
      "        [3.4093],\n",
      "        [2.7223]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 14, Loss: 1.5395712109969406\n",
      "tensor([[ 0.9925,  0.1219],\n",
      "        [ 0.0228,  0.9997],\n",
      "        [-0.2618,  0.9651]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.8933],\n",
      "        [3.4086],\n",
      "        [2.6520]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 15, Loss: 1.440930693307424\n",
      "tensor([[ 0.9904,  0.1385],\n",
      "        [-0.0154,  0.9999],\n",
      "        [-0.3415,  0.9399]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[2.9793],\n",
      "        [3.4131],\n",
      "        [2.5855]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 16, Loss: 1.3638677432635615\n",
      "tensor([[ 0.9893,  0.1461],\n",
      "        [-0.0455,  0.9990],\n",
      "        [-0.4156,  0.9095]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.0646],\n",
      "        [3.4222],\n",
      "        [2.5229]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 17, Loss: 1.3095136172855144\n",
      "tensor([[ 0.9893,  0.1456],\n",
      "        [-0.0673,  0.9977],\n",
      "        [-0.4838,  0.8752]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.1489],\n",
      "        [3.4350],\n",
      "        [2.4643]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 18, Loss: 1.2305187420545334\n",
      "tensor([[ 0.9905,  0.1378],\n",
      "        [-0.0819,  0.9966],\n",
      "        [-0.5461,  0.8377]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.2327],\n",
      "        [3.4509],\n",
      "        [2.4099]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 19, Loss: 1.1743689414802594\n",
      "tensor([[ 0.9926,  0.1215],\n",
      "        [-0.0899,  0.9960],\n",
      "        [-0.6027,  0.7980]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.3148],\n",
      "        [3.4675],\n",
      "        [2.3591]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 20, Loss: 1.1830240481480747\n",
      "tensor([[ 0.9949,  0.1006],\n",
      "        [-0.0905,  0.9959],\n",
      "        [-0.6538,  0.7567]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.3963],\n",
      "        [3.4844],\n",
      "        [2.3122]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 21, Loss: 1.1393714085315525\n",
      "tensor([[ 0.9972,  0.0746],\n",
      "        [-0.0852,  0.9964],\n",
      "        [-0.6996,  0.7145]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.4771],\n",
      "        [3.5028],\n",
      "        [2.2689]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 22, Loss: 1.0881338235319642\n",
      "tensor([[ 0.9990,  0.0439],\n",
      "        [-0.0753,  0.9972],\n",
      "        [-0.7406,  0.6720]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.5569],\n",
      "        [3.5222],\n",
      "        [2.2289]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 23, Loss: 1.0927519785375928\n",
      "tensor([[ 1.0000,  0.0094],\n",
      "        [-0.0609,  0.9981],\n",
      "        [-0.7769,  0.6296]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.6359],\n",
      "        [3.5425],\n",
      "        [2.1924]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 24, Loss: 1.0464262976061527\n",
      "tensor([[ 0.9996, -0.0278],\n",
      "        [-0.0432,  0.9991],\n",
      "        [-0.8091,  0.5877]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.7142],\n",
      "        [3.5644],\n",
      "        [2.1590]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 25, Loss: 1.0262304433086458\n",
      "tensor([[ 0.9977, -0.0675],\n",
      "        [-0.0216,  0.9998],\n",
      "        [-0.8373,  0.5467]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.7913],\n",
      "        [3.5872],\n",
      "        [2.1288]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 26, Loss: 1.011969173099745\n",
      "tensor([[ 0.9943, -0.1064],\n",
      "        [ 0.0024,  1.0000],\n",
      "        [-0.8620,  0.5069]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.8683],\n",
      "        [3.6104],\n",
      "        [2.1015]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 27, Loss: 0.9890160197279447\n",
      "tensor([[ 0.9895, -0.1446],\n",
      "        [ 0.0276,  0.9996],\n",
      "        [-0.8836,  0.4683]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[3.9449],\n",
      "        [3.6367],\n",
      "        [2.0771]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 28, Loss: 0.9449228579820349\n",
      "tensor([[ 0.9835, -0.1810],\n",
      "        [ 0.0542,  0.9985],\n",
      "        [-0.9023,  0.4310]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.0205],\n",
      "        [3.6641],\n",
      "        [2.0552]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 29, Loss: 0.9697032388738475\n",
      "tensor([[ 0.9769, -0.2137],\n",
      "        [ 0.0809,  0.9967],\n",
      "        [-0.9186,  0.3953]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.0955],\n",
      "        [3.6929],\n",
      "        [2.0358]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 30, Loss: 0.9493902704425866\n",
      "tensor([[ 0.9699, -0.2434],\n",
      "        [ 0.1078,  0.9942],\n",
      "        [-0.9325,  0.3611]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.1695],\n",
      "        [3.7227],\n",
      "        [2.0188]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 31, Loss: 0.9297904236919217\n",
      "tensor([[ 0.9631, -0.2692],\n",
      "        [ 0.1353,  0.9908],\n",
      "        [-0.9445,  0.3286]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.2423],\n",
      "        [3.7531],\n",
      "        [2.0041]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 32, Loss: 0.925568648217228\n",
      "tensor([[ 0.9566, -0.2915],\n",
      "        [ 0.1616,  0.9869],\n",
      "        [-0.9547,  0.2976]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.3143],\n",
      "        [3.7849],\n",
      "        [1.9914]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 33, Loss: 0.908301698257986\n",
      "tensor([[ 0.9516, -0.3073],\n",
      "        [ 0.1854,  0.9827],\n",
      "        [-0.9633,  0.2683]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.3854],\n",
      "        [3.8186],\n",
      "        [1.9807]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 34, Loss: 0.9059040327065567\n",
      "tensor([[ 0.9484, -0.3170],\n",
      "        [ 0.2081,  0.9781],\n",
      "        [-0.9706,  0.2405]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.4548],\n",
      "        [3.8523],\n",
      "        [1.9719]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 35, Loss: 0.9232933484068774\n",
      "tensor([[ 0.9474, -0.3200],\n",
      "        [ 0.2284,  0.9736],\n",
      "        [-0.9768,  0.2142]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.5228],\n",
      "        [3.8867],\n",
      "        [1.9648]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 36, Loss: 0.915182942338916\n",
      "tensor([[ 0.9485, -0.3168],\n",
      "        [ 0.2455,  0.9694],\n",
      "        [-0.9819,  0.1894]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.5896],\n",
      "        [3.9222],\n",
      "        [1.9593]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 37, Loss: 0.9116442791170184\n",
      "tensor([[ 0.9514, -0.3079],\n",
      "        [ 0.2594,  0.9658],\n",
      "        [-0.9861,  0.1661]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.6552],\n",
      "        [3.9586],\n",
      "        [1.9553]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 38, Loss: 0.9044283742724917\n",
      "tensor([[ 0.9555, -0.2949],\n",
      "        [ 0.2708,  0.9626],\n",
      "        [-0.9895,  0.1442]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.7191],\n",
      "        [3.9940],\n",
      "        [1.9526]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 39, Loss: 0.920986669675916\n",
      "tensor([[ 0.9608, -0.2774],\n",
      "        [ 0.2790,  0.9603],\n",
      "        [-0.9923,  0.1236]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.7825],\n",
      "        [4.0302],\n",
      "        [1.9513]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 40, Loss: 0.8873366159110212\n",
      "tensor([[ 0.9658, -0.2592],\n",
      "        [ 0.2850,  0.9585],\n",
      "        [-0.9946,  0.1042]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.8446],\n",
      "        [4.0651],\n",
      "        [1.9511]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 41, Loss: 0.9131310382609797\n",
      "tensor([[ 0.9711, -0.2387],\n",
      "        [ 0.2872,  0.9579],\n",
      "        [-0.9963,  0.0860]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.9064],\n",
      "        [4.1003],\n",
      "        [1.9519]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 42, Loss: 0.8840274187461754\n",
      "tensor([[ 0.9760, -0.2179],\n",
      "        [ 0.2866,  0.9581],\n",
      "        [-0.9976,  0.0689]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[4.9676],\n",
      "        [4.1351],\n",
      "        [1.9538]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 43, Loss: 0.8863494822956177\n",
      "tensor([[ 0.9805, -0.1963],\n",
      "        [ 0.2841,  0.9588],\n",
      "        [-0.9986,  0.0530]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.0279],\n",
      "        [4.1691],\n",
      "        [1.9566]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 44, Loss: 0.9211123268877593\n",
      "tensor([[ 0.9845, -0.1752],\n",
      "        [ 0.2803,  0.9599],\n",
      "        [-0.9993,  0.0381]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.0876],\n",
      "        [4.2025],\n",
      "        [1.9603]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 45, Loss: 0.8861406970866404\n",
      "tensor([[ 0.9875, -0.1574],\n",
      "        [ 0.2756,  0.9613],\n",
      "        [-0.9997,  0.0242]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.1468],\n",
      "        [4.2361],\n",
      "        [1.9649]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 46, Loss: 0.8801318568261454\n",
      "tensor([[ 0.9901, -0.1405],\n",
      "        [ 0.2703,  0.9628],\n",
      "        [-0.9999,  0.0113]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.2054],\n",
      "        [4.2691],\n",
      "        [1.9701]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 47, Loss: 0.8904148359167067\n",
      "tensor([[ 9.9229e-01, -1.2396e-01],\n",
      "        [ 2.6252e-01,  9.6493e-01],\n",
      "        [-1.0000e+00, -7.2078e-04]], dtype=torch.float64,\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.2639],\n",
      "        [4.3016],\n",
      "        [1.9761]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 48, Loss: 0.8850189937602693\n",
      "tensor([[ 0.9938, -0.1114],\n",
      "        [ 0.2537,  0.9673],\n",
      "        [-0.9999, -0.0119]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.3213],\n",
      "        [4.3321],\n",
      "        [1.9827]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 49, Loss: 0.9111805494022804\n",
      "tensor([[ 0.9946, -0.1041],\n",
      "        [ 0.2447,  0.9696],\n",
      "        [-0.9998, -0.0224]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.3784],\n",
      "        [4.3634],\n",
      "        [1.9899]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 50, Loss: 0.8638484543961846\n",
      "tensor([[ 0.9947, -0.1024],\n",
      "        [ 0.2353,  0.9719],\n",
      "        [-0.9995, -0.0321]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.4350],\n",
      "        [4.3951],\n",
      "        [1.9976]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 51, Loss: 0.8697105440396031\n",
      "tensor([[ 0.9945, -0.1043],\n",
      "        [ 0.2256,  0.9742],\n",
      "        [-0.9992, -0.0410]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.4913],\n",
      "        [4.4262],\n",
      "        [2.0058]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 52, Loss: 0.8774106438149151\n",
      "tensor([[ 0.9943, -0.1071],\n",
      "        [ 0.2161,  0.9764],\n",
      "        [-0.9988, -0.0493]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.5473],\n",
      "        [4.4570],\n",
      "        [2.0144]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 53, Loss: 0.8758735564295721\n",
      "tensor([[ 0.9937, -0.1124],\n",
      "        [ 0.2068,  0.9784],\n",
      "        [-0.9984, -0.0570]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.6030],\n",
      "        [4.4881],\n",
      "        [2.0234]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 54, Loss: 0.864815029752368\n",
      "tensor([[ 0.9928, -0.1199],\n",
      "        [ 0.1988,  0.9800],\n",
      "        [-0.9980, -0.0639]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.6583],\n",
      "        [4.5191],\n",
      "        [2.0328]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 55, Loss: 0.8648575843914733\n",
      "tensor([[ 0.9916, -0.1290],\n",
      "        [ 0.1915,  0.9815],\n",
      "        [-0.9975, -0.0703]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.7134],\n",
      "        [4.5509],\n",
      "        [2.0425]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 56, Loss: 0.8685590418287455\n",
      "tensor([[ 0.9904, -0.1381],\n",
      "        [ 0.1867,  0.9824],\n",
      "        [-0.9971, -0.0762]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.7680],\n",
      "        [4.5821],\n",
      "        [2.0524]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 57, Loss: 0.8688765582243969\n",
      "tensor([[ 0.9893, -0.1462],\n",
      "        [ 0.1833,  0.9831],\n",
      "        [-0.9967, -0.0815]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.8222],\n",
      "        [4.6142],\n",
      "        [2.0627]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 58, Loss: 0.8481570303810685\n",
      "tensor([[ 0.9884, -0.1522],\n",
      "        [ 0.1818,  0.9833],\n",
      "        [-0.9963, -0.0865]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.8756],\n",
      "        [4.6449],\n",
      "        [2.0732]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 59, Loss: 0.8639841593524299\n",
      "tensor([[ 0.9872, -0.1593],\n",
      "        [ 0.1832,  0.9831],\n",
      "        [-0.9959, -0.0910]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.9280],\n",
      "        [4.6729],\n",
      "        [2.0839]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 60, Loss: 0.8762589556582353\n",
      "tensor([[ 0.9861, -0.1662],\n",
      "        [ 0.1854,  0.9827],\n",
      "        [-0.9955, -0.0950]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[5.9802],\n",
      "        [4.7016],\n",
      "        [2.0949]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 61, Loss: 0.8425620425706336\n",
      "tensor([[ 0.9850, -0.1728],\n",
      "        [ 0.1873,  0.9823],\n",
      "        [-0.9951, -0.0988]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.0323],\n",
      "        [4.7301],\n",
      "        [2.1060]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 62, Loss: 0.8385306526079537\n",
      "tensor([[ 0.9841, -0.1775],\n",
      "        [ 0.1881,  0.9822],\n",
      "        [-0.9948, -0.1022]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.0838],\n",
      "        [4.7577],\n",
      "        [2.1173]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 63, Loss: 0.8590744558109384\n",
      "tensor([[ 0.9835, -0.1808],\n",
      "        [ 0.1907,  0.9816],\n",
      "        [-0.9944, -0.1054]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.1350],\n",
      "        [4.7844],\n",
      "        [2.1287]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 64, Loss: 0.8569865256846964\n",
      "tensor([[ 0.9833, -0.1819],\n",
      "        [ 0.1936,  0.9811],\n",
      "        [-0.9941, -0.1082]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.1854],\n",
      "        [4.8084],\n",
      "        [2.1402]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 65, Loss: 0.8811183802678366\n",
      "tensor([[ 0.9835, -0.1808],\n",
      "        [ 0.1957,  0.9807],\n",
      "        [-0.9939, -0.1106]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.2354],\n",
      "        [4.8309],\n",
      "        [2.1517]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 66, Loss: 0.8728924451330938\n",
      "tensor([[ 0.9839, -0.1786],\n",
      "        [ 0.1973,  0.9803],\n",
      "        [-0.9936, -0.1129]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.2853],\n",
      "        [4.8547],\n",
      "        [2.1634]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 67, Loss: 0.8352724333626147\n",
      "tensor([[ 0.9846, -0.1747],\n",
      "        [ 0.1993,  0.9799],\n",
      "        [-0.9934, -0.1149]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.3342],\n",
      "        [4.8767],\n",
      "        [2.1751]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 68, Loss: 0.8702853710369493\n",
      "tensor([[ 0.9855, -0.1696],\n",
      "        [ 0.1999,  0.9798],\n",
      "        [-0.9932, -0.1167]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.3829],\n",
      "        [4.8974],\n",
      "        [2.1869]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 69, Loss: 0.8727406029844819\n",
      "tensor([[ 0.9860, -0.1668],\n",
      "        [ 0.2004,  0.9797],\n",
      "        [-0.9930, -0.1182]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.4313],\n",
      "        [4.9180],\n",
      "        [2.1988]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 70, Loss: 0.8410550861581072\n",
      "tensor([[ 0.9867, -0.1625],\n",
      "        [ 0.1994,  0.9799],\n",
      "        [-0.9928, -0.1196]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.4791],\n",
      "        [4.9387],\n",
      "        [2.2108]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 71, Loss: 0.8550272441756899\n",
      "tensor([[ 0.9880, -0.1546],\n",
      "        [ 0.1989,  0.9800],\n",
      "        [-0.9927, -0.1209]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.5267],\n",
      "        [4.9599],\n",
      "        [2.2228]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 72, Loss: 0.8514882388890683\n",
      "tensor([[ 0.9888, -0.1492],\n",
      "        [ 0.1994,  0.9799],\n",
      "        [-0.9926, -0.1218]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.5741],\n",
      "        [4.9804],\n",
      "        [2.2348]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 73, Loss: 0.8436504532331097\n",
      "tensor([[ 0.9895, -0.1448],\n",
      "        [ 0.2004,  0.9797],\n",
      "        [-0.9925, -0.1225]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.6214],\n",
      "        [5.0030],\n",
      "        [2.2468]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 74, Loss: 0.8226188024668935\n",
      "tensor([[ 0.9898, -0.1428],\n",
      "        [ 0.2006,  0.9797],\n",
      "        [-0.9924, -0.1231]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.6686],\n",
      "        [5.0269],\n",
      "        [2.2588]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 75, Loss: 0.8123031228380635\n",
      "tensor([[ 0.9896, -0.1437],\n",
      "        [ 0.1989,  0.9800],\n",
      "        [-0.9923, -0.1235]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.7153],\n",
      "        [5.0510],\n",
      "        [2.2708]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 76, Loss: 0.8296636073430879\n",
      "tensor([[ 0.9893, -0.1460],\n",
      "        [ 0.1976,  0.9803],\n",
      "        [-0.9923, -0.1238]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.7614],\n",
      "        [5.0744],\n",
      "        [2.2829]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 77, Loss: 0.8502758140726978\n",
      "tensor([[ 0.9885, -0.1514],\n",
      "        [ 0.1963,  0.9805],\n",
      "        [-0.9923, -0.1240]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.8068],\n",
      "        [5.0984],\n",
      "        [2.2950]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 78, Loss: 0.8295736594690528\n",
      "tensor([[ 0.9874, -0.1580],\n",
      "        [ 0.1949,  0.9808],\n",
      "        [-0.9923, -0.1240]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.8524],\n",
      "        [5.1226],\n",
      "        [2.3070]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 79, Loss: 0.8210675549048456\n",
      "tensor([[ 0.9864, -0.1643],\n",
      "        [ 0.1955,  0.9807],\n",
      "        [-0.9923, -0.1238]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.8973],\n",
      "        [5.1442],\n",
      "        [2.3191]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 80, Loss: 0.8661101307682153\n",
      "tensor([[ 0.9856, -0.1688],\n",
      "        [ 0.1956,  0.9807],\n",
      "        [-0.9923, -0.1235]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.9422],\n",
      "        [5.1667],\n",
      "        [2.3312]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 81, Loss: 0.8242809428475943\n",
      "tensor([[ 0.9843, -0.1763],\n",
      "        [ 0.1976,  0.9803],\n",
      "        [-0.9924, -0.1232]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[6.9860],\n",
      "        [5.1849],\n",
      "        [2.3432]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 82, Loss: 0.8791190215589755\n",
      "tensor([[ 0.9836, -0.1803],\n",
      "        [ 0.2008,  0.9796],\n",
      "        [-0.9924, -0.1229]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.0295],\n",
      "        [5.2028],\n",
      "        [2.3552]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 83, Loss: 0.8296301036634359\n",
      "tensor([[ 0.9832, -0.1826],\n",
      "        [ 0.2038,  0.9790],\n",
      "        [-0.9925, -0.1224]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.0730],\n",
      "        [5.2208],\n",
      "        [2.3672]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 84, Loss: 0.8163304876067636\n",
      "tensor([[ 0.9835, -0.1809],\n",
      "        [ 0.2071,  0.9783],\n",
      "        [-0.9925, -0.1219]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1161],\n",
      "        [5.2376],\n",
      "        [2.3793]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 85, Loss: 0.8439785227121559\n",
      "tensor([[ 0.9842, -0.1768],\n",
      "        [ 0.2093,  0.9778],\n",
      "        [-0.9926, -0.1214]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.1592],\n",
      "        [5.2550],\n",
      "        [2.3914]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 86, Loss: 0.8213788530261157\n",
      "tensor([[ 0.9848, -0.1735],\n",
      "        [ 0.2124,  0.9772],\n",
      "        [-0.9927, -0.1207]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2015],\n",
      "        [5.2727],\n",
      "        [2.4035]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 87, Loss: 0.8432985275912241\n",
      "tensor([[ 0.9862, -0.1655],\n",
      "        [ 0.2123,  0.9772],\n",
      "        [-0.9928, -0.1200]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2437],\n",
      "        [5.2888],\n",
      "        [2.4156]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 88, Loss: 0.8433238787762195\n",
      "tensor([[ 0.9883, -0.1525],\n",
      "        [ 0.2099,  0.9777],\n",
      "        [-0.9929, -0.1193]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.2858],\n",
      "        [5.3074],\n",
      "        [2.4276]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 89, Loss: 0.7990068214880485\n",
      "tensor([[ 0.9896, -0.1436],\n",
      "        [ 0.2091,  0.9779],\n",
      "        [-0.9930, -0.1185]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3278],\n",
      "        [5.3251],\n",
      "        [2.4396]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 90, Loss: 0.8285323016905664\n",
      "tensor([[ 0.9905, -0.1375],\n",
      "        [ 0.2052,  0.9787],\n",
      "        [-0.9931, -0.1177]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.3688],\n",
      "        [5.3443],\n",
      "        [2.4517]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 91, Loss: 0.8200192606719522\n",
      "tensor([[ 0.9908, -0.1353],\n",
      "        [ 0.2011,  0.9796],\n",
      "        [-0.9932, -0.1168]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4096],\n",
      "        [5.3635],\n",
      "        [2.4637]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 92, Loss: 0.8256584424853385\n",
      "tensor([[ 0.9907, -0.1357],\n",
      "        [ 0.1969,  0.9804],\n",
      "        [-0.9933, -0.1159]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4504],\n",
      "        [5.3818],\n",
      "        [2.4757]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 93, Loss: 0.8227982372950944\n",
      "tensor([[ 0.9906, -0.1370],\n",
      "        [ 0.1928,  0.9812],\n",
      "        [-0.9934, -0.1149]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.4911],\n",
      "        [5.4023],\n",
      "        [2.4876]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 94, Loss: 0.7936894610024593\n",
      "tensor([[ 0.9906, -0.1365],\n",
      "        [ 0.1901,  0.9818],\n",
      "        [-0.9935, -0.1138]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5320],\n",
      "        [5.4222],\n",
      "        [2.4995]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 95, Loss: 0.8333702669121785\n",
      "tensor([[ 0.9903, -0.1387],\n",
      "        [ 0.1899,  0.9818],\n",
      "        [-0.9936, -0.1126]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5722],\n",
      "        [5.4413],\n",
      "        [2.5113]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 96, Loss: 0.8305084373906614\n",
      "tensor([[ 0.9901, -0.1401],\n",
      "        [ 0.1907,  0.9816],\n",
      "        [-0.9938, -0.1112]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6128],\n",
      "        [5.4612],\n",
      "        [2.5231]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 97, Loss: 0.8044564695768512\n",
      "tensor([[ 0.9897, -0.1429],\n",
      "        [ 0.1954,  0.9807],\n",
      "        [-0.9940, -0.1097]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6528],\n",
      "        [5.4784],\n",
      "        [2.5348]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 98, Loss: 0.844156497037842\n",
      "tensor([[ 0.9889, -0.1483],\n",
      "        [ 0.2005,  0.9797],\n",
      "        [-0.9941, -0.1081]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6924],\n",
      "        [5.4956],\n",
      "        [2.5466]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 99, Loss: 0.8119434358038037\n",
      "tensor([[ 0.9886, -0.1504],\n",
      "        [ 0.2018,  0.9794],\n",
      "        [-0.9943, -0.1065]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7321],\n",
      "        [5.5143],\n",
      "        [2.5583]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 100, Loss: 0.7932894103496524\n",
      "tensor([[ 0.9872, -0.1596],\n",
      "        [ 0.2056,  0.9786],\n",
      "        [-0.9945, -0.1050]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7715],\n",
      "        [5.5299],\n",
      "        [2.5700]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 101, Loss: 0.8284806709571869\n",
      "tensor([[ 0.9858, -0.1682],\n",
      "        [ 0.2080,  0.9781],\n",
      "        [-0.9946, -0.1036]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8102],\n",
      "        [5.5446],\n",
      "        [2.5817]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 102, Loss: 0.8407824486023494\n",
      "tensor([[ 0.9848, -0.1739],\n",
      "        [ 0.2104,  0.9776],\n",
      "        [-0.9947, -0.1023]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8484],\n",
      "        [5.5554],\n",
      "        [2.5933]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 103, Loss: 0.8504443553255964\n",
      "tensor([[ 0.9838, -0.1792],\n",
      "        [ 0.2104,  0.9776],\n",
      "        [-0.9949, -0.1012]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8866],\n",
      "        [5.5652],\n",
      "        [2.6049]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 104, Loss: 0.8178625704295993\n",
      "tensor([[ 0.9834, -0.1817],\n",
      "        [ 0.2086,  0.9780],\n",
      "        [-0.9950, -0.1001]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9243],\n",
      "        [5.5750],\n",
      "        [2.6164]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 105, Loss: 0.8386443370752132\n",
      "tensor([[ 0.9839, -0.1788],\n",
      "        [ 0.2100,  0.9777],\n",
      "        [-0.9951, -0.0990]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9612],\n",
      "        [5.5827],\n",
      "        [2.6279]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 106, Loss: 0.8421068797210195\n",
      "tensor([[ 0.9845, -0.1755],\n",
      "        [ 0.2108,  0.9775],\n",
      "        [-0.9952, -0.0977]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9980],\n",
      "        [5.5894],\n",
      "        [2.6392]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 107, Loss: 0.8457477011996565\n",
      "tensor([[ 0.9858, -0.1682],\n",
      "        [ 0.2110,  0.9775],\n",
      "        [-0.9953, -0.0966]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0348],\n",
      "        [5.5944],\n",
      "        [2.6506]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 108, Loss: 0.8373938252675146\n",
      "tensor([[ 0.9873, -0.1588],\n",
      "        [ 0.2100,  0.9777],\n",
      "        [-0.9954, -0.0956]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0714],\n",
      "        [5.6007],\n",
      "        [2.6620]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 109, Loss: 0.8016413485998823\n",
      "tensor([[ 0.9891, -0.1473],\n",
      "        [ 0.2080,  0.9781],\n",
      "        [-0.9955, -0.0948]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1083],\n",
      "        [5.6076],\n",
      "        [2.6733]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 110, Loss: 0.7976252240408229\n",
      "tensor([[ 0.9902, -0.1400],\n",
      "        [ 0.2077,  0.9782],\n",
      "        [-0.9956, -0.0941]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1454],\n",
      "        [5.6162],\n",
      "        [2.6847]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 111, Loss: 0.798759785366364\n",
      "tensor([[ 0.9910, -0.1339],\n",
      "        [ 0.2055,  0.9787],\n",
      "        [-0.9956, -0.0934]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1827],\n",
      "        [5.6255],\n",
      "        [2.6959]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 112, Loss: 0.794876366728436\n",
      "tensor([[ 0.9915, -0.1303],\n",
      "        [ 0.2050,  0.9788],\n",
      "        [-0.9957, -0.0927]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2199],\n",
      "        [5.6338],\n",
      "        [2.7072]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 113, Loss: 0.8206282598618589\n",
      "tensor([[ 0.9916, -0.1291],\n",
      "        [ 0.2062,  0.9785],\n",
      "        [-0.9958, -0.0920]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2565],\n",
      "        [5.6393],\n",
      "        [2.7184]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 114, Loss: 0.8427934926096544\n",
      "tensor([[ 0.9913, -0.1318],\n",
      "        [ 0.2059,  0.9786],\n",
      "        [-0.9958, -0.0914]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2929],\n",
      "        [5.6445],\n",
      "        [2.7296]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 115, Loss: 0.8156601584692488\n",
      "tensor([[ 0.9901, -0.1405],\n",
      "        [ 0.2054,  0.9787],\n",
      "        [-0.9959, -0.0908]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3291],\n",
      "        [5.6473],\n",
      "        [2.7407]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 116, Loss: 0.8366707156502923\n",
      "tensor([[ 0.9881, -0.1537],\n",
      "        [ 0.2047,  0.9788],\n",
      "        [-0.9959, -0.0901]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3648],\n",
      "        [5.6522],\n",
      "        [2.7519]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 117, Loss: 0.8081575454991013\n",
      "tensor([[ 0.9866, -0.1634],\n",
      "        [ 0.2072,  0.9783],\n",
      "        [-0.9960, -0.0893]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4003],\n",
      "        [5.6548],\n",
      "        [2.7630]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 118, Loss: 0.8462853280328348\n",
      "tensor([[ 0.9847, -0.1740],\n",
      "        [ 0.2104,  0.9776],\n",
      "        [-0.9961, -0.0886]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4356],\n",
      "        [5.6578],\n",
      "        [2.7741]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 119, Loss: 0.8054980498591563\n",
      "tensor([[ 0.9834, -0.1817],\n",
      "        [ 0.2115,  0.9774],\n",
      "        [-0.9961, -0.0880]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4709],\n",
      "        [5.6599],\n",
      "        [2.7851]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 120, Loss: 0.8116149653323027\n",
      "tensor([[ 0.9817, -0.1902],\n",
      "        [ 0.2151,  0.9766],\n",
      "        [-0.9962, -0.0872]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5059],\n",
      "        [5.6599],\n",
      "        [2.7962]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 121, Loss: 0.8283731392212473\n",
      "tensor([[ 0.9805, -0.1966],\n",
      "        [ 0.2187,  0.9758],\n",
      "        [-0.9963, -0.0865]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5412],\n",
      "        [5.6611],\n",
      "        [2.8072]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 122, Loss: 0.8013360859004671\n",
      "tensor([[ 0.9806, -0.1958],\n",
      "        [ 0.2192,  0.9757],\n",
      "        [-0.9963, -0.0857]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5759],\n",
      "        [5.6622],\n",
      "        [2.8183]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 123, Loss: 0.8240411547616024\n",
      "tensor([[ 0.9813, -0.1924],\n",
      "        [ 0.2230,  0.9748],\n",
      "        [-0.9964, -0.0848]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.6095],\n",
      "        [5.6607],\n",
      "        [2.8293]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 124, Loss: 0.8496084613705235\n",
      "tensor([[ 0.9831, -0.1832],\n",
      "        [ 0.2251,  0.9743],\n",
      "        [-0.9965, -0.0837]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.6426],\n",
      "        [5.6573],\n",
      "        [2.8402]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 125, Loss: 0.8447150922075863\n",
      "tensor([[ 0.9839, -0.1786],\n",
      "        [ 0.2299,  0.9732],\n",
      "        [-0.9966, -0.0827]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.6750],\n",
      "        [5.6527],\n",
      "        [2.8512]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 126, Loss: 0.8289690582646067\n",
      "tensor([[ 0.9854, -0.1702],\n",
      "        [ 0.2330,  0.9725],\n",
      "        [-0.9967, -0.0818]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.7075],\n",
      "        [5.6498],\n",
      "        [2.8620]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 127, Loss: 0.8120997747732588\n",
      "tensor([[ 0.9877, -0.1561],\n",
      "        [ 0.2328,  0.9725],\n",
      "        [-0.9967, -0.0810]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.7400],\n",
      "        [5.6491],\n",
      "        [2.8728]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 128, Loss: 0.8112497646789599\n",
      "tensor([[ 0.9896, -0.1440],\n",
      "        [ 0.2303,  0.9731],\n",
      "        [-0.9968, -0.0804]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.7727],\n",
      "        [5.6484],\n",
      "        [2.8836]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 129, Loss: 0.807561156234342\n",
      "tensor([[ 0.9908, -0.1352],\n",
      "        [ 0.2299,  0.9732],\n",
      "        [-0.9968, -0.0796]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.8043],\n",
      "        [5.6468],\n",
      "        [2.8943]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 130, Loss: 0.848401323327384\n",
      "tensor([[ 0.9921, -0.1251],\n",
      "        [ 0.2274,  0.9738],\n",
      "        [-0.9969, -0.0789]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.8364],\n",
      "        [5.6484],\n",
      "        [2.9050]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 131, Loss: 0.774719206309066\n",
      "tensor([[ 0.9927, -0.1205],\n",
      "        [ 0.2231,  0.9748],\n",
      "        [-0.9969, -0.0781]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.8692],\n",
      "        [5.6505],\n",
      "        [2.9156]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 132, Loss: 0.793130453827015\n",
      "tensor([[ 0.9927, -0.1208],\n",
      "        [ 0.2213,  0.9752],\n",
      "        [-0.9970, -0.0774]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.9007],\n",
      "        [5.6501],\n",
      "        [2.9262]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 133, Loss: 0.8572630855100996\n",
      "tensor([[ 0.9919, -0.1269],\n",
      "        [ 0.2183,  0.9759],\n",
      "        [-0.9971, -0.0767]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.9322],\n",
      "        [5.6505],\n",
      "        [2.9368]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 134, Loss: 0.8061520285541511\n",
      "tensor([[ 0.9913, -0.1316],\n",
      "        [ 0.2114,  0.9774],\n",
      "        [-0.9971, -0.0760]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.9635],\n",
      "        [5.6507],\n",
      "        [2.9472]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 135, Loss: 0.8272735198208698\n",
      "tensor([[ 0.9909, -0.1346],\n",
      "        [ 0.2049,  0.9788],\n",
      "        [-0.9971, -0.0755]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.9953],\n",
      "        [5.6546],\n",
      "        [2.9576]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 136, Loss: 0.7758434741284499\n",
      "tensor([[ 0.9905, -0.1374],\n",
      "        [ 0.1995,  0.9799],\n",
      "        [-0.9972, -0.0750]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.0271],\n",
      "        [5.6605],\n",
      "        [2.9681]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 137, Loss: 0.8156908784984329\n",
      "tensor([[ 0.9900, -0.1412],\n",
      "        [ 0.1958,  0.9806],\n",
      "        [-0.9972, -0.0744]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.0589],\n",
      "        [5.6623],\n",
      "        [2.9784]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 138, Loss: 0.8374734647536116\n",
      "tensor([[ 0.9887, -0.1498],\n",
      "        [ 0.1973,  0.9803],\n",
      "        [-0.9973, -0.0739]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.0900],\n",
      "        [5.6588],\n",
      "        [2.9888]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 139, Loss: 0.8540978812188432\n",
      "tensor([[ 0.9877, -0.1563],\n",
      "        [ 0.1980,  0.9802],\n",
      "        [-0.9973, -0.0736]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.1214],\n",
      "        [5.6562],\n",
      "        [2.9991]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 140, Loss: 0.7973002892061135\n",
      "tensor([[ 0.9872, -0.1597],\n",
      "        [ 0.1993,  0.9799],\n",
      "        [-0.9973, -0.0732]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.1530],\n",
      "        [5.6562],\n",
      "        [3.0095]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 141, Loss: 0.7892776298288046\n",
      "tensor([[ 0.9880, -0.1546],\n",
      "        [ 0.1977,  0.9803],\n",
      "        [-0.9974, -0.0726]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.1840],\n",
      "        [5.6561],\n",
      "        [3.0199]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 142, Loss: 0.8212556406445519\n",
      "tensor([[ 0.9895, -0.1442],\n",
      "        [ 0.1965,  0.9805],\n",
      "        [-0.9974, -0.0719]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.2144],\n",
      "        [5.6547],\n",
      "        [3.0301]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 143, Loss: 0.8349526846854861\n",
      "tensor([[ 0.9907, -0.1358],\n",
      "        [ 0.1956,  0.9807],\n",
      "        [-0.9975, -0.0711]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.2451],\n",
      "        [5.6537],\n",
      "        [3.0404]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 144, Loss: 0.7962456787450295\n",
      "tensor([[ 0.9915, -0.1304],\n",
      "        [ 0.1939,  0.9810],\n",
      "        [-0.9975, -0.0706]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.2761],\n",
      "        [5.6528],\n",
      "        [3.0506]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 145, Loss: 0.8080859545019607\n",
      "tensor([[ 0.9915, -0.1300],\n",
      "        [ 0.1953,  0.9808],\n",
      "        [-0.9976, -0.0698]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.3073],\n",
      "        [5.6511],\n",
      "        [3.0607]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 146, Loss: 0.8247668671109593\n",
      "tensor([[ 0.9907, -0.1361],\n",
      "        [ 0.2003,  0.9797],\n",
      "        [-0.9976, -0.0694]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.3382],\n",
      "        [5.6481],\n",
      "        [3.0709]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 147, Loss: 0.8061789030221613\n",
      "tensor([[ 0.9897, -0.1431],\n",
      "        [ 0.2036,  0.9791],\n",
      "        [-0.9976, -0.0689]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.3693],\n",
      "        [5.6474],\n",
      "        [3.0811]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 148, Loss: 0.7705531291836302\n",
      "tensor([[ 0.9885, -0.1512],\n",
      "        [ 0.2113,  0.9774],\n",
      "        [-0.9977, -0.0685]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.4004],\n",
      "        [5.6428],\n",
      "        [3.0912]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 149, Loss: 0.8333358152339955\n",
      "tensor([[ 0.9870, -0.1608],\n",
      "        [ 0.2199,  0.9755],\n",
      "        [-0.9977, -0.0682]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.4309],\n",
      "        [5.6383],\n",
      "        [3.1014]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 150, Loss: 0.819405335444744\n",
      "tensor([[ 0.9847, -0.1744],\n",
      "        [ 0.2248,  0.9744],\n",
      "        [-0.9977, -0.0680]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.4609],\n",
      "        [5.6336],\n",
      "        [3.1115]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 151, Loss: 0.8095063951346254\n",
      "tensor([[ 0.9837, -0.1800],\n",
      "        [ 0.2272,  0.9739],\n",
      "        [-0.9977, -0.0676]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.4909],\n",
      "        [5.6318],\n",
      "        [3.1216]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 152, Loss: 0.7818896108228689\n",
      "tensor([[ 0.9840, -0.1783],\n",
      "        [ 0.2256,  0.9742],\n",
      "        [-0.9977, -0.0675]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.5203],\n",
      "        [5.6327],\n",
      "        [3.1318]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 153, Loss: 0.797719363504345\n",
      "tensor([[ 0.9841, -0.1774],\n",
      "        [ 0.2248,  0.9744],\n",
      "        [-0.9977, -0.0675]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.5500],\n",
      "        [5.6318],\n",
      "        [3.1419]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 154, Loss: 0.8069807125940666\n",
      "tensor([[ 0.9847, -0.1741],\n",
      "        [ 0.2222,  0.9750],\n",
      "        [-0.9977, -0.0674]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.5792],\n",
      "        [5.6336],\n",
      "        [3.1519]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 155, Loss: 0.8007210417687245\n",
      "tensor([[ 0.9859, -0.1672],\n",
      "        [ 0.2218,  0.9751],\n",
      "        [-0.9977, -0.0673]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.6084],\n",
      "        [5.6349],\n",
      "        [3.1619]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 156, Loss: 0.7956742499696181\n",
      "tensor([[ 0.9873, -0.1592],\n",
      "        [ 0.2195,  0.9756],\n",
      "        [-0.9977, -0.0673]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.6373],\n",
      "        [5.6326],\n",
      "        [3.1717]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 157, Loss: 0.8513280498379268\n",
      "tensor([[ 0.9892, -0.1468],\n",
      "        [ 0.2149,  0.9766],\n",
      "        [-0.9977, -0.0673]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.6661],\n",
      "        [5.6314],\n",
      "        [3.1816]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 158, Loss: 0.7954136283857255\n",
      "tensor([[ 0.9907, -0.1358],\n",
      "        [ 0.2151,  0.9766],\n",
      "        [-0.9977, -0.0674]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.6946],\n",
      "        [5.6289],\n",
      "        [3.1914]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 159, Loss: 0.8146323483447896\n",
      "tensor([[ 0.9917, -0.1282],\n",
      "        [ 0.2156,  0.9765],\n",
      "        [-0.9977, -0.0676]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.7232],\n",
      "        [5.6284],\n",
      "        [3.2012]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 160, Loss: 0.7900438761507993\n",
      "tensor([[ 0.9930, -0.1184],\n",
      "        [ 0.2182,  0.9759],\n",
      "        [-0.9977, -0.0677]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.7514],\n",
      "        [5.6281],\n",
      "        [3.2110]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 161, Loss: 0.8218649301972905\n",
      "tensor([[ 0.9930, -0.1184],\n",
      "        [ 0.2206,  0.9754],\n",
      "        [-0.9977, -0.0677]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.7799],\n",
      "        [5.6284],\n",
      "        [3.2208]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 162, Loss: 0.7886275749690814\n",
      "tensor([[ 0.9931, -0.1173],\n",
      "        [ 0.2260,  0.9741],\n",
      "        [-0.9977, -0.0677]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.8079],\n",
      "        [5.6274],\n",
      "        [3.2305]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 163, Loss: 0.8266301667497526\n",
      "tensor([[ 0.9937, -0.1121],\n",
      "        [ 0.2240,  0.9746],\n",
      "        [-0.9977, -0.0678]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.8363],\n",
      "        [5.6280],\n",
      "        [3.2404]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 164, Loss: 0.785971467073907\n",
      "tensor([[ 0.9944, -0.1055],\n",
      "        [ 0.2188,  0.9758],\n",
      "        [-0.9977, -0.0680]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.8652],\n",
      "        [5.6299],\n",
      "        [3.2501]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 165, Loss: 0.7797086488672209\n",
      "tensor([[ 0.9949, -0.1011],\n",
      "        [ 0.2119,  0.9773],\n",
      "        [-0.9977, -0.0682]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.8941],\n",
      "        [5.6309],\n",
      "        [3.2598]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 166, Loss: 0.8149284576811509\n",
      "tensor([[ 0.9948, -0.1014],\n",
      "        [ 0.2028,  0.9792],\n",
      "        [-0.9977, -0.0684]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.9233],\n",
      "        [5.6342],\n",
      "        [3.2695]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 167, Loss: 0.7755219423264027\n",
      "tensor([[ 0.9931, -0.1169],\n",
      "        [ 0.1950,  0.9808],\n",
      "        [-0.9976, -0.0687]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.9522],\n",
      "        [5.6381],\n",
      "        [3.2792]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 168, Loss: 0.7882921025871488\n",
      "tensor([[ 0.9907, -0.1363],\n",
      "        [ 0.1875,  0.9823],\n",
      "        [-0.9976, -0.0692]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.9803],\n",
      "        [5.6382],\n",
      "        [3.2889]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 169, Loss: 0.8341753232680243\n",
      "tensor([[ 0.9872, -0.1596],\n",
      "        [ 0.1802,  0.9836],\n",
      "        [-0.9976, -0.0698]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.0088],\n",
      "        [ 5.6383],\n",
      "        [ 3.2985]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 170, Loss: 0.7802267746224616\n",
      "tensor([[ 0.9834, -0.1817],\n",
      "        [ 0.1736,  0.9848],\n",
      "        [-0.9975, -0.0704]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.0373],\n",
      "        [ 5.6353],\n",
      "        [ 3.3081]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 171, Loss: 0.8191830046821377\n",
      "tensor([[ 0.9803, -0.1977],\n",
      "        [ 0.1708,  0.9853],\n",
      "        [-0.9975, -0.0709]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.0661],\n",
      "        [ 5.6358],\n",
      "        [ 3.3177]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 172, Loss: 0.7576665224161939\n",
      "tensor([[ 0.9792, -0.2030],\n",
      "        [ 0.1701,  0.9854],\n",
      "        [-0.9975, -0.0711]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.0947],\n",
      "        [ 5.6378],\n",
      "        [ 3.3272]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 173, Loss: 0.7909664327651408\n",
      "tensor([[ 0.9803, -0.1975],\n",
      "        [ 0.1759,  0.9844],\n",
      "        [-0.9975, -0.0712]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.1223],\n",
      "        [ 5.6374],\n",
      "        [ 3.3365]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 174, Loss: 0.8350358639892334\n",
      "tensor([[ 0.9822, -0.1878],\n",
      "        [ 0.1850,  0.9827],\n",
      "        [-0.9975, -0.0713]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.1501],\n",
      "        [ 5.6363],\n",
      "        [ 3.3458]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 175, Loss: 0.7953709467837767\n",
      "tensor([[ 0.9842, -0.1772],\n",
      "        [ 0.1933,  0.9811],\n",
      "        [-0.9975, -0.0713]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.1784],\n",
      "        [ 5.6360],\n",
      "        [ 3.3551]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 176, Loss: 0.7854465543517801\n",
      "tensor([[ 0.9861, -0.1659],\n",
      "        [ 0.2034,  0.9791],\n",
      "        [-0.9975, -0.0711]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.2058],\n",
      "        [ 5.6357],\n",
      "        [ 3.3644]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 177, Loss: 0.8007958838077593\n",
      "tensor([[ 0.9889, -0.1484],\n",
      "        [ 0.2116,  0.9774],\n",
      "        [-0.9975, -0.0713]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.2331],\n",
      "        [ 5.6349],\n",
      "        [ 3.3736]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 178, Loss: 0.7975000566826993\n",
      "tensor([[ 0.9916, -0.1291],\n",
      "        [ 0.2188,  0.9758],\n",
      "        [-0.9975, -0.0713]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.2600],\n",
      "        [ 5.6313],\n",
      "        [ 3.3828]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 179, Loss: 0.8449080033717712\n",
      "tensor([[ 0.9930, -0.1178],\n",
      "        [ 0.2248,  0.9744],\n",
      "        [-0.9975, -0.0713]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.2870],\n",
      "        [ 5.6282],\n",
      "        [ 3.3920]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 180, Loss: 0.7879753861117897\n",
      "tensor([[ 0.9942, -0.1076],\n",
      "        [ 0.2316,  0.9728],\n",
      "        [-0.9975, -0.0709]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.3135],\n",
      "        [ 5.6215],\n",
      "        [ 3.4012]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 181, Loss: 0.83825929486973\n",
      "tensor([[ 0.9943, -0.1071],\n",
      "        [ 0.2371,  0.9715],\n",
      "        [-0.9975, -0.0707]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.3401],\n",
      "        [ 5.6165],\n",
      "        [ 3.4102]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 182, Loss: 0.792807865979224\n",
      "tensor([[ 0.9931, -0.1172],\n",
      "        [ 0.2410,  0.9705],\n",
      "        [-0.9975, -0.0706]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.3670],\n",
      "        [ 5.6129],\n",
      "        [ 3.4193]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 183, Loss: 0.7773210861694304\n",
      "tensor([[ 0.9922, -0.1246],\n",
      "        [ 0.2441,  0.9697],\n",
      "        [-0.9975, -0.0705]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.3942],\n",
      "        [ 5.6108],\n",
      "        [ 3.4285]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 184, Loss: 0.7909684892441066\n",
      "tensor([[ 0.9904, -0.1381],\n",
      "        [ 0.2475,  0.9689],\n",
      "        [-0.9975, -0.0702]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.4212],\n",
      "        [ 5.6074],\n",
      "        [ 3.4377]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 185, Loss: 0.8111217480280958\n",
      "tensor([[ 0.9874, -0.1583],\n",
      "        [ 0.2481,  0.9687],\n",
      "        [-0.9975, -0.0700]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.4473],\n",
      "        [ 5.5973],\n",
      "        [ 3.4468]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 186, Loss: 0.860562892083981\n",
      "tensor([[ 0.9854, -0.1700],\n",
      "        [ 0.2516,  0.9678],\n",
      "        [-0.9976, -0.0699]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.4732],\n",
      "        [ 5.5852],\n",
      "        [ 3.4559]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 187, Loss: 0.8166052316470622\n",
      "tensor([[ 0.9838, -0.1793],\n",
      "        [ 0.2507,  0.9681],\n",
      "        [-0.9976, -0.0697]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.4986],\n",
      "        [ 5.5713],\n",
      "        [ 3.4648]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 188, Loss: 0.844904745987649\n",
      "tensor([[ 0.9834, -0.1813],\n",
      "        [ 0.2515,  0.9679],\n",
      "        [-0.9976, -0.0694]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.5240],\n",
      "        [ 5.5562],\n",
      "        [ 3.4737]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 189, Loss: 0.8196275849306653\n",
      "tensor([[ 0.9851, -0.1717],\n",
      "        [ 0.2501,  0.9682],\n",
      "        [-0.9976, -0.0689]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.5498],\n",
      "        [ 5.5447],\n",
      "        [ 3.4826]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 190, Loss: 0.7789523365660586\n",
      "tensor([[ 0.9872, -0.1595],\n",
      "        [ 0.2460,  0.9693],\n",
      "        [-0.9976, -0.0686]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.5757],\n",
      "        [ 5.5361],\n",
      "        [ 3.4914]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 191, Loss: 0.7916324936628419\n",
      "tensor([[ 0.9909, -0.1346],\n",
      "        [ 0.2426,  0.9701],\n",
      "        [-0.9977, -0.0683]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.6015],\n",
      "        [ 5.5282],\n",
      "        [ 3.5002]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 192, Loss: 0.7983152134189083\n",
      "tensor([[ 0.9934, -0.1148],\n",
      "        [ 0.2351,  0.9720],\n",
      "        [-0.9977, -0.0678]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.6275],\n",
      "        [ 5.5235],\n",
      "        [ 3.5089]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 193, Loss: 0.7839601865045783\n",
      "tensor([[ 0.9957, -0.0930],\n",
      "        [ 0.2252,  0.9743],\n",
      "        [-0.9977, -0.0672]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.6532],\n",
      "        [ 5.5188],\n",
      "        [ 3.5176]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 194, Loss: 0.8121010452936771\n",
      "tensor([[ 0.9974, -0.0727],\n",
      "        [ 0.2103,  0.9776],\n",
      "        [-0.9978, -0.0664]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.6789],\n",
      "        [ 5.5165],\n",
      "        [ 3.5262]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 195, Loss: 0.7836216759532625\n",
      "tensor([[ 0.9976, -0.0698],\n",
      "        [ 0.1956,  0.9807],\n",
      "        [-0.9978, -0.0657]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.7044],\n",
      "        [ 5.5152],\n",
      "        [ 3.5349]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 196, Loss: 0.7962778523064208\n",
      "tensor([[ 0.9972, -0.0747],\n",
      "        [ 0.1872,  0.9823],\n",
      "        [-0.9979, -0.0647]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.7305],\n",
      "        [ 5.5134],\n",
      "        [ 3.5436]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 197, Loss: 0.7872964686220248\n",
      "tensor([[ 0.9961, -0.0879],\n",
      "        [ 0.1788,  0.9839],\n",
      "        [-0.9980, -0.0636]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.7571],\n",
      "        [ 5.5138],\n",
      "        [ 3.5522]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 198, Loss: 0.7781040846226202\n",
      "tensor([[ 0.9945, -0.1049],\n",
      "        [ 0.1711,  0.9853],\n",
      "        [-0.9980, -0.0627]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.7840],\n",
      "        [ 5.5164],\n",
      "        [ 3.5608]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 199, Loss: 0.7804369190414837\n",
      "tensor([[ 0.9917, -0.1284],\n",
      "        [ 0.1688,  0.9857],\n",
      "        [-0.9981, -0.0618]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.8110],\n",
      "        [ 5.5183],\n",
      "        [ 3.5695]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 200, Loss: 0.7958866814916474\n",
      "tensor([[ 0.9886, -0.1509],\n",
      "        [ 0.1664,  0.9861],\n",
      "        [-0.9981, -0.0609]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.8386],\n",
      "        [ 5.5222],\n",
      "        [ 3.5781]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 201, Loss: 0.7691159219411847\n",
      "tensor([[ 0.9839, -0.1789],\n",
      "        [ 0.1731,  0.9849],\n",
      "        [-0.9982, -0.0597]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.8660],\n",
      "        [ 5.5246],\n",
      "        [ 3.5868]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 202, Loss: 0.8100502012745557\n",
      "tensor([[ 0.9803, -0.1977],\n",
      "        [ 0.1834,  0.9830],\n",
      "        [-0.9983, -0.0586]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.8932],\n",
      "        [ 5.5304],\n",
      "        [ 3.5954]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 203, Loss: 0.7629448571182234\n",
      "tensor([[ 0.9788, -0.2050],\n",
      "        [ 0.1936,  0.9811],\n",
      "        [-0.9984, -0.0573]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.9200],\n",
      "        [ 5.5383],\n",
      "        [ 3.6039]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 204, Loss: 0.7850697417905694\n",
      "tensor([[ 0.9800, -0.1989],\n",
      "        [ 0.2042,  0.9789],\n",
      "        [-0.9984, -0.0560]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.9463],\n",
      "        [ 5.5474],\n",
      "        [ 3.6124]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 205, Loss: 0.7883628840838142\n",
      "tensor([[ 0.9829, -0.1839],\n",
      "        [ 0.2110,  0.9775],\n",
      "        [-0.9985, -0.0549]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.9716],\n",
      "        [ 5.5564],\n",
      "        [ 3.6209]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 206, Loss: 0.8129219940557196\n",
      "tensor([[ 0.9873, -0.1588],\n",
      "        [ 0.2165,  0.9763],\n",
      "        [-0.9985, -0.0539]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.9968],\n",
      "        [ 5.5658],\n",
      "        [ 3.6294]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 207, Loss: 0.7915833569286893\n",
      "tensor([[ 0.9901, -0.1401],\n",
      "        [ 0.2219,  0.9751],\n",
      "        [-0.9986, -0.0533]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.0223],\n",
      "        [ 5.5747],\n",
      "        [ 3.6377]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 208, Loss: 0.7873718980921252\n",
      "tensor([[ 0.9930, -0.1178],\n",
      "        [ 0.2226,  0.9749],\n",
      "        [-0.9986, -0.0528]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.0478],\n",
      "        [ 5.5858],\n",
      "        [ 3.6461]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 209, Loss: 0.7796166576695864\n",
      "tensor([[ 0.9945, -0.1050],\n",
      "        [ 0.2255,  0.9742],\n",
      "        [-0.9986, -0.0526]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.0734],\n",
      "        [ 5.5947],\n",
      "        [ 3.6545]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 210, Loss: 0.7944506638340209\n",
      "tensor([[ 0.9940, -0.1090],\n",
      "        [ 0.2253,  0.9743],\n",
      "        [-0.9986, -0.0524]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.0991],\n",
      "        [ 5.6032],\n",
      "        [ 3.6630]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 211, Loss: 0.776533119045759\n",
      "tensor([[ 0.9933, -0.1152],\n",
      "        [ 0.2217,  0.9751],\n",
      "        [-0.9986, -0.0524]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.1243],\n",
      "        [ 5.6107],\n",
      "        [ 3.6714]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 212, Loss: 0.7929746386753059\n",
      "tensor([[ 0.9925, -0.1225],\n",
      "        [ 0.2143,  0.9768],\n",
      "        [-0.9986, -0.0524]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.1499],\n",
      "        [ 5.6189],\n",
      "        [ 3.6798]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 213, Loss: 0.7757544590831146\n",
      "tensor([[ 0.9905, -0.1375],\n",
      "        [ 0.2084,  0.9780],\n",
      "        [-0.9986, -0.0521]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.1753],\n",
      "        [ 5.6268],\n",
      "        [ 3.6881]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 214, Loss: 0.8022514549195833\n",
      "tensor([[ 0.9889, -0.1487],\n",
      "        [ 0.1994,  0.9799],\n",
      "        [-0.9987, -0.0516]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.2007],\n",
      "        [ 5.6364],\n",
      "        [ 3.6965]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 215, Loss: 0.7672556851992083\n",
      "tensor([[ 0.9872, -0.1598],\n",
      "        [ 0.1901,  0.9818],\n",
      "        [-0.9987, -0.0511]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.2258],\n",
      "        [ 5.6472],\n",
      "        [ 3.7047]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 216, Loss: 0.7821100043385051\n",
      "tensor([[ 0.9859, -0.1673],\n",
      "        [ 0.1813,  0.9834],\n",
      "        [-0.9987, -0.0507]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.2510],\n",
      "        [ 5.6571],\n",
      "        [ 3.7128]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 217, Loss: 0.8016364957427061\n",
      "tensor([[ 0.9863, -0.1647],\n",
      "        [ 0.1725,  0.9850],\n",
      "        [-0.9987, -0.0502]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.2766],\n",
      "        [ 5.6666],\n",
      "        [ 3.7208]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 218, Loss: 0.771967898653224\n",
      "tensor([[ 0.9872, -0.1597],\n",
      "        [ 0.1671,  0.9859],\n",
      "        [-0.9988, -0.0497]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.3023],\n",
      "        [ 5.6749],\n",
      "        [ 3.7288]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 219, Loss: 0.8025803164618133\n",
      "tensor([[ 0.9883, -0.1526],\n",
      "        [ 0.1670,  0.9860],\n",
      "        [-0.9988, -0.0488]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.3283],\n",
      "        [ 5.6852],\n",
      "        [ 3.7366]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 220, Loss: 0.7654220550968208\n",
      "tensor([[ 0.9895, -0.1446],\n",
      "        [ 0.1681,  0.9858],\n",
      "        [-0.9988, -0.0480]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.3543],\n",
      "        [ 5.6967],\n",
      "        [ 3.7445]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 221, Loss: 0.7564904305291185\n",
      "tensor([[ 0.9899, -0.1419],\n",
      "        [ 0.1685,  0.9857],\n",
      "        [-0.9989, -0.0475]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.3795],\n",
      "        [ 5.7052],\n",
      "        [ 3.7526]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 222, Loss: 0.7969959170720435\n",
      "tensor([[ 0.9894, -0.1453],\n",
      "        [ 0.1745,  0.9847],\n",
      "        [-0.9989, -0.0469]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.4044],\n",
      "        [ 5.7126],\n",
      "        [ 3.7606]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 223, Loss: 0.7998026264000044\n",
      "tensor([[ 0.9898, -0.1423],\n",
      "        [ 0.1779,  0.9840],\n",
      "        [-0.9989, -0.0460]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.4286],\n",
      "        [ 5.7202],\n",
      "        [ 3.7686]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 224, Loss: 0.8129737960918918\n",
      "tensor([[ 0.9908, -0.1355],\n",
      "        [ 0.1775,  0.9841],\n",
      "        [-0.9990, -0.0454]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.4529],\n",
      "        [ 5.7256],\n",
      "        [ 3.7765]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 225, Loss: 0.7920286587431901\n",
      "tensor([[ 0.9912, -0.1323],\n",
      "        [ 0.1799,  0.9837],\n",
      "        [-0.9990, -0.0447]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.4770],\n",
      "        [ 5.7304],\n",
      "        [ 3.7844]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 226, Loss: 0.7877053510133599\n",
      "tensor([[ 0.9913, -0.1317],\n",
      "        [ 0.1858,  0.9826],\n",
      "        [-0.9990, -0.0443]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.5014],\n",
      "        [ 5.7319],\n",
      "        [ 3.7922]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 227, Loss: 0.7980109978783624\n",
      "tensor([[ 0.9919, -0.1273],\n",
      "        [ 0.1901,  0.9818],\n",
      "        [-0.9990, -0.0439]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.5255],\n",
      "        [ 5.7305],\n",
      "        [ 3.8001]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 228, Loss: 0.8151854335332098\n",
      "tensor([[ 0.9932, -0.1165],\n",
      "        [ 0.1891,  0.9820],\n",
      "        [-0.9990, -0.0440]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.5498],\n",
      "        [ 5.7309],\n",
      "        [ 3.8079]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 229, Loss: 0.772478250882115\n",
      "tensor([[ 0.9932, -0.1162],\n",
      "        [ 0.1925,  0.9813],\n",
      "        [-0.9990, -0.0437]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.5741],\n",
      "        [ 5.7315],\n",
      "        [ 3.8157]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 230, Loss: 0.8012262414364509\n",
      "tensor([[ 0.9924, -0.1229],\n",
      "        [ 0.1960,  0.9806],\n",
      "        [-0.9991, -0.0432]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.5990],\n",
      "        [ 5.7315],\n",
      "        [ 3.8234]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 231, Loss: 0.7754825772943642\n",
      "tensor([[ 0.9909, -0.1348],\n",
      "        [ 0.2023,  0.9793],\n",
      "        [-0.9991, -0.0425]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.6231],\n",
      "        [ 5.7315],\n",
      "        [ 3.8312]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 232, Loss: 0.8002216417710294\n",
      "tensor([[ 0.9881, -0.1536],\n",
      "        [ 0.2106,  0.9776],\n",
      "        [-0.9991, -0.0419]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.6468],\n",
      "        [ 5.7309],\n",
      "        [ 3.8390]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 233, Loss: 0.7973330678049629\n",
      "tensor([[ 0.9857, -0.1683],\n",
      "        [ 0.2181,  0.9759],\n",
      "        [-0.9991, -0.0414]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.6712],\n",
      "        [ 5.7306],\n",
      "        [ 3.8466]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 234, Loss: 0.7701567007549448\n",
      "tensor([[ 0.9836, -0.1805],\n",
      "        [ 0.2252,  0.9743],\n",
      "        [-0.9992, -0.0407]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.6956],\n",
      "        [ 5.7315],\n",
      "        [ 3.8542]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 235, Loss: 0.7745426976016493\n",
      "tensor([[ 0.9842, -0.1771],\n",
      "        [ 0.2317,  0.9728],\n",
      "        [-0.9992, -0.0401]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.7193],\n",
      "        [ 5.7330],\n",
      "        [ 3.8618]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 236, Loss: 0.7992722668185476\n",
      "tensor([[ 0.9861, -0.1664],\n",
      "        [ 0.2383,  0.9712],\n",
      "        [-0.9992, -0.0398]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.7422],\n",
      "        [ 5.7357],\n",
      "        [ 3.8694]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 237, Loss: 0.782783855325107\n",
      "tensor([[ 0.9880, -0.1546],\n",
      "        [ 0.2407,  0.9706],\n",
      "        [-0.9992, -0.0396]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.7642],\n",
      "        [ 5.7421],\n",
      "        [ 3.8770]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 238, Loss: 0.7724975487939565\n",
      "tensor([[ 0.9900, -0.1408],\n",
      "        [ 0.2438,  0.9698],\n",
      "        [-0.9992, -0.0394]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.7862],\n",
      "        [ 5.7479],\n",
      "        [ 3.8845]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 239, Loss: 0.7925815083137033\n",
      "tensor([[ 0.9910, -0.1342],\n",
      "        [ 0.2457,  0.9694],\n",
      "        [-0.9992, -0.0394]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.8074],\n",
      "        [ 5.7528],\n",
      "        [ 3.8919]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 240, Loss: 0.8091610235996356\n",
      "tensor([[ 0.9916, -0.1293],\n",
      "        [ 0.2425,  0.9702],\n",
      "        [-0.9992, -0.0397]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.8291],\n",
      "        [ 5.7565],\n",
      "        [ 3.8993]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 241, Loss: 0.7846810718791154\n",
      "tensor([[ 0.9930, -0.1180],\n",
      "        [ 0.2390,  0.9710],\n",
      "        [-0.9992, -0.0397]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.8503],\n",
      "        [ 5.7618],\n",
      "        [ 3.9067]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 242, Loss: 0.7902178597547787\n",
      "tensor([[ 0.9951, -0.0987],\n",
      "        [ 0.2371,  0.9715],\n",
      "        [-0.9992, -0.0397]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.8713],\n",
      "        [ 5.7685],\n",
      "        [ 3.9141]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 243, Loss: 0.7783830322366324\n",
      "tensor([[ 0.9954, -0.0962],\n",
      "        [ 0.2390,  0.9710],\n",
      "        [-0.9992, -0.0400]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.8917],\n",
      "        [ 5.7701],\n",
      "        [ 3.9215]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 244, Loss: 0.8198412711351466\n",
      "tensor([[ 0.9941, -0.1081],\n",
      "        [ 0.2411,  0.9705],\n",
      "        [-0.9992, -0.0403]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.9125],\n",
      "        [ 5.7673],\n",
      "        [ 3.9288]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 245, Loss: 0.8197390095803826\n",
      "tensor([[ 0.9927, -0.1203],\n",
      "        [ 0.2384,  0.9712],\n",
      "        [-0.9992, -0.0406]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.9328],\n",
      "        [ 5.7669],\n",
      "        [ 3.9362]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 246, Loss: 0.7742015858031065\n",
      "tensor([[ 0.9910, -0.1338],\n",
      "        [ 0.2374,  0.9714],\n",
      "        [-0.9992, -0.0412]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.9531],\n",
      "        [ 5.7656],\n",
      "        [ 3.9437]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 247, Loss: 0.7898246609220537\n",
      "tensor([[ 0.9901, -0.1407],\n",
      "        [ 0.2317,  0.9728],\n",
      "        [-0.9991, -0.0420]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.9737],\n",
      "        [ 5.7668],\n",
      "        [ 3.9513]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 248, Loss: 0.7603879913393881\n",
      "tensor([[ 0.9892, -0.1467],\n",
      "        [ 0.2292,  0.9734],\n",
      "        [-0.9991, -0.0428]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.9939],\n",
      "        [ 5.7662],\n",
      "        [ 3.9589]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 249, Loss: 0.8043407883371039\n",
      "tensor([[ 0.9887, -0.1501],\n",
      "        [ 0.2252,  0.9743],\n",
      "        [-0.9990, -0.0436]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.0146],\n",
      "        [ 5.7649],\n",
      "        [ 3.9665]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 250, Loss: 0.7774585589050406\n",
      "tensor([[ 0.9884, -0.1521],\n",
      "        [ 0.2259,  0.9742],\n",
      "        [-0.9990, -0.0443]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.0353],\n",
      "        [ 5.7567],\n",
      "        [ 3.9742]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 251, Loss: 0.8245790809952487\n",
      "tensor([[ 0.9893, -0.1462],\n",
      "        [ 0.2255,  0.9742],\n",
      "        [-0.9990, -0.0445]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.0559],\n",
      "        [ 5.7507],\n",
      "        [ 3.9818]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 252, Loss: 0.7893151097913791\n",
      "tensor([[ 0.9904, -0.1384],\n",
      "        [ 0.2221,  0.9750],\n",
      "        [-0.9990, -0.0447]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.0768],\n",
      "        [ 5.7457],\n",
      "        [ 3.9892]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 253, Loss: 0.7871962949611945\n",
      "tensor([[ 0.9922, -0.1249],\n",
      "        [ 0.2156,  0.9765],\n",
      "        [-0.9990, -0.0450]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.0976],\n",
      "        [ 5.7370],\n",
      "        [ 3.9965]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 254, Loss: 0.8173766196396822\n",
      "tensor([[ 0.9927, -0.1209],\n",
      "        [ 0.2172,  0.9761],\n",
      "        [-0.9990, -0.0449]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.1184],\n",
      "        [ 5.7237],\n",
      "        [ 4.0039]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 255, Loss: 0.8129055880870826\n",
      "tensor([[ 0.9932, -0.1162],\n",
      "        [ 0.2178,  0.9760],\n",
      "        [-0.9990, -0.0450]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.1388],\n",
      "        [ 5.7081],\n",
      "        [ 4.0111]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 256, Loss: 0.8516475578639554\n",
      "tensor([[ 0.9942, -0.1079],\n",
      "        [ 0.2104,  0.9776],\n",
      "        [-0.9990, -0.0452]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.1592],\n",
      "        [ 5.6963],\n",
      "        [ 4.0183]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 257, Loss: 0.7646537377751476\n",
      "tensor([[ 0.9940, -0.1094],\n",
      "        [ 0.2040,  0.9790],\n",
      "        [-0.9990, -0.0454]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.1793],\n",
      "        [ 5.6821],\n",
      "        [ 4.0254]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 258, Loss: 0.8179745584797541\n",
      "tensor([[ 0.9932, -0.1164],\n",
      "        [ 0.2016,  0.9795],\n",
      "        [-0.9989, -0.0459]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.1993],\n",
      "        [ 5.6645],\n",
      "        [ 4.0326]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 259, Loss: 0.8178833058745552\n",
      "tensor([[ 0.9920, -0.1266],\n",
      "        [ 0.2039,  0.9790],\n",
      "        [-0.9989, -0.0463]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.2195],\n",
      "        [ 5.6453],\n",
      "        [ 4.0397]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 260, Loss: 0.7989148558284169\n",
      "tensor([[ 0.9901, -0.1407],\n",
      "        [ 0.2028,  0.9792],\n",
      "        [-0.9989, -0.0470]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.2407],\n",
      "        [ 5.6281],\n",
      "        [ 4.0469]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 261, Loss: 0.7586186222271001\n",
      "tensor([[ 0.9875, -0.1576],\n",
      "        [ 0.2032,  0.9791],\n",
      "        [-0.9989, -0.0477]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.2615],\n",
      "        [ 5.6149],\n",
      "        [ 4.0540]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 262, Loss: 0.7840976240283223\n",
      "tensor([[ 0.9862, -0.1655],\n",
      "        [ 0.2055,  0.9787],\n",
      "        [-0.9988, -0.0483]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.2832],\n",
      "        [ 5.6038],\n",
      "        [ 4.0611]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 263, Loss: 0.7715029860990705\n",
      "tensor([[ 0.9853, -0.1707],\n",
      "        [ 0.2065,  0.9784],\n",
      "        [-0.9988, -0.0490]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.3041],\n",
      "        [ 5.5975],\n",
      "        [ 4.0682]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 264, Loss: 0.767728771308963\n",
      "tensor([[ 0.9856, -0.1692],\n",
      "        [ 0.2093,  0.9779],\n",
      "        [-0.9988, -0.0495]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.3248],\n",
      "        [ 5.5964],\n",
      "        [ 4.0752]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 265, Loss: 0.7574994299883514\n",
      "tensor([[ 0.9863, -0.1651],\n",
      "        [ 0.2156,  0.9765],\n",
      "        [-0.9988, -0.0497]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.3456],\n",
      "        [ 5.5899],\n",
      "        [ 4.0822]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 266, Loss: 0.8259623925544255\n",
      "tensor([[ 0.9871, -0.1599],\n",
      "        [ 0.2214,  0.9752],\n",
      "        [-0.9988, -0.0499]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.3674],\n",
      "        [ 5.5835],\n",
      "        [ 4.0890]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 267, Loss: 0.7724509333287747\n",
      "tensor([[ 0.9884, -0.1516],\n",
      "        [ 0.2306,  0.9730],\n",
      "        [-0.9987, -0.0502]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.3886],\n",
      "        [ 5.5766],\n",
      "        [ 4.0960]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 268, Loss: 0.7921774712352461\n",
      "tensor([[ 0.9904, -0.1385],\n",
      "        [ 0.2356,  0.9718],\n",
      "        [-0.9987, -0.0502]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.4097],\n",
      "        [ 5.5692],\n",
      "        [ 4.1029]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 269, Loss: 0.8024102188832428\n",
      "tensor([[ 0.9917, -0.1288],\n",
      "        [ 0.2362,  0.9717],\n",
      "        [-0.9987, -0.0503]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.4304],\n",
      "        [ 5.5632],\n",
      "        [ 4.1098]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 270, Loss: 0.7812742997134164\n",
      "tensor([[ 0.9934, -0.1149],\n",
      "        [ 0.2342,  0.9722],\n",
      "        [-0.9987, -0.0504]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.4506],\n",
      "        [ 5.5557],\n",
      "        [ 4.1166]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 271, Loss: 0.8245597480927948\n",
      "tensor([[ 0.9942, -0.1074],\n",
      "        [ 0.2281,  0.9736],\n",
      "        [-0.9987, -0.0506]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.4714],\n",
      "        [ 5.5491],\n",
      "        [ 4.1233]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 272, Loss: 0.779954117711315\n",
      "tensor([[ 0.9941, -0.1081],\n",
      "        [ 0.2167,  0.9762],\n",
      "        [-0.9987, -0.0506]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.4923],\n",
      "        [ 5.5459],\n",
      "        [ 4.1300]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 273, Loss: 0.7661079465297835\n",
      "tensor([[ 0.9941, -0.1084],\n",
      "        [ 0.2019,  0.9794],\n",
      "        [-0.9987, -0.0507]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.5128],\n",
      "        [ 5.5458],\n",
      "        [ 4.1366]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 274, Loss: 0.7893557940247391\n",
      "tensor([[ 0.9920, -0.1259],\n",
      "        [ 0.1887,  0.9820],\n",
      "        [-0.9987, -0.0508]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.5332],\n",
      "        [ 5.5494],\n",
      "        [ 4.1434]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 275, Loss: 0.7507450884297758\n",
      "tensor([[ 0.9888, -0.1493],\n",
      "        [ 0.1839,  0.9829],\n",
      "        [-0.9987, -0.0504]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.5536],\n",
      "        [ 5.5514],\n",
      "        [ 4.1500]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 276, Loss: 0.7862382328832695\n",
      "tensor([[ 0.9858, -0.1677],\n",
      "        [ 0.1866,  0.9824],\n",
      "        [-0.9987, -0.0500]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.5731],\n",
      "        [ 5.5514],\n",
      "        [ 4.1566]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 277, Loss: 0.8130198809283463\n",
      "tensor([[ 0.9856, -0.1694],\n",
      "        [ 0.1880,  0.9822],\n",
      "        [-0.9988, -0.0499]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.5929],\n",
      "        [ 5.5544],\n",
      "        [ 4.1633]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 278, Loss: 0.7699450300646152\n",
      "tensor([[ 0.9857, -0.1685],\n",
      "        [ 0.1887,  0.9820],\n",
      "        [-0.9988, -0.0496]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.6127],\n",
      "        [ 5.5571],\n",
      "        [ 4.1697]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 279, Loss: 0.7935050071689013\n",
      "tensor([[ 0.9868, -0.1618],\n",
      "        [ 0.1867,  0.9824],\n",
      "        [-0.9988, -0.0495]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.6312],\n",
      "        [ 5.5575],\n",
      "        [ 4.1763]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 280, Loss: 0.8082287850622021\n",
      "tensor([[ 0.9889, -0.1484],\n",
      "        [ 0.1866,  0.9824],\n",
      "        [-0.9988, -0.0492]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.6502],\n",
      "        [ 5.5569],\n",
      "        [ 4.1828]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 281, Loss: 0.7897778776200596\n",
      "tensor([[ 0.9906, -0.1369],\n",
      "        [ 0.1916,  0.9815],\n",
      "        [-0.9988, -0.0490]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.6684],\n",
      "        [ 5.5578],\n",
      "        [ 4.1895]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 282, Loss: 0.778020209548933\n",
      "tensor([[ 0.9927, -0.1204],\n",
      "        [ 0.1914,  0.9815],\n",
      "        [-0.9988, -0.0490]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.6867],\n",
      "        [ 5.5604],\n",
      "        [ 4.1961]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 283, Loss: 0.7730579765868043\n",
      "tensor([[ 0.9949, -0.1006],\n",
      "        [ 0.1924,  0.9813],\n",
      "        [-0.9988, -0.0484]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.7056],\n",
      "        [ 5.5650],\n",
      "        [ 4.2026]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 284, Loss: 0.78358143372407\n",
      "tensor([[ 0.9952, -0.0982],\n",
      "        [ 0.1991,  0.9800],\n",
      "        [-0.9989, -0.0476]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.7245],\n",
      "        [ 5.5650],\n",
      "        [ 4.2090]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 285, Loss: 0.8140795874256791\n",
      "tensor([[ 0.9946, -0.1041],\n",
      "        [ 0.2054,  0.9787],\n",
      "        [-0.9989, -0.0467]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.7436],\n",
      "        [ 5.5677],\n",
      "        [ 4.2155]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 286, Loss: 0.7584162399411349\n",
      "tensor([[ 0.9924, -0.1227],\n",
      "        [ 0.2163,  0.9763],\n",
      "        [-0.9990, -0.0455]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.7618],\n",
      "        [ 5.5628],\n",
      "        [ 4.2219]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 287, Loss: 0.8485249178108487\n",
      "tensor([[ 0.9891, -0.1475],\n",
      "        [ 0.2260,  0.9741],\n",
      "        [-0.9990, -0.0447]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.7804],\n",
      "        [ 5.5578],\n",
      "        [ 4.2284]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 288, Loss: 0.7842276915035673\n",
      "tensor([[ 0.9856, -0.1692],\n",
      "        [ 0.2368,  0.9715],\n",
      "        [-0.9990, -0.0439]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.7992],\n",
      "        [ 5.5520],\n",
      "        [ 4.2348]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 289, Loss: 0.7857472514636256\n",
      "tensor([[ 0.9819, -0.1896],\n",
      "        [ 0.2462,  0.9692],\n",
      "        [-0.9991, -0.0432]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.8177],\n",
      "        [ 5.5515],\n",
      "        [ 4.2414]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 290, Loss: 0.7502107217305032\n",
      "tensor([[ 0.9810, -0.1942],\n",
      "        [ 0.2485,  0.9686],\n",
      "        [-0.9991, -0.0428]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.8354],\n",
      "        [ 5.5481],\n",
      "        [ 4.2478]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 291, Loss: 0.8192990882386708\n",
      "tensor([[ 0.9831, -0.1830],\n",
      "        [ 0.2446,  0.9696],\n",
      "        [-0.9991, -0.0420]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.8531],\n",
      "        [ 5.5470],\n",
      "        [ 4.2542]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 292, Loss: 0.7704535884268073\n",
      "tensor([[ 0.9861, -0.1662],\n",
      "        [ 0.2382,  0.9712],\n",
      "        [-0.9992, -0.0409]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.8715],\n",
      "        [ 5.5502],\n",
      "        [ 4.2606]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 293, Loss: 0.7522377498107415\n",
      "tensor([[ 0.9894, -0.1452],\n",
      "        [ 0.2317,  0.9728],\n",
      "        [-0.9992, -0.0399]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.8898],\n",
      "        [ 5.5549],\n",
      "        [ 4.2670]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 294, Loss: 0.7741227744448558\n",
      "tensor([[ 0.9926, -0.1210],\n",
      "        [ 0.2211,  0.9753],\n",
      "        [-0.9992, -0.0390]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.9084],\n",
      "        [ 5.5632],\n",
      "        [ 4.2734]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 295, Loss: 0.7572748438751137\n",
      "tensor([[ 0.9958, -0.0911],\n",
      "        [ 0.2088,  0.9780],\n",
      "        [-0.9993, -0.0382]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.9272],\n",
      "        [ 5.5754],\n",
      "        [ 4.2799]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 296, Loss: 0.746695100919489\n",
      "tensor([[ 0.9976, -0.0698],\n",
      "        [ 0.2021,  0.9794],\n",
      "        [-0.9993, -0.0379]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.9457],\n",
      "        [ 5.5856],\n",
      "        [ 4.2863]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 297, Loss: 0.7905295153070045\n",
      "tensor([[ 0.9975, -0.0708],\n",
      "        [ 0.1945,  0.9809],\n",
      "        [-0.9993, -0.0380]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.9648],\n",
      "        [ 5.5940],\n",
      "        [ 4.2926]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 298, Loss: 0.7898458491992483\n",
      "tensor([[ 0.9963, -0.0856],\n",
      "        [ 0.1847,  0.9828],\n",
      "        [-0.9993, -0.0376]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.9840],\n",
      "        [ 5.6045],\n",
      "        [ 4.2988]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 299, Loss: 0.7812340482892498\n",
      "tensor([[ 0.9937, -0.1119],\n",
      "        [ 0.1813,  0.9834],\n",
      "        [-0.9993, -0.0373]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.0025],\n",
      "        [ 5.6107],\n",
      "        [ 4.3049]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 300, Loss: 0.810400178247366\n",
      "tensor([[ 0.9909, -0.1345],\n",
      "        [ 0.1779,  0.9841],\n",
      "        [-0.9993, -0.0375]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.0207],\n",
      "        [ 5.6158],\n",
      "        [ 4.3109]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 301, Loss: 0.8030891221416413\n",
      "tensor([[ 0.9867, -0.1628],\n",
      "        [ 0.1839,  0.9830],\n",
      "        [-0.9993, -0.0373]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.0395],\n",
      "        [ 5.6207],\n",
      "        [ 4.3170]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 302, Loss: 0.7570849902197414\n",
      "tensor([[ 0.9826, -0.1855],\n",
      "        [ 0.1933,  0.9811],\n",
      "        [-0.9993, -0.0373]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.0573],\n",
      "        [ 5.6271],\n",
      "        [ 4.3231]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 303, Loss: 0.7758379377977742\n",
      "tensor([[ 0.9815, -0.1914],\n",
      "        [ 0.2005,  0.9797],\n",
      "        [-0.9993, -0.0374]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.0749],\n",
      "        [ 5.6318],\n",
      "        [ 4.3292]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 304, Loss: 0.7919897770577484\n",
      "tensor([[ 0.9834, -0.1814],\n",
      "        [ 0.2050,  0.9788],\n",
      "        [-0.9993, -0.0379]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.0924],\n",
      "        [ 5.6338],\n",
      "        [ 4.3354]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 305, Loss: 0.794766095252401\n",
      "tensor([[ 0.9878, -0.1557],\n",
      "        [ 0.2057,  0.9786],\n",
      "        [-0.9993, -0.0382]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.1097],\n",
      "        [ 5.6353],\n",
      "        [ 4.3415]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 306, Loss: 0.7905141007728279\n",
      "tensor([[ 0.9921, -0.1252],\n",
      "        [ 0.2058,  0.9786],\n",
      "        [-0.9992, -0.0389]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.1274],\n",
      "        [ 5.6383],\n",
      "        [ 4.3476]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 307, Loss: 0.7746428851393743\n",
      "tensor([[ 0.9944, -0.1057],\n",
      "        [ 0.2118,  0.9773],\n",
      "        [-0.9992, -0.0392]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.1462],\n",
      "        [ 5.6434],\n",
      "        [ 4.3537]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 308, Loss: 0.7450503135904522\n",
      "tensor([[ 0.9956, -0.0932],\n",
      "        [ 0.2208,  0.9753],\n",
      "        [-0.9992, -0.0397]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.1648],\n",
      "        [ 5.6456],\n",
      "        [ 4.3598]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 309, Loss: 0.8017444862230078\n",
      "tensor([[ 0.9961, -0.0878],\n",
      "        [ 0.2252,  0.9743],\n",
      "        [-0.9992, -0.0406]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.1830],\n",
      "        [ 5.6497],\n",
      "        [ 4.3658]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 310, Loss: 0.7797469970416896\n",
      "tensor([[ 0.9952, -0.0977],\n",
      "        [ 0.2358,  0.9718],\n",
      "        [-0.9992, -0.0408]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.2012],\n",
      "        [ 5.6484],\n",
      "        [ 4.3719]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 311, Loss: 0.8179943163632042\n",
      "tensor([[ 0.9922, -0.1250],\n",
      "        [ 0.2496,  0.9684],\n",
      "        [-0.9992, -0.0410]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.2184],\n",
      "        [ 5.6441],\n",
      "        [ 4.3779]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 312, Loss: 0.8174655931495906\n",
      "tensor([[ 0.9892, -0.1463],\n",
      "        [ 0.2557,  0.9668],\n",
      "        [-0.9992, -0.0410]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.2349],\n",
      "        [ 5.6383],\n",
      "        [ 4.3840]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 313, Loss: 0.8190636812662713\n",
      "tensor([[ 0.9865, -0.1640],\n",
      "        [ 0.2561,  0.9666],\n",
      "        [-0.9992, -0.0409]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.2518],\n",
      "        [ 5.6373],\n",
      "        [ 4.3902]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 314, Loss: 0.7548919010980096\n",
      "tensor([[ 0.9858, -0.1678],\n",
      "        [ 0.2508,  0.9680],\n",
      "        [-0.9992, -0.0409]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.2694],\n",
      "        [ 5.6316],\n",
      "        [ 4.3965]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 315, Loss: 0.8041352681125802\n",
      "tensor([[ 0.9871, -0.1603],\n",
      "        [ 0.2417,  0.9704],\n",
      "        [-0.9992, -0.0404]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.2868],\n",
      "        [ 5.6233],\n",
      "        [ 4.4027]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 316, Loss: 0.813554025385267\n",
      "tensor([[ 0.9903, -0.1392],\n",
      "        [ 0.2264,  0.9740],\n",
      "        [-0.9992, -0.0404]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.3040],\n",
      "        [ 5.6181],\n",
      "        [ 4.4090]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 317, Loss: 0.7490199424922143\n",
      "tensor([[ 0.9941, -0.1088],\n",
      "        [ 0.2129,  0.9771],\n",
      "        [-0.9992, -0.0403]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.3218],\n",
      "        [ 5.6131],\n",
      "        [ 4.4154]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 318, Loss: 0.7781877684690574\n",
      "tensor([[ 0.9960, -0.0889],\n",
      "        [ 0.1952,  0.9808],\n",
      "        [-0.9992, -0.0404]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.3387],\n",
      "        [ 5.6101],\n",
      "        [ 4.4216]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 319, Loss: 0.7794006988754668\n",
      "tensor([[ 0.9971, -0.0758],\n",
      "        [ 0.1838,  0.9830],\n",
      "        [-0.9992, -0.0410]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.3560],\n",
      "        [ 5.6034],\n",
      "        [ 4.4279]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 320, Loss: 0.8002410195992015\n",
      "tensor([[ 0.9975, -0.0707],\n",
      "        [ 0.1743,  0.9847],\n",
      "        [-0.9991, -0.0414]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.3738],\n",
      "        [ 5.5965],\n",
      "        [ 4.4339]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 321, Loss: 0.7888214661765308\n",
      "tensor([[ 0.9968, -0.0800],\n",
      "        [ 0.1696,  0.9855],\n",
      "        [-0.9991, -0.0418]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.3917],\n",
      "        [ 5.5912],\n",
      "        [ 4.4398]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 322, Loss: 0.7786874945136116\n",
      "tensor([[ 0.9950, -0.0996],\n",
      "        [ 0.1664,  0.9861],\n",
      "        [-0.9991, -0.0421]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.4093],\n",
      "        [ 5.5899],\n",
      "        [ 4.4457]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 323, Loss: 0.7750764741961484\n",
      "tensor([[ 0.9919, -0.1273],\n",
      "        [ 0.1697,  0.9855],\n",
      "        [-0.9991, -0.0425]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.4269],\n",
      "        [ 5.5864],\n",
      "        [ 4.4516]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 324, Loss: 0.7896602769031628\n",
      "tensor([[ 0.9906, -0.1366],\n",
      "        [ 0.1733,  0.9849],\n",
      "        [-0.9991, -0.0429]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.4447],\n",
      "        [ 5.5868],\n",
      "        [ 4.4575]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 325, Loss: 0.7577553489415211\n",
      "tensor([[ 0.9904, -0.1381],\n",
      "        [ 0.1769,  0.9842],\n",
      "        [-0.9991, -0.0431]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.4619],\n",
      "        [ 5.5888],\n",
      "        [ 4.4633]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 326, Loss: 0.7848723112667215\n",
      "tensor([[ 0.9904, -0.1384],\n",
      "        [ 0.1767,  0.9843],\n",
      "        [-0.9991, -0.0432]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.4789],\n",
      "        [ 5.5925],\n",
      "        [ 4.4692]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 327, Loss: 0.7731791213178625\n",
      "tensor([[ 0.9909, -0.1344],\n",
      "        [ 0.1756,  0.9845],\n",
      "        [-0.9991, -0.0429]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.4962],\n",
      "        [ 5.5990],\n",
      "        [ 4.4751]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 328, Loss: 0.7585955806976563\n",
      "tensor([[ 0.9908, -0.1352],\n",
      "        [ 0.1816,  0.9834],\n",
      "        [-0.9991, -0.0427]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.5139],\n",
      "        [ 5.6056],\n",
      "        [ 4.4811]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 329, Loss: 0.7640269864033236\n",
      "tensor([[ 0.9905, -0.1378],\n",
      "        [ 0.1854,  0.9827],\n",
      "        [-0.9991, -0.0430]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.5318],\n",
      "        [ 5.6135],\n",
      "        [ 4.4868]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 330, Loss: 0.7878990501469936\n",
      "tensor([[ 0.9912, -0.1326],\n",
      "        [ 0.1879,  0.9822],\n",
      "        [-0.9991, -0.0433]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.5501],\n",
      "        [ 5.6236],\n",
      "        [ 4.4924]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 331, Loss: 0.770472577963175\n",
      "tensor([[ 0.9912, -0.1324],\n",
      "        [ 0.1968,  0.9804],\n",
      "        [-0.9991, -0.0432]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.5680],\n",
      "        [ 5.6299],\n",
      "        [ 4.4981]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 332, Loss: 0.8067668235294686\n",
      "tensor([[ 0.9926, -0.1214],\n",
      "        [ 0.2043,  0.9789],\n",
      "        [-0.9991, -0.0427]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.5856],\n",
      "        [ 5.6351],\n",
      "        [ 4.5036]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 333, Loss: 0.821305084821319\n",
      "tensor([[ 0.9947, -0.1030],\n",
      "        [ 0.2137,  0.9769],\n",
      "        [-0.9991, -0.0421]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.6034],\n",
      "        [ 5.6408],\n",
      "        [ 4.5090]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 334, Loss: 0.7752586019478092\n",
      "tensor([[ 0.9950, -0.0998],\n",
      "        [ 0.2274,  0.9738],\n",
      "        [-0.9992, -0.0412]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.6210],\n",
      "        [ 5.6440],\n",
      "        [ 4.5145]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 335, Loss: 0.7958844414878318\n",
      "tensor([[ 0.9943, -0.1069],\n",
      "        [ 0.2431,  0.9700],\n",
      "        [-0.9992, -0.0404]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.6392],\n",
      "        [ 5.6437],\n",
      "        [ 4.5201]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 336, Loss: 0.7805761699977499\n",
      "tensor([[ 0.9916, -0.1292],\n",
      "        [ 0.2632,  0.9647],\n",
      "        [-0.9992, -0.0397]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.6565],\n",
      "        [ 5.6374],\n",
      "        [ 4.5258]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 337, Loss: 0.8101269863353324\n",
      "tensor([[ 0.9880, -0.1547],\n",
      "        [ 0.2834,  0.9590],\n",
      "        [-0.9992, -0.0397]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.6730],\n",
      "        [ 5.6300],\n",
      "        [ 4.5316]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 338, Loss: 0.7860049008615927\n",
      "tensor([[ 0.9869, -0.1614],\n",
      "        [ 0.2944,  0.9557],\n",
      "        [-0.9992, -0.0396]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.6902],\n",
      "        [ 5.6234],\n",
      "        [ 4.5374]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 339, Loss: 0.7796612993704609\n",
      "tensor([[ 0.9884, -0.1516],\n",
      "        [ 0.2979,  0.9546],\n",
      "        [-0.9992, -0.0397]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.7061],\n",
      "        [ 5.6182],\n",
      "        [ 4.5433]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 340, Loss: 0.8001635391875282\n",
      "tensor([[ 0.9906, -0.1366],\n",
      "        [ 0.2940,  0.9558],\n",
      "        [-0.9992, -0.0401]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.7228],\n",
      "        [ 5.6124],\n",
      "        [ 4.5492]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 341, Loss: 0.7804728065552332\n",
      "tensor([[ 0.9917, -0.1286],\n",
      "        [ 0.2848,  0.9586],\n",
      "        [-0.9992, -0.0407]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.7388],\n",
      "        [ 5.6082],\n",
      "        [ 4.5550]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 342, Loss: 0.792648477493049\n",
      "tensor([[ 0.9936, -0.1127],\n",
      "        [ 0.2702,  0.9628],\n",
      "        [-0.9992, -0.0411]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.7545],\n",
      "        [ 5.6053],\n",
      "        [ 4.5608]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 343, Loss: 0.7884085463897221\n",
      "tensor([[ 0.9947, -0.1031],\n",
      "        [ 0.2534,  0.9674],\n",
      "        [-0.9991, -0.0415]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.7707],\n",
      "        [ 5.6017],\n",
      "        [ 4.5665]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 344, Loss: 0.785807208537692\n",
      "tensor([[ 0.9955, -0.0950],\n",
      "        [ 0.2309,  0.9730],\n",
      "        [-0.9991, -0.0416]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.7881],\n",
      "        [ 5.6007],\n",
      "        [ 4.5721]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 345, Loss: 0.750673913295758\n",
      "tensor([[ 0.9950, -0.0995],\n",
      "        [ 0.2110,  0.9775],\n",
      "        [-0.9991, -0.0416]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.8054],\n",
      "        [ 5.5977],\n",
      "        [ 4.5777]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 346, Loss: 0.7973392546694009\n",
      "tensor([[ 0.9945, -0.1051],\n",
      "        [ 0.1918,  0.9814],\n",
      "        [-0.9991, -0.0415]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.8230],\n",
      "        [ 5.5982],\n",
      "        [ 4.5833]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 347, Loss: 0.752749635009409\n",
      "tensor([[ 0.9938, -0.1111],\n",
      "        [ 0.1755,  0.9845],\n",
      "        [-0.9992, -0.0409]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.8400],\n",
      "        [ 5.6006],\n",
      "        [ 4.5889]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 348, Loss: 0.7769058523569471\n",
      "tensor([[ 0.9925, -0.1220],\n",
      "        [ 0.1643,  0.9864],\n",
      "        [-0.9992, -0.0406]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.8567],\n",
      "        [ 5.6027],\n",
      "        [ 4.5944]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 349, Loss: 0.7981470225516077\n",
      "tensor([[ 0.9900, -0.1408],\n",
      "        [ 0.1544,  0.9880],\n",
      "        [-0.9992, -0.0404]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.8732],\n",
      "        [ 5.6014],\n",
      "        [ 4.5999]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 350, Loss: 0.8066195038834278\n",
      "tensor([[ 0.9878, -0.1555],\n",
      "        [ 0.1552,  0.9879],\n",
      "        [-0.9992, -0.0399]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.8899],\n",
      "        [ 5.5957],\n",
      "        [ 4.6054]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 351, Loss: 0.8107585478640541\n",
      "tensor([[ 0.9874, -0.1585],\n",
      "        [ 0.1622,  0.9868],\n",
      "        [-0.9992, -0.0396]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.9069],\n",
      "        [ 5.5912],\n",
      "        [ 4.6109]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 352, Loss: 0.772510709786299\n",
      "tensor([[ 0.9878, -0.1558],\n",
      "        [ 0.1739,  0.9848],\n",
      "        [-0.9992, -0.0389]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.9243],\n",
      "        [ 5.5891],\n",
      "        [ 4.6165]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 353, Loss: 0.7644787341681942\n",
      "tensor([[ 0.9912, -0.1325],\n",
      "        [ 0.1794,  0.9838],\n",
      "        [-0.9993, -0.0385]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.9412],\n",
      "        [ 5.5893],\n",
      "        [ 4.6219]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 354, Loss: 0.7792843492939937\n",
      "tensor([[ 0.9938, -0.1114],\n",
      "        [ 0.1841,  0.9829],\n",
      "        [-0.9993, -0.0381]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.9591],\n",
      "        [ 5.5861],\n",
      "        [ 4.6272]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 355, Loss: 0.7826330999831734\n",
      "tensor([[ 0.9960, -0.0897],\n",
      "        [ 0.1888,  0.9820],\n",
      "        [-0.9993, -0.0373]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.9773],\n",
      "        [ 5.5832],\n",
      "        [ 4.6326]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 356, Loss: 0.7729057793286491\n",
      "tensor([[ 0.9959, -0.0905],\n",
      "        [ 0.1985,  0.9801],\n",
      "        [-0.9993, -0.0363]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[13.9942],\n",
      "        [ 5.5768],\n",
      "        [ 4.6380]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 357, Loss: 0.8155727425606571\n",
      "tensor([[ 0.9956, -0.0941],\n",
      "        [ 0.2115,  0.9774],\n",
      "        [-0.9994, -0.0355]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.0108],\n",
      "        [ 5.5684],\n",
      "        [ 4.6432]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 358, Loss: 0.8110737249389747\n",
      "tensor([[ 0.9941, -0.1083],\n",
      "        [ 0.2225,  0.9749],\n",
      "        [-0.9994, -0.0345]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.0283],\n",
      "        [ 5.5626],\n",
      "        [ 4.6485]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 359, Loss: 0.7665071987300789\n",
      "tensor([[ 0.9927, -0.1209],\n",
      "        [ 0.2262,  0.9741],\n",
      "        [-0.9994, -0.0341]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.0458],\n",
      "        [ 5.5582],\n",
      "        [ 4.6537]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 360, Loss: 0.7874579931417\n",
      "tensor([[ 0.9900, -0.1408],\n",
      "        [ 0.2281,  0.9736],\n",
      "        [-0.9994, -0.0333]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.0636],\n",
      "        [ 5.5579],\n",
      "        [ 4.6587]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 361, Loss: 0.7517681310280506\n",
      "tensor([[ 0.9871, -0.1599],\n",
      "        [ 0.2261,  0.9741],\n",
      "        [-0.9995, -0.0330]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.0816],\n",
      "        [ 5.5596],\n",
      "        [ 4.6639]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 362, Loss: 0.7553403838379206\n",
      "tensor([[ 0.9863, -0.1651],\n",
      "        [ 0.2244,  0.9745],\n",
      "        [-0.9995, -0.0330]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.1004],\n",
      "        [ 5.5611],\n",
      "        [ 4.6692]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 363, Loss: 0.7677456468909717\n",
      "tensor([[ 0.9857, -0.1683],\n",
      "        [ 0.2229,  0.9748],\n",
      "        [-0.9994, -0.0335]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.1186],\n",
      "        [ 5.5615],\n",
      "        [ 4.6745]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 364, Loss: 0.7962851021647923\n",
      "tensor([[ 0.9861, -0.1661],\n",
      "        [ 0.2220,  0.9750],\n",
      "        [-0.9994, -0.0339]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.1359],\n",
      "        [ 5.5634],\n",
      "        [ 4.6798]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 365, Loss: 0.7838184950850211\n",
      "tensor([[ 0.9880, -0.1543],\n",
      "        [ 0.2193,  0.9756],\n",
      "        [-0.9994, -0.0342]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.1520],\n",
      "        [ 5.5658],\n",
      "        [ 4.6852]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 366, Loss: 0.7875067085532622\n",
      "tensor([[ 0.9913, -0.1318],\n",
      "        [ 0.2151,  0.9766],\n",
      "        [-0.9994, -0.0349]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.1682],\n",
      "        [ 5.5705],\n",
      "        [ 4.6905]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 367, Loss: 0.7634142915009697\n",
      "tensor([[ 0.9950, -0.0998],\n",
      "        [ 0.2095,  0.9778],\n",
      "        [-0.9994, -0.0355]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.1843],\n",
      "        [ 5.5769],\n",
      "        [ 4.6959]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 368, Loss: 0.7676977041728161\n",
      "tensor([[ 0.9974, -0.0725],\n",
      "        [ 0.1982,  0.9802],\n",
      "        [-0.9993, -0.0361]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.2006],\n",
      "        [ 5.5859],\n",
      "        [ 4.7012]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 369, Loss: 0.7589007551198538\n",
      "tensor([[ 0.9986, -0.0538],\n",
      "        [ 0.1902,  0.9817],\n",
      "        [-0.9993, -0.0361]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.2174],\n",
      "        [ 5.5967],\n",
      "        [ 4.7066]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 370, Loss: 0.7664269204067915\n",
      "tensor([[ 0.9984, -0.0565],\n",
      "        [ 0.1861,  0.9825],\n",
      "        [-0.9994, -0.0359]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.2350],\n",
      "        [ 5.6083],\n",
      "        [ 4.7119]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 371, Loss: 0.7656125049073338\n",
      "tensor([[ 0.9960, -0.0890],\n",
      "        [ 0.1827,  0.9832],\n",
      "        [-0.9994, -0.0359]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.2526],\n",
      "        [ 5.6225],\n",
      "        [ 4.7173]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 372, Loss: 0.7488996124046078\n",
      "tensor([[ 0.9913, -0.1315],\n",
      "        [ 0.1830,  0.9831],\n",
      "        [-0.9993, -0.0364]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.2707],\n",
      "        [ 5.6313],\n",
      "        [ 4.7227]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 373, Loss: 0.777611435500851\n",
      "tensor([[ 0.9862, -0.1657],\n",
      "        [ 0.1777,  0.9841],\n",
      "        [-0.9993, -0.0373]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.2879],\n",
      "        [ 5.6429],\n",
      "        [ 4.7281]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 374, Loss: 0.7637136305485231\n",
      "tensor([[ 0.9838, -0.1795],\n",
      "        [ 0.1710,  0.9853],\n",
      "        [-0.9993, -0.0382]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.3035],\n",
      "        [ 5.6523],\n",
      "        [ 4.7335]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 375, Loss: 0.8053660319939782\n",
      "tensor([[ 0.9818, -0.1897],\n",
      "        [ 0.1727,  0.9850],\n",
      "        [-0.9992, -0.0391]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.3192],\n",
      "        [ 5.6593],\n",
      "        [ 4.7388]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 376, Loss: 0.7833506313541649\n",
      "tensor([[ 0.9808, -0.1950],\n",
      "        [ 0.1828,  0.9832],\n",
      "        [-0.9992, -0.0403]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.3345],\n",
      "        [ 5.6657],\n",
      "        [ 4.7441]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 377, Loss: 0.7732706661471951\n",
      "tensor([[ 0.9841, -0.1778],\n",
      "        [ 0.1962,  0.9806],\n",
      "        [-0.9992, -0.0409]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.3489],\n",
      "        [ 5.6723],\n",
      "        [ 4.7492]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 378, Loss: 0.8001086442759614\n",
      "tensor([[ 0.9901, -0.1405],\n",
      "        [ 0.2062,  0.9785],\n",
      "        [-0.9991, -0.0415]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.3629],\n",
      "        [ 5.6832],\n",
      "        [ 4.7544]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 379, Loss: 0.7528343375240013\n",
      "tensor([[ 0.9954, -0.0962],\n",
      "        [ 0.2143,  0.9768],\n",
      "        [-0.9991, -0.0417]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.3776],\n",
      "        [ 5.6909],\n",
      "        [ 4.7594]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 380, Loss: 0.7851745013783024\n",
      "tensor([[ 0.9981, -0.0611],\n",
      "        [ 0.2164,  0.9763],\n",
      "        [-0.9991, -0.0422]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.3929],\n",
      "        [ 5.6937],\n",
      "        [ 4.7644]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 381, Loss: 0.7880184881951524\n",
      "tensor([[ 0.9991, -0.0434],\n",
      "        [ 0.2133,  0.9770],\n",
      "        [-0.9991, -0.0426]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.4081],\n",
      "        [ 5.6953],\n",
      "        [ 4.7693]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 382, Loss: 0.795862896923921\n",
      "tensor([[ 0.9993, -0.0385],\n",
      "        [ 0.2068,  0.9784],\n",
      "        [-0.9991, -0.0432]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.4244],\n",
      "        [ 5.6977],\n",
      "        [ 4.7743]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 383, Loss: 0.7550246519715306\n",
      "tensor([[ 0.9980, -0.0630],\n",
      "        [ 0.2041,  0.9789],\n",
      "        [-0.9991, -0.0434]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.4410],\n",
      "        [ 5.7028],\n",
      "        [ 4.7792]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 384, Loss: 0.7647571505493183\n",
      "tensor([[ 0.9952, -0.0974],\n",
      "        [ 0.2114,  0.9774],\n",
      "        [-0.9991, -0.0434]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.4582],\n",
      "        [ 5.7033],\n",
      "        [ 4.7841]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 385, Loss: 0.7829921175841568\n",
      "tensor([[ 0.9914, -0.1311],\n",
      "        [ 0.2230,  0.9748],\n",
      "        [-0.9991, -0.0426]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.4769],\n",
      "        [ 5.7074],\n",
      "        [ 4.7891]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 386, Loss: 0.732636301546432\n",
      "tensor([[ 0.9860, -0.1667],\n",
      "        [ 0.2352,  0.9719],\n",
      "        [-0.9991, -0.0418]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.4959],\n",
      "        [ 5.7109],\n",
      "        [ 4.7941]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 387, Loss: 0.7621924121427202\n",
      "tensor([[ 0.9828, -0.1845],\n",
      "        [ 0.2455,  0.9694],\n",
      "        [-0.9992, -0.0409]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.5150],\n",
      "        [ 5.7168],\n",
      "        [ 4.7991]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 388, Loss: 0.7485186592534254\n",
      "tensor([[ 0.9837, -0.1797],\n",
      "        [ 0.2495,  0.9684],\n",
      "        [-0.9992, -0.0402]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.5331],\n",
      "        [ 5.7214],\n",
      "        [ 4.8040]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 389, Loss: 0.7938101278429335\n",
      "tensor([[ 0.9860, -0.1669],\n",
      "        [ 0.2497,  0.9683],\n",
      "        [-0.9992, -0.0395]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.5508],\n",
      "        [ 5.7266],\n",
      "        [ 4.8090]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 390, Loss: 0.7651504835468348\n",
      "tensor([[ 0.9898, -0.1427],\n",
      "        [ 0.2488,  0.9686],\n",
      "        [-0.9993, -0.0382]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.5684],\n",
      "        [ 5.7366],\n",
      "        [ 4.8139]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 391, Loss: 0.7460247984423377\n",
      "tensor([[ 0.9929, -0.1187],\n",
      "        [ 0.2426,  0.9701],\n",
      "        [-0.9993, -0.0371]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.5863],\n",
      "        [ 5.7478],\n",
      "        [ 4.8188]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 392, Loss: 0.7528191653941271\n",
      "tensor([[ 0.9938, -0.1110],\n",
      "        [ 0.2367,  0.9716],\n",
      "        [-0.9994, -0.0360]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.6040],\n",
      "        [ 5.7554],\n",
      "        [ 4.8236]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 393, Loss: 0.7937697850658846\n",
      "tensor([[ 0.9949, -0.1009],\n",
      "        [ 0.2304,  0.9731],\n",
      "        [-0.9994, -0.0352]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.6218],\n",
      "        [ 5.7631],\n",
      "        [ 4.8284]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 394, Loss: 0.7730691388261807\n",
      "tensor([[ 0.9963, -0.0864],\n",
      "        [ 0.2187,  0.9758],\n",
      "        [-0.9994, -0.0340]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.6402],\n",
      "        [ 5.7737],\n",
      "        [ 4.8333]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 395, Loss: 0.7470016240163888\n",
      "tensor([[ 0.9964, -0.0853],\n",
      "        [ 0.2111,  0.9775],\n",
      "        [-0.9994, -0.0332]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.6581],\n",
      "        [ 5.7758],\n",
      "        [ 4.8381]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 396, Loss: 0.8245810038279356\n",
      "tensor([[ 0.9948, -0.1017],\n",
      "        [ 0.2137,  0.9769],\n",
      "        [-0.9995, -0.0323]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.6753],\n",
      "        [ 5.7758],\n",
      "        [ 4.8431]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 397, Loss: 0.7808120332970757\n",
      "tensor([[ 0.9920, -0.1261],\n",
      "        [ 0.2183,  0.9759],\n",
      "        [-0.9995, -0.0322]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.6926],\n",
      "        [ 5.7742],\n",
      "        [ 4.8481]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 398, Loss: 0.7774258210843442\n",
      "tensor([[ 0.9885, -0.1511],\n",
      "        [ 0.2144,  0.9768],\n",
      "        [-0.9995, -0.0323]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.7086],\n",
      "        [ 5.7729],\n",
      "        [ 4.8530]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 399, Loss: 0.7960516499124562\n",
      "tensor([[ 0.9865, -0.1640],\n",
      "        [ 0.2115,  0.9774],\n",
      "        [-0.9995, -0.0320]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.7251],\n",
      "        [ 5.7699],\n",
      "        [ 4.8578]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 400, Loss: 0.7794266170065343\n",
      "tensor([[ 0.9846, -0.1746],\n",
      "        [ 0.2153,  0.9766],\n",
      "        [-0.9995, -0.0316]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.7414],\n",
      "        [ 5.7681],\n",
      "        [ 4.8626]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 401, Loss: 0.7785424882049338\n",
      "tensor([[ 0.9857, -0.1686],\n",
      "        [ 0.2137,  0.9769],\n",
      "        [-0.9995, -0.0317]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.7566],\n",
      "        [ 5.7671],\n",
      "        [ 4.8673]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 402, Loss: 0.7886546157196852\n",
      "tensor([[ 0.9866, -0.1634],\n",
      "        [ 0.2149,  0.9766],\n",
      "        [-0.9995, -0.0318]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.7720],\n",
      "        [ 5.7650],\n",
      "        [ 4.8719]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 403, Loss: 0.7720744198933882\n",
      "tensor([[ 0.9881, -0.1537],\n",
      "        [ 0.2208,  0.9753],\n",
      "        [-0.9995, -0.0323]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.7880],\n",
      "        [ 5.7635],\n",
      "        [ 4.8767]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 404, Loss: 0.7531775676769026\n",
      "tensor([[ 0.9918, -0.1279],\n",
      "        [ 0.2231,  0.9748],\n",
      "        [-0.9995, -0.0325]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.8034],\n",
      "        [ 5.7647],\n",
      "        [ 4.8815]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 405, Loss: 0.7684035910801175\n",
      "tensor([[ 0.9946, -0.1039],\n",
      "        [ 0.2260,  0.9741],\n",
      "        [-0.9995, -0.0325]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.8196],\n",
      "        [ 5.7658],\n",
      "        [ 4.8863]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 406, Loss: 0.7690172930645814\n",
      "tensor([[ 0.9963, -0.0865],\n",
      "        [ 0.2256,  0.9742],\n",
      "        [-0.9995, -0.0326]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.8364],\n",
      "        [ 5.7668],\n",
      "        [ 4.8910]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 407, Loss: 0.7790752762897706\n",
      "tensor([[ 0.9960, -0.0897],\n",
      "        [ 0.2253,  0.9743],\n",
      "        [-0.9995, -0.0328]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.8533],\n",
      "        [ 5.7654],\n",
      "        [ 4.8957]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 408, Loss: 0.7761499963056284\n",
      "tensor([[ 0.9946, -0.1036],\n",
      "        [ 0.2222,  0.9750],\n",
      "        [-0.9995, -0.0329]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.8702],\n",
      "        [ 5.7676],\n",
      "        [ 4.9004]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 409, Loss: 0.7545247768632892\n",
      "tensor([[ 0.9920, -0.1266],\n",
      "        [ 0.2237,  0.9747],\n",
      "        [-0.9994, -0.0332]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.8861],\n",
      "        [ 5.7622],\n",
      "        [ 4.9052]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 410, Loss: 0.8316481207232671\n",
      "tensor([[ 0.9887, -0.1497],\n",
      "        [ 0.2251,  0.9743],\n",
      "        [-0.9994, -0.0336]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.9020],\n",
      "        [ 5.7522],\n",
      "        [ 4.9098]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 411, Loss: 0.8173105035965392\n",
      "tensor([[ 0.9858, -0.1677],\n",
      "        [ 0.2229,  0.9748],\n",
      "        [-0.9994, -0.0340]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.9179],\n",
      "        [ 5.7398],\n",
      "        [ 4.9145]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 412, Loss: 0.7864786491395579\n",
      "tensor([[ 0.9833, -0.1819],\n",
      "        [ 0.2231,  0.9748],\n",
      "        [-0.9994, -0.0341]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.9332],\n",
      "        [ 5.7262],\n",
      "        [ 4.9191]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 413, Loss: 0.7969972423025302\n",
      "tensor([[ 0.9811, -0.1933],\n",
      "        [ 0.2247,  0.9744],\n",
      "        [-0.9994, -0.0340]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.9490],\n",
      "        [ 5.7127],\n",
      "        [ 4.9239]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 414, Loss: 0.7607881117853954\n",
      "tensor([[ 0.9828, -0.1849],\n",
      "        [ 0.2270,  0.9739],\n",
      "        [-0.9994, -0.0342]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.9639],\n",
      "        [ 5.6963],\n",
      "        [ 4.9285]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 415, Loss: 0.808371371602328\n",
      "tensor([[ 0.9873, -0.1589],\n",
      "        [ 0.2223,  0.9750],\n",
      "        [-0.9994, -0.0342]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.9790],\n",
      "        [ 5.6805],\n",
      "        [ 4.9331]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 416, Loss: 0.7793738303797987\n",
      "tensor([[ 0.9916, -0.1295],\n",
      "        [ 0.2143,  0.9768],\n",
      "        [-0.9994, -0.0339]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[14.9945],\n",
      "        [ 5.6691],\n",
      "        [ 4.9375]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 417, Loss: 0.7645487919740781\n",
      "tensor([[ 0.9945, -0.1049],\n",
      "        [ 0.2058,  0.9786],\n",
      "        [-0.9994, -0.0336]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.0095],\n",
      "        [ 5.6570],\n",
      "        [ 4.9420]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 418, Loss: 0.7974496281964535\n",
      "tensor([[ 0.9959, -0.0909],\n",
      "        [ 0.2009,  0.9796],\n",
      "        [-0.9994, -0.0332]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.0254],\n",
      "        [ 5.6485],\n",
      "        [ 4.9465]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 419, Loss: 0.7433036543210917\n",
      "tensor([[ 0.9954, -0.0961],\n",
      "        [ 0.2051,  0.9787],\n",
      "        [-0.9995, -0.0327]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.0406],\n",
      "        [ 5.6368],\n",
      "        [ 4.9509]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 420, Loss: 0.8106770275577799\n",
      "tensor([[ 0.9934, -0.1144],\n",
      "        [ 0.2103,  0.9776],\n",
      "        [-0.9995, -0.0314]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.0561],\n",
      "        [ 5.6297],\n",
      "        [ 4.9554]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 421, Loss: 0.7458231916962641\n",
      "tensor([[ 0.9907, -0.1359],\n",
      "        [ 0.2131,  0.9770],\n",
      "        [-0.9995, -0.0300]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.0723],\n",
      "        [ 5.6236],\n",
      "        [ 4.9598]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 422, Loss: 0.7648276077838951\n",
      "tensor([[ 0.9871, -0.1599],\n",
      "        [ 0.2167,  0.9762],\n",
      "        [-0.9996, -0.0290]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.0880],\n",
      "        [ 5.6117],\n",
      "        [ 4.9642]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 423, Loss: 0.8102601808950286\n",
      "tensor([[ 0.9855, -0.1695],\n",
      "        [ 0.2185,  0.9758],\n",
      "        [-0.9996, -0.0279]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.1044],\n",
      "        [ 5.6022],\n",
      "        [ 4.9687]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 424, Loss: 0.7502603110138236\n",
      "tensor([[ 0.9872, -0.1594],\n",
      "        [ 0.2203,  0.9754],\n",
      "        [-0.9996, -0.0266]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.1207],\n",
      "        [ 5.5982],\n",
      "        [ 4.9732]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 425, Loss: 0.7626788411280542\n",
      "tensor([[ 0.9904, -0.1381],\n",
      "        [ 0.2205,  0.9754],\n",
      "        [-0.9997, -0.0252]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.1364],\n",
      "        [ 5.5946],\n",
      "        [ 4.9777]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 426, Loss: 0.7848014320211626\n",
      "tensor([[ 0.9930, -0.1184],\n",
      "        [ 0.2209,  0.9753],\n",
      "        [-0.9997, -0.0242]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.1526],\n",
      "        [ 5.5927],\n",
      "        [ 4.9822]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 427, Loss: 0.7522278106299036\n",
      "tensor([[ 0.9950, -0.0995],\n",
      "        [ 0.2157,  0.9765],\n",
      "        [-0.9997, -0.0227]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.1683],\n",
      "        [ 5.5960],\n",
      "        [ 4.9868]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 428, Loss: 0.7486605415741802\n",
      "tensor([[ 0.9960, -0.0898],\n",
      "        [ 0.2101,  0.9777],\n",
      "        [-0.9998, -0.0211]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.1837],\n",
      "        [ 5.5996],\n",
      "        [ 4.9913]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 429, Loss: 0.7753802307289848\n",
      "tensor([[ 0.9950, -0.1000],\n",
      "        [ 0.2034,  0.9791],\n",
      "        [-0.9998, -0.0203]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.1988],\n",
      "        [ 5.6042],\n",
      "        [ 4.9959]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 430, Loss: 0.7641286505873012\n",
      "tensor([[ 0.9934, -0.1151],\n",
      "        [ 0.1983,  0.9802],\n",
      "        [-0.9998, -0.0198]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.2144],\n",
      "        [ 5.6111],\n",
      "        [ 5.0004]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 431, Loss: 0.7608518376005368\n",
      "tensor([[ 0.9913, -0.1315],\n",
      "        [ 0.1950,  0.9808],\n",
      "        [-0.9998, -0.0197]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.2305],\n",
      "        [ 5.6207],\n",
      "        [ 5.0051]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 432, Loss: 0.7471784283324288\n",
      "tensor([[ 0.9888, -0.1489],\n",
      "        [ 0.2060,  0.9786],\n",
      "        [-0.9998, -0.0202]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.2460],\n",
      "        [ 5.6207],\n",
      "        [ 5.0099]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 433, Loss: 0.8066310766911277\n",
      "tensor([[ 0.9880, -0.1547],\n",
      "        [ 0.2185,  0.9758],\n",
      "        [-0.9998, -0.0206]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.2618],\n",
      "        [ 5.6210],\n",
      "        [ 5.0145]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 434, Loss: 0.7758532671945496\n",
      "tensor([[ 0.9881, -0.1537],\n",
      "        [ 0.2268,  0.9739],\n",
      "        [-0.9998, -0.0213]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.2774],\n",
      "        [ 5.6240],\n",
      "        [ 5.0192]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 435, Loss: 0.7704067821750944\n",
      "tensor([[ 0.9887, -0.1496],\n",
      "        [ 0.2369,  0.9715],\n",
      "        [-0.9998, -0.0216]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.2927],\n",
      "        [ 5.6316],\n",
      "        [ 5.0238]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 436, Loss: 0.7545830374880883\n",
      "tensor([[ 0.9912, -0.1320],\n",
      "        [ 0.2371,  0.9715],\n",
      "        [-0.9998, -0.0220]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.3093],\n",
      "        [ 5.6417],\n",
      "        [ 5.0283]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 437, Loss: 0.749243686595468\n",
      "tensor([[ 0.9927, -0.1208],\n",
      "        [ 0.2363,  0.9717],\n",
      "        [-0.9997, -0.0228]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.3255],\n",
      "        [ 5.6513],\n",
      "        [ 5.0328]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 438, Loss: 0.7727700547913968\n",
      "tensor([[ 0.9928, -0.1199],\n",
      "        [ 0.2272,  0.9739],\n",
      "        [-0.9997, -0.0234]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.3414],\n",
      "        [ 5.6588],\n",
      "        [ 5.0371]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 439, Loss: 0.7907829008651239\n",
      "tensor([[ 0.9918, -0.1278],\n",
      "        [ 0.2133,  0.9770],\n",
      "        [-0.9997, -0.0242]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.3574],\n",
      "        [ 5.6661],\n",
      "        [ 5.0416]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 440, Loss: 0.7598256946053867\n",
      "tensor([[ 0.9915, -0.1305],\n",
      "        [ 0.2016,  0.9795],\n",
      "        [-0.9997, -0.0255]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.3727],\n",
      "        [ 5.6704],\n",
      "        [ 5.0460]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 441, Loss: 0.797645204182605\n",
      "tensor([[ 0.9892, -0.1463],\n",
      "        [ 0.1957,  0.9807],\n",
      "        [-0.9997, -0.0262]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.3877],\n",
      "        [ 5.6725],\n",
      "        [ 5.0503]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 442, Loss: 0.7881754129880597\n",
      "tensor([[ 0.9892, -0.1466],\n",
      "        [ 0.1926,  0.9813],\n",
      "        [-0.9996, -0.0267]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.4025],\n",
      "        [ 5.6758],\n",
      "        [ 5.0546]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 443, Loss: 0.7777305489533854\n",
      "tensor([[ 0.9891, -0.1475],\n",
      "        [ 0.1967,  0.9805],\n",
      "        [-0.9996, -0.0279]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.4178],\n",
      "        [ 5.6762],\n",
      "        [ 5.0589]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 444, Loss: 0.7649557592078124\n",
      "tensor([[ 0.9875, -0.1575],\n",
      "        [ 0.2083,  0.9781],\n",
      "        [-0.9996, -0.0294]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.4337],\n",
      "        [ 5.6701],\n",
      "        [ 5.0632]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 445, Loss: 0.7902217088691662\n",
      "tensor([[ 0.9861, -0.1664],\n",
      "        [ 0.2218,  0.9751],\n",
      "        [-0.9995, -0.0306]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.4501],\n",
      "        [ 5.6621],\n",
      "        [ 5.0673]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 446, Loss: 0.7793888752710462\n",
      "tensor([[ 0.9853, -0.1711],\n",
      "        [ 0.2419,  0.9703],\n",
      "        [-0.9995, -0.0315]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.4667],\n",
      "        [ 5.6557],\n",
      "        [ 5.0714]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 447, Loss: 0.7665951113230411\n",
      "tensor([[ 0.9874, -0.1581],\n",
      "        [ 0.2535,  0.9673],\n",
      "        [-0.9995, -0.0324]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.4834],\n",
      "        [ 5.6474],\n",
      "        [ 5.0756]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 448, Loss: 0.7836863515364402\n",
      "tensor([[ 0.9901, -0.1400],\n",
      "        [ 0.2553,  0.9669],\n",
      "        [-0.9994, -0.0332]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[15.5001],\n",
      "        [ 5.6325],\n",
      "        [ 5.0795]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 449, Loss: 0.8050283601700462\n"
     ]
    }
   ],
   "source": [
    "import geotorch\n",
    "\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "\n",
    "class GalleryParams(torch.nn.Module):\n",
    "    def __init__(self, init_mean, init_kappa):\n",
    "        super(GalleryParams, self).__init__()\n",
    "        self.gallery_means = torch.nn.Parameter(torch.tensor(init_mean))\n",
    "        # self.gallery_kappas =  torch.nn.Parameter(torch.tensor(init_kappa))\n",
    "        # self.gallery_means = torch.nn.Parameter(torch.rand(3, 2, dtype=torch.float64))\n",
    "        self.gallery_kappas = torch.nn.Parameter(\n",
    "            torch.rand(3, 1, dtype=torch.float64) * 10\n",
    "        )\n",
    "\n",
    "\n",
    "gallery_params = GalleryParams(init_mean, init_kappa)\n",
    "target_class = torch.tensor(gallery_subject_ids_sorted)\n",
    "T = torch.nn.Parameter(torch.tensor(1.0))\n",
    "geotorch.sphere(gallery_params, \"gallery_means\")\n",
    "\n",
    "\n",
    "# train\n",
    "M = 10\n",
    "mc_prob = MonteCarloPredictiveProb(M=M)\n",
    "\n",
    "# optimizer = torch.optim.Adam([gallery_means, gallery_kappas], lr=10e-3)\n",
    "optimizer = torch.optim.Adam(gallery_params.parameters(), lr=0.1)\n",
    "\n",
    "num_steps = 450\n",
    "\n",
    "for iter in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    # compute nll loss\n",
    "    log_probs = mc_prob(\n",
    "        gallery_features,\n",
    "        gallery_unc,\n",
    "        gallery_params.gallery_means,\n",
    "        gallery_params.gallery_kappas,\n",
    "        T,\n",
    "    )[:, :, :-1]\n",
    "    probs = torch.exp(log_probs)\n",
    "    mean_probs = torch.mean(probs, axis=1)\n",
    "    log_probs_new = torch.log(mean_probs)\n",
    "    loss = nll_loss(log_probs_new, target_class)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(gallery_params.gallery_means)\n",
    "    print(torch.norm(gallery_params.gallery_means, dim=-1))\n",
    "    print(gallery_params.gallery_kappas)\n",
    "    print(f\"Iteration {iter}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fontsize = 20\n",
    "linewidth = 3\n",
    "dot_size = 80\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "draw_circle(ax, linewidth)\n",
    "for class_id, (class_mean, kappa) in enumerate(\n",
    "    zip(gallery_params.gallery_means, gallery_params.gallery_kappas)\n",
    "):\n",
    "    color = colors[class_id]\n",
    "    class_mean = class_mean.detach().numpy()\n",
    "    kappa = kappa.detach().numpy()\n",
    "    class_point_angle = np.angle([class_mean[0] + 1j * class_mean[1]])[0]\n",
    "    draw_dencity(\n",
    "        class_point_angle,\n",
    "        kappa,\n",
    "        ax,\n",
    "        linewidth=3,\n",
    "        color=color,\n",
    "        range=np.pi / 2,\n",
    "        draw_center=True,\n",
    "        dot_size=dot_size,\n",
    "        type=\"power\",\n",
    "    )\n",
    "    for position in np.where(gallery_subject_ids_sorted == class_id)[0]:\n",
    "        point_angle = np.angle(\n",
    "            [gallery_features[position][0] + 1j * gallery_features[position][1]]\n",
    "        )[0]\n",
    "        draw_dencity(\n",
    "            point_angle,\n",
    "            gallery_unc[position],\n",
    "            ax,\n",
    "            linewidth=1,\n",
    "            color=color,\n",
    "            range=np.pi / 2,\n",
    "            scale=0.1,\n",
    "            draw_center=True,\n",
    "            dot_size=20,\n",
    "        )\n",
    "fig.gca().set_aspect(\"equal\")\n",
    "fig.show()\n",
    "plt.savefig(\"/app/outputs/images/trained.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Probability estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_angles = np.array([np.pi / 3])\n",
    "test_vectors = get_vectors_by_angle(test_angles)\n",
    "test_kappa = np.array([[10]])\n",
    "M = 100\n",
    "mc_prob = MonteCarloPredictiveProb(M=M)\n",
    "\n",
    "log_probs = mc_prob(\n",
    "    test_vectors,\n",
    "    test_kappa,\n",
    "    gallery_params.gallery_means,\n",
    "    gallery_params.gallery_kappas,\n",
    "    T,\n",
    ")  # [:, :, :-1]\n",
    "probs = torch.exp(log_probs)\n",
    "mean_probs = torch.mean(probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0491, 0.4923, 0.0034, 0.4552]], dtype=torch.float64,\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fontsize = 20\n",
    "linewidth = 3\n",
    "dot_size = 80\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "draw_circle(ax, linewidth)\n",
    "draw_dencity(\n",
    "    test_angles[0],\n",
    "    test_kappa[0],\n",
    "    ax,\n",
    "    linewidth=1,\n",
    "    color=\"tab:cyan\",\n",
    "    range=np.pi / 2,\n",
    "    scale=1,\n",
    "    draw_center=True,\n",
    "    dot_size=20,\n",
    ")\n",
    "for class_id, (class_mean, kappa) in enumerate(\n",
    "    zip(gallery_params.gallery_means, gallery_params.gallery_kappas)\n",
    "):\n",
    "    color = colors[class_id]\n",
    "    class_mean = class_mean.detach().numpy()\n",
    "    kappa = kappa.detach().numpy()\n",
    "    class_point_angle = np.angle([class_mean[0] + 1j * class_mean[1]])[0]\n",
    "    draw_dencity(\n",
    "        class_point_angle,\n",
    "        kappa,\n",
    "        ax,\n",
    "        linewidth=3,\n",
    "        color=color,\n",
    "        range=np.pi / 2,\n",
    "        draw_center=True,\n",
    "        dot_size=dot_size,\n",
    "        type=\"power\",\n",
    "    )\n",
    "fig.gca().set_aspect(\"equal\")\n",
    "fig.show()\n",
    "plt.savefig(\"/app/outputs/images/test.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tab:blue', 'tab:orange', 'tab:green']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
