{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "        p(\\textbf{z}|c) &= \\mathcal{C}_d(\\kappa)\\exp\\left(\\kappa\\mu^T_{c}\\textbf{z}\\right)\\\\\n",
    "        \\mathcal{C}_d(\\kappa) &= \\frac{(\\kappa)^{d/2-1}}{(2\\pi)^{d/2}\\mathcal{I}_{d/2-1}(\\kappa)}\n",
    "\\end{align}\n",
    "\n",
    "With d=2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import iv, gamma\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# mpl.style.use('classic')\n",
    "\n",
    "\n",
    "def z_Prob(z, mus, kappa, d=2, beta=0.5):\n",
    "    K = mus.shape[-1]\n",
    "    p_c = (1 - beta) / K\n",
    "    class_probs = np.array([z_vonMises_dencity(z, mu, kappa) for mu in mus.T])\n",
    "    return np.sum(class_probs * p_c) + (1 / (2 * np.pi)) * beta\n",
    "\n",
    "\n",
    "def z_class_prob(class_id, z, mus, kappa, d=2, beta=0.5):\n",
    "    p_z = z_Prob(z, mus, kappa, d, beta)\n",
    "    K = mus.shape[1]\n",
    "    if class_id == K:\n",
    "        p_c = beta\n",
    "        return (1 / (2 * np.pi)) * p_c / p_z\n",
    "    else:\n",
    "        p_c = (1 - beta) / K\n",
    "        return (z_vonMises_dencity(z, mus[:, class_id], kappa) * p_c) / p_z\n",
    "\n",
    "\n",
    "def z_vonMises_dencity(z, mu_c, kappa, d=2):\n",
    "    C_d = kappa ** (d / 2 - 1) / ((2 * np.pi) ** (d / 2) * iv(d / 2 - 1, kappa))\n",
    "    return C_d * np.exp(kappa * np.dot(z, mu_c))\n",
    "\n",
    "\n",
    "def z_power_dencity(z, mu_c, kappa, d=2):\n",
    "    alpha = (d - 1) / 2 + kappa\n",
    "    beta = (d - 1) / 2\n",
    "    M_d = gamma(alpha + beta) / (2 ** (alpha + beta) * np.pi**beta * gamma(alpha))\n",
    "    return M_d * (1 + np.dot(z, mu_c)) ** kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_by_angle(angles):\n",
    "    return np.array([[np.cos(plot_angle), np.sin(plot_angle)] for plot_angle in angles])\n",
    "\n",
    "\n",
    "def compute_class_probs(class_id, zs, mus, kappa, beta):\n",
    "    class_probes = []\n",
    "    for z in zs:\n",
    "        class_prob = z_class_prob(class_id, z, mus, kappa, beta=beta)\n",
    "        class_probes.append(class_prob)\n",
    "    class_probes = np.array(class_probes)\n",
    "    return class_probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_circle(ax, linewidth, zorder=4):\n",
    "    # plot circle\n",
    "    theta = np.linspace(0, 2 * np.pi, 150)\n",
    "    a = np.cos(theta)\n",
    "    b = np.sin(theta)\n",
    "    circle = plt.Circle((0, 0), 1, color=\"blue\", zorder=4, alpha=0.1)\n",
    "    ax.add_patch(circle)\n",
    "    ax.plot(a, b, color=\"tab:gray\", zorder=4, linewidth=linewidth)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "\n",
    "def draw_decity(ax):\n",
    "    pass\n",
    "\n",
    "\n",
    "def draw_example(kappa, gallery_class_angles, text_shift, save_name, beta=0.5):\n",
    "    fontsize = 20\n",
    "    linewidth = 3\n",
    "    dot_size = 80\n",
    "\n",
    "    test_color = \"tab:cyan\"\n",
    "    # gallery_class_angles = [0.4, 0.25]\n",
    "    ident_uncertain_test_point = (\n",
    "        gallery_class_angles[-1] + gallery_class_angles[-2]\n",
    "    ) / 2 - 0.02\n",
    "    test_points_angles = [\n",
    "        ident_uncertain_test_point,\n",
    "        gallery_class_angles[-1]\n",
    "        - (ident_uncertain_test_point - gallery_class_angles[-1]),\n",
    "    ]\n",
    "\n",
    "    gallery_class_angles = np.array(gallery_class_angles) * 2 * np.pi\n",
    "    test_points_angles = np.array(test_points_angles) * 2 * np.pi\n",
    "    theta = np.linspace(0, 2 * np.pi, 150)\n",
    "\n",
    "    colors = list(mcolors.TABLEAU_COLORS)[: len(gallery_class_angles)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    draw_circle(ax, linewidth)\n",
    "\n",
    "    draw_dencity_angles = np.linspace(-np.pi / 3, np.pi / 3, 150)\n",
    "    mus = np.stack([np.cos(gallery_class_angles), np.sin(gallery_class_angles)], axis=0)\n",
    "    class_to_class_probs = []\n",
    "\n",
    "    for i, (angle, color) in enumerate(zip(gallery_class_angles, colors)):\n",
    "        mu_c = mus[:, i]\n",
    "        plot_angles = angle + draw_dencity_angles\n",
    "        zs = get_vectors_by_angle(plot_angles)\n",
    "        class_probes = compute_class_probs(i, zs, mus, kappa, beta)\n",
    "        v = zs.T * (1 + class_probes[np.newaxis, :])\n",
    "        ax.plot(v[0], v[1], c=color, linewidth=linewidth)\n",
    "\n",
    "        ax.scatter([np.cos(angle)], [np.sin(angle)], c=color, s=dot_size, zorder=5)\n",
    "        ax.scatter([0], [0], color=\"black\", s=20)\n",
    "\n",
    "        # plot decity\n",
    "        # ax.scatter(points[:, 0], points[:, 1], color=color, s=3)\n",
    "\n",
    "    # plot_uniform_prob\n",
    "    zs = get_vectors_by_angle(theta)\n",
    "    class_probes = compute_class_probs(mus.shape[1], zs, mus, kappa, beta)\n",
    "    v = zs.T * (1 + class_probes[np.newaxis, :])\n",
    "    # ax.plot(v[0], v[1], color='black')\n",
    "\n",
    "    # plot unc\n",
    "    all_probs = []\n",
    "    for i in range(mus.shape[1] + 1):\n",
    "        class_probes = compute_class_probs(i, zs, mus, kappa, beta)\n",
    "        all_probs.append(class_probes)\n",
    "    all_probs = np.stack(all_probs, axis=0)\n",
    "    unc = -np.sum(all_probs * np.log(all_probs), axis=0)\n",
    "    # unc = -np.max(all_probs, axis=0) + 1\n",
    "    v = zs.T * (1 + unc[np.newaxis, :])\n",
    "    ax.plot(v[0], v[1], color=\"tab:red\", linewidth=linewidth)\n",
    "\n",
    "    # plot test points\n",
    "    for test_angle in test_points_angles:\n",
    "        ax.scatter(\n",
    "            [np.cos(test_angle)],\n",
    "            [np.sin(test_angle)],\n",
    "            c=test_color,\n",
    "            s=dot_size,\n",
    "            zorder=5,\n",
    "        )\n",
    "\n",
    "    test_point_vectors = get_vectors_by_angle(test_points_angles)\n",
    "    # entropy value\n",
    "\n",
    "    probs_at_test_points = []\n",
    "    for i in range(mus.shape[1] + 1):\n",
    "        class_probes = compute_class_probs(i, test_point_vectors, mus, kappa, beta)\n",
    "        probs_at_test_points.append(class_probes)\n",
    "    probs_at_test_points = np.stack(probs_at_test_points, axis=0)\n",
    "\n",
    "    unc_test = -np.sum(probs_at_test_points * np.log(probs_at_test_points), axis=0)\n",
    "    unc_test = np.round(unc_test, 2)\n",
    "    # unc_test = -np.max(probs_at_test_points, axis=0) + 1\n",
    "    # unc_test = np.round(unc_test, 2)\n",
    "    ax.annotate(\n",
    "        f\"${unc_test[0]}$\",\n",
    "        xy=test_point_vectors[0],\n",
    "        xytext=[\n",
    "            test_point_vectors[0][0] + text_shift[0][0],\n",
    "            test_point_vectors[0][1] + text_shift[0][1],\n",
    "        ],\n",
    "        fontsize=fontsize,\n",
    "    )\n",
    "    ax.annotate(\n",
    "        f\"${unc_test[1]}$\",\n",
    "        xy=test_point_vectors[1],\n",
    "        xytext=[\n",
    "            test_point_vectors[1][0] + text_shift[1][0],\n",
    "            test_point_vectors[1][1] + text_shift[1][1],\n",
    "        ],\n",
    "        fontsize=fontsize,\n",
    "    )\n",
    "    fig.gca().set_aspect(\"equal\")\n",
    "    plt.savefig(save_name, dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_example(\n",
    "#     kappa=15,\n",
    "#     gallery_class_angles=[0.4, 0.25],\n",
    "#     text_shift=[[-0.2, -0.3], [-0.2, 0.1]],\n",
    "#     save_name=\"test.png\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Identification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_example(kappa = 15, gallery_class_angles = [0.4, 0.25], text_shift = [[-0.2, -0.3], [-0.2, 0.1]], save_name='/app/paper_assets/images/false_ident_example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False accept/reject example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_example(kappa = 13, gallery_class_angles = [0.48, 0.25], text_shift = [[-0.1, -0.3], [-0.35, 0.2]], save_name='/app/paper_assets/images/false_accept-reject_example.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_dencity(\n",
    "    angle,\n",
    "    kappa,\n",
    "    ax,\n",
    "    linewidth,\n",
    "    color,\n",
    "    range=np.pi / 3,\n",
    "    scale=1,\n",
    "    draw_center=False,\n",
    "    dot_size=40,\n",
    "    type=\"vMF\",\n",
    "):\n",
    "    assert type in [\"vMF\", \"power\"]\n",
    "    draw_dencity_angles = np.linspace(-range, range, 150)\n",
    "    plot_angles = angle + draw_dencity_angles\n",
    "    zs = get_vectors_by_angle(plot_angles)\n",
    "    mu = get_vectors_by_angle([angle])[0]\n",
    "    if type == \"vMF\":\n",
    "        dencities = z_vonMises_dencity(zs, mu, kappa)\n",
    "    else:\n",
    "        dencities = z_power_dencity(zs, mu, kappa)\n",
    "    v = zs.T * (1 + dencities * scale)\n",
    "    ax.plot(v[0], v[1], c=color, linewidth=linewidth)\n",
    "    if draw_center:\n",
    "        ax.scatter([np.cos(angle)], [np.sin(angle)], c=color, s=dot_size, zorder=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.samplers import random_VMF, VonMisesFisher\n",
    "\n",
    "gallery_features = []  # N gallery samples X 512\n",
    "gallery_unc_log = []  # N gallery samples X 1\n",
    "gallery_subject_ids_sorted = []  # N gallery samples\n",
    "\n",
    "\n",
    "class_center_angles = np.array([0, np.pi / 2, np.pi])\n",
    "class_z_kappa = np.array([9, 6, 5])\n",
    "colors = list(mcolors.TABLEAU_COLORS)[: len(class_center_angles)]\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(2)\n",
    "\n",
    "for class_id, (angle, kappa) in enumerate(zip(class_center_angles, class_z_kappa)):\n",
    "    num_samples = rng.integers(5, 8)\n",
    "    gallery_subject_ids_sorted.extend([class_id] * num_samples)\n",
    "    gallery_unc_log.extend(4 * rng.random(num_samples) + 2)\n",
    "    samples = random_VMF(\n",
    "        get_vectors_by_angle([angle])[0], kappa=kappa, size=num_samples\n",
    "    )\n",
    "    gallery_features.extend(samples)\n",
    "\n",
    "gallery_features = np.array(gallery_features)\n",
    "gallery_unc_log = np.array(gallery_unc_log).reshape(-1, 1)\n",
    "gallery_subject_ids_sorted = np.array(gallery_subject_ids_sorted)\n",
    "\n",
    "gallery_unc = np.exp(gallery_unc_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = VonMisesFisher(3)\n",
    "feature_mean = sampler(np.array([[0, 1], [1, 0]]), np.array([[10], [15]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fontsize = 20\n",
    "linewidth = 3\n",
    "dot_size = 80\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "draw_circle(ax, linewidth)\n",
    "for class_id, (angle, kappa) in enumerate(zip(class_center_angles, class_z_kappa)):\n",
    "    color = colors[class_id]\n",
    "    # draw_dencity(\n",
    "    #     angle,\n",
    "    #     kappa,\n",
    "    #     ax,\n",
    "    #     linewidth=3,\n",
    "    #     color=color,\n",
    "    #     range=np.pi / 2,\n",
    "    #     draw_center=True,\n",
    "    #     dot_size=dot_size,\n",
    "    # )\n",
    "    for position in np.where(gallery_subject_ids_sorted == class_id)[0]:\n",
    "        point_angle = np.angle(\n",
    "            [gallery_features[position][0] + 1j * gallery_features[position][1]]\n",
    "        )[0]\n",
    "        draw_dencity(\n",
    "            point_angle,\n",
    "            gallery_unc[position],\n",
    "            ax,\n",
    "            linewidth=1,\n",
    "            color=color,\n",
    "            range=np.pi / 2,\n",
    "            scale=0.1,\n",
    "            draw_center=True,\n",
    "            dot_size=20,\n",
    "        )\n",
    "fig.gca().set_aspect(\"equal\")\n",
    "fig.show()\n",
    "plt.savefig(\"/app/outputs/images/generation.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "gallery_features.shape\n",
    "init_mean = np.array(\n",
    "    [\n",
    "        np.mean(gallery_features[gallery_subject_ids_sorted == c], axis=0)\n",
    "        for c in range(3)\n",
    "    ]\n",
    ")\n",
    "# init_mean = init_mean / np.linalg.norm(init_mean, axis=1, keepdims=True)\n",
    "\n",
    "init_kappa = np.array(\n",
    "    [np.mean(gallery_unc[gallery_subject_ids_sorted == c], axis=0) for c in range(3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19, 2), (19, 1))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gallery_features.shape, gallery_unc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from scipy.special import ive, hyp0f1, loggamma\n",
    "\n",
    "\n",
    "class MonteCarloPredictiveProb:\n",
    "    def __init__(\n",
    "        self,\n",
    "        M: int,\n",
    "        gallery_prior: str = \"power\",\n",
    "        unc_model: str = \"vMF\",\n",
    "        beta: float = 0.5,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        params:\n",
    "        M -- number of MC samples\n",
    "        gallery_prior -- model for p(z|c)\n",
    "        unc_model -- form of p(z|x)\n",
    "        \"\"\"\n",
    "        self.M = M\n",
    "        assert gallery_prior in [\"power\", \"vMF\"]\n",
    "        assert unc_model in [\"vMF\", \"PFE\"]\n",
    "        if unc_model == \"vMF\":\n",
    "            self.sampler = VonMisesFisher(self.M)\n",
    "        self.gallery_prior = gallery_prior\n",
    "        self.unc_model = unc_model\n",
    "        self.beta = beta\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        mean: np.array,\n",
    "        kappa: np.array,\n",
    "        gallery_means: torch.nn.Parameter,\n",
    "        gallery_kappas: torch.nn.Parameter,\n",
    "        T: torch.nn.Parameter,\n",
    "    ) -> Any:\n",
    "        self.K = gallery_means.shape[0]\n",
    "        # print(self.K)\n",
    "        zs = torch.tensor(self.sampler(mean, kappa))\n",
    "        d = torch.tensor([mean.shape[-1]])\n",
    "        # print(zs.shape, gallery_means.shape)\n",
    "        # print(zs, gallery_means)\n",
    "        similarities = zs @ gallery_means.T\n",
    "        # print(similarities.shape)\n",
    "        # print(similarities)\n",
    "        if self.gallery_prior == \"power\":\n",
    "            log_m_c_power = (\n",
    "                torch.special.gammaln(d - 1 + gallery_kappas)\n",
    "                + torch.special.gammaln(d / 2 + gallery_kappas)\n",
    "                + gallery_kappas * np.log(2)\n",
    "                - torch.special.gammaln(d / 2)\n",
    "                - torch.special.gammaln(d - 1 + 2 * gallery_kappas)\n",
    "            )\n",
    "            m_c_power = torch.exp(log_m_c_power)\n",
    "            log_uniform_dencity = (\n",
    "                torch.special.gammaln(d / 2) - np.log(2) - (d / 2) * np.log(np.pi)\n",
    "            )\n",
    "            log_normalizer = log_m_c_power + log_uniform_dencity\n",
    "        # compute log z prob\n",
    "        p_c = ((1 - self.beta) / self.K) ** (1 / T)\n",
    "        # print(similarities.shape, gallery_kappas.shape, log_uniform.shape, m_c_power.shape)\n",
    "        logit_sum = (\n",
    "            torch.sum(\n",
    "                (m_c_power[..., :, 0] ** (1 / T))\n",
    "                * ((1 + similarities) ** (gallery_kappas[..., :, 0] * (1 / T))),\n",
    "                dim=-1,\n",
    "            )\n",
    "            * p_c\n",
    "        )\n",
    "        log_z_prob = (1 / T) * log_uniform_dencity + torch.log(\n",
    "            logit_sum + (self.beta) ** (1 / T)\n",
    "        )\n",
    "\n",
    "        log_beta = np.log(self.beta)\n",
    "        # print(similarities.shape, gallery_kappas.shape)\n",
    "        uniform_log_prob = (1 / T) * (log_uniform_dencity + log_beta) - log_z_prob\n",
    "\n",
    "        # compute gallery classes log prob\n",
    "        pz_c = (\n",
    "            torch.log((1 + similarities)) * gallery_kappas[..., :, 0]\n",
    "            + log_normalizer[..., :, 0]\n",
    "        )\n",
    "        # print(pz_c.shape, log_z_prob.shape)\n",
    "        gallery_log_probs = (1 / T) * (\n",
    "            pz_c + np.log((1 - self.beta) / self.K)\n",
    "        ) - log_z_prob[..., np.newaxis]\n",
    "        # print(uniform_log_prob.shape)\n",
    "        log_probs = torch.cat(\n",
    "            [gallery_log_probs, uniform_log_prob[..., np.newaxis]], dim=-1\n",
    "        )\n",
    "        # print(log_probs.shape)\n",
    "        # print(torch.sum(log_probs, dim=-1))\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4870,  0.8734],\n",
      "        [-0.5543,  0.8323],\n",
      "        [-0.8208, -0.5712]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.0284],\n",
      "        [2.7930],\n",
      "        [8.8899]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 0, Loss: 5.055512716009559\n",
      "tensor([[-0.3607,  0.9327],\n",
      "        [-0.4335,  0.9011],\n",
      "        [-0.8920, -0.4521]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.9289],\n",
      "        [2.8590],\n",
      "        [8.7921]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 1, Loss: 4.414321656875508\n",
      "tensor([[-0.2300,  0.9732],\n",
      "        [-0.3098,  0.9508],\n",
      "        [-0.9445, -0.3286]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.8321],\n",
      "        [2.9393],\n",
      "        [8.6975]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 2, Loss: 3.5168603613880864\n",
      "tensor([[-0.0967,  0.9953],\n",
      "        [-0.1927,  0.9813],\n",
      "        [-0.9788, -0.2050]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.7370],\n",
      "        [3.0233],\n",
      "        [8.6084]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 3, Loss: 3.0435863824800906\n",
      "tensor([[ 0.0359,  0.9994],\n",
      "        [-0.0886,  0.9961],\n",
      "        [-0.9961, -0.0887]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.6446],\n",
      "        [3.1123],\n",
      "        [8.5257]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 4, Loss: 2.618654993142185\n",
      "tensor([[ 0.1650,  0.9863],\n",
      "        [-0.0075,  1.0000],\n",
      "        [-0.9999,  0.0154]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5554],\n",
      "        [3.2051],\n",
      "        [8.4491]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 5, Loss: 2.2481272025434866\n",
      "tensor([[ 0.2882,  0.9576],\n",
      "        [ 0.0492,  0.9988],\n",
      "        [-0.9948,  0.1021]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4698],\n",
      "        [3.3005],\n",
      "        [8.3752]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 6, Loss: 1.9793194289306661\n",
      "tensor([[ 0.4036,  0.9149],\n",
      "        [ 0.0812,  0.9967],\n",
      "        [-0.9857,  0.1687]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3881],\n",
      "        [3.3977],\n",
      "        [8.3030]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 7, Loss: 1.7589899605052186\n",
      "tensor([[ 0.5085,  0.8611],\n",
      "        [ 0.0932,  0.9956],\n",
      "        [-0.9768,  0.2143]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3117],\n",
      "        [3.4955],\n",
      "        [8.2317]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 8, Loss: 1.487873577953161\n",
      "tensor([[ 0.6021,  0.7984],\n",
      "        [ 0.0908,  0.9959],\n",
      "        [-0.9703,  0.2420]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2407],\n",
      "        [3.5932],\n",
      "        [8.1622]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 9, Loss: 1.3793237305798263\n",
      "tensor([[ 0.6846,  0.7289],\n",
      "        [ 0.0792,  0.9969],\n",
      "        [-0.9682,  0.2500]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1743],\n",
      "        [3.6906],\n",
      "        [8.0911]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 10, Loss: 1.3666643051397607\n",
      "tensor([[ 0.7559,  0.6547],\n",
      "        [ 0.0575,  0.9983],\n",
      "        [-0.9705,  0.2410]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1131],\n",
      "        [3.7872],\n",
      "        [8.0190]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 11, Loss: 1.2027280521496602\n",
      "tensor([[ 0.8160,  0.5781],\n",
      "        [ 0.0323,  0.9995],\n",
      "        [-0.9754,  0.2203]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0569],\n",
      "        [3.8830],\n",
      "        [7.9487]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 12, Loss: 1.0860846921109069\n",
      "tensor([[ 0.8654,  0.5010],\n",
      "        [ 0.0072,  1.0000],\n",
      "        [-0.9816,  0.1909]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0060],\n",
      "        [3.9775],\n",
      "        [7.8787]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 13, Loss: 0.9972949345699201\n",
      "tensor([[ 0.9054,  0.4246],\n",
      "        [-0.0147,  0.9999],\n",
      "        [-0.9881,  0.1536]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9597],\n",
      "        [4.0706],\n",
      "        [7.8092]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 14, Loss: 0.9587926947125535\n",
      "tensor([[ 0.9367,  0.3502],\n",
      "        [-0.0292,  0.9996],\n",
      "        [-0.9940,  0.1092]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9179],\n",
      "        [4.1617],\n",
      "        [7.7391]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 15, Loss: 0.9563949867964164\n",
      "tensor([[ 0.9604,  0.2787],\n",
      "        [-0.0329,  0.9995],\n",
      "        [-0.9981,  0.0622]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8804],\n",
      "        [4.2504],\n",
      "        [7.6701]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 16, Loss: 0.8953282255916504\n",
      "tensor([[ 0.9775,  0.2108],\n",
      "        [-0.0279,  0.9996],\n",
      "        [-0.9999,  0.0162]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8468],\n",
      "        [4.3369],\n",
      "        [7.6038]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 17, Loss: 0.8297947418292072\n",
      "tensor([[ 0.9891,  0.1473],\n",
      "        [-0.0192,  0.9998],\n",
      "        [-0.9996, -0.0277]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8169],\n",
      "        [4.4216],\n",
      "        [7.5395]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 18, Loss: 0.806764997656237\n",
      "tensor([[ 0.9961,  0.0881],\n",
      "        [-0.0024,  1.0000],\n",
      "        [-0.9976, -0.0687]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7904],\n",
      "        [4.5039],\n",
      "        [7.4767]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 19, Loss: 0.8114385141053508\n",
      "tensor([[ 0.9994,  0.0338],\n",
      "        [ 0.0166,  0.9999],\n",
      "        [-0.9945, -0.1047]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7669],\n",
      "        [4.5847],\n",
      "        [7.4159]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 20, Loss: 0.7917926333671915\n",
      "tensor([[ 0.9999, -0.0158],\n",
      "        [ 0.0391,  0.9992],\n",
      "        [-0.9910, -0.1338]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7461],\n",
      "        [4.6640],\n",
      "        [7.3568]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 21, Loss: 0.7857336605615256\n",
      "tensor([[ 0.9982, -0.0602],\n",
      "        [ 0.0620,  0.9981],\n",
      "        [-0.9876, -0.1572]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7277],\n",
      "        [4.7418],\n",
      "        [7.2973]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 22, Loss: 0.8307122985638982\n",
      "tensor([[ 0.9950, -0.0999],\n",
      "        [ 0.0795,  0.9968],\n",
      "        [-0.9847, -0.1743]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7116],\n",
      "        [4.8177],\n",
      "        [7.2383]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 23, Loss: 0.8114988773209074\n",
      "tensor([[ 0.9909, -0.1347],\n",
      "        [ 0.0879,  0.9961],\n",
      "        [-0.9828, -0.1847]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6973],\n",
      "        [4.8928],\n",
      "        [7.1786]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 24, Loss: 0.8136549202199459\n",
      "tensor([[ 0.9863, -0.1652],\n",
      "        [ 0.0939,  0.9956],\n",
      "        [-0.9821, -0.1886]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6847],\n",
      "        [4.9664],\n",
      "        [7.1189]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 25, Loss: 0.8295191515450849\n",
      "tensor([[ 0.9815, -0.1913],\n",
      "        [ 0.0953,  0.9955],\n",
      "        [-0.9824, -0.1867]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6736],\n",
      "        [5.0391],\n",
      "        [7.0578]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 26, Loss: 0.8349762820296702\n",
      "tensor([[ 0.9770, -0.2130],\n",
      "        [ 0.0886,  0.9961],\n",
      "        [-0.9837, -0.1799]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6637],\n",
      "        [5.1107],\n",
      "        [6.9972]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 27, Loss: 0.8366802047781794\n",
      "tensor([[ 0.9730, -0.2309],\n",
      "        [ 0.0757,  0.9971],\n",
      "        [-0.9855, -0.1697]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6548],\n",
      "        [5.1813],\n",
      "        [6.9363]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 28, Loss: 0.8243590474260274\n",
      "tensor([[ 0.9696, -0.2447],\n",
      "        [ 0.0575,  0.9983],\n",
      "        [-0.9878, -0.1558]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6469],\n",
      "        [5.2512],\n",
      "        [6.8754]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 29, Loss: 0.839974133340369\n",
      "tensor([[ 0.9669, -0.2551],\n",
      "        [ 0.0361,  0.9993],\n",
      "        [-0.9904, -0.1385]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6398],\n",
      "        [5.3197],\n",
      "        [6.8137]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 30, Loss: 0.8472446585035386\n",
      "tensor([[ 0.9649, -0.2626],\n",
      "        [ 0.0175,  0.9998],\n",
      "        [-0.9928, -0.1196]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6335],\n",
      "        [5.3869],\n",
      "        [6.7525]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 31, Loss: 0.8106824015327116\n",
      "tensor([[ 0.9637, -0.2669],\n",
      "        [ 0.0072,  1.0000],\n",
      "        [-0.9951, -0.0992]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6278],\n",
      "        [5.4530],\n",
      "        [6.6924]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 32, Loss: 0.8316558959937895\n",
      "tensor([[ 0.9634, -0.2680],\n",
      "        [-0.0035,  1.0000],\n",
      "        [-0.9970, -0.0779]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6226],\n",
      "        [5.5183],\n",
      "        [6.6340]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 33, Loss: 0.8247323087044434\n",
      "tensor([[ 0.9639, -0.2664],\n",
      "        [-0.0076,  1.0000],\n",
      "        [-0.9982, -0.0592]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6177],\n",
      "        [5.5816],\n",
      "        [6.5762]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 34, Loss: 0.849198607942\n",
      "tensor([[ 0.9650, -0.2623],\n",
      "        [-0.0111,  0.9999],\n",
      "        [-0.9991, -0.0428]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6133],\n",
      "        [5.6437],\n",
      "        [6.5190]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 35, Loss: 0.8409466172726772\n",
      "tensor([[ 0.9667, -0.2560],\n",
      "        [-0.0090,  1.0000],\n",
      "        [-0.9996, -0.0271]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6093],\n",
      "        [5.7047],\n",
      "        [6.4619]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 36, Loss: 0.8384950518546838\n",
      "tensor([[ 0.9688, -0.2479],\n",
      "        [ 0.0012,  1.0000],\n",
      "        [-0.9999, -0.0161]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6058],\n",
      "        [5.7641],\n",
      "        [6.4040]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 37, Loss: 0.8481073950887319\n",
      "tensor([[ 0.9712, -0.2382],\n",
      "        [ 0.0098,  1.0000],\n",
      "        [-1.0000, -0.0098]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6027],\n",
      "        [5.8227],\n",
      "        [6.3476]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 38, Loss: 0.8019887146792726\n",
      "tensor([[ 0.9740, -0.2267],\n",
      "        [ 0.0163,  0.9999],\n",
      "        [-1.0000, -0.0066]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5999],\n",
      "        [5.8813],\n",
      "        [6.2932]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 39, Loss: 0.7960753974338389\n",
      "tensor([[ 0.9768, -0.2144],\n",
      "        [ 0.0293,  0.9996],\n",
      "        [-1.0000, -0.0051]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5977],\n",
      "        [5.9389],\n",
      "        [6.2413]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 40, Loss: 0.7864126967925827\n",
      "tensor([[ 0.9796, -0.2009],\n",
      "        [ 0.0365,  0.9993],\n",
      "        [-1.0000, -0.0064]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5959],\n",
      "        [5.9954],\n",
      "        [6.1907]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 41, Loss: 0.805763826783411\n",
      "tensor([[ 0.9824, -0.1868],\n",
      "        [ 0.0468,  0.9989],\n",
      "        [-0.9999, -0.0101]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5947],\n",
      "        [6.0503],\n",
      "        [6.1404]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 42, Loss: 0.8166473985139444\n",
      "tensor([[ 0.9850, -0.1723],\n",
      "        [ 0.0589,  0.9983],\n",
      "        [-0.9999, -0.0145]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5939],\n",
      "        [6.1039],\n",
      "        [6.0919]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 43, Loss: 0.7881908012899982\n",
      "tensor([[ 0.9875, -0.1576],\n",
      "        [ 0.0705,  0.9975],\n",
      "        [-0.9998, -0.0211]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5938],\n",
      "        [6.1560],\n",
      "        [6.0472]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 44, Loss: 0.7687114824098834\n",
      "tensor([[ 0.9898, -0.1424],\n",
      "        [ 0.0776,  0.9970],\n",
      "        [-0.9996, -0.0297]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5941],\n",
      "        [6.2073],\n",
      "        [6.0022]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 45, Loss: 0.816444891687511\n",
      "tensor([[ 0.9919, -0.1271],\n",
      "        [ 0.0795,  0.9968],\n",
      "        [-0.9992, -0.0401]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5949],\n",
      "        [6.2580],\n",
      "        [5.9566]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 46, Loss: 0.80021805950279\n",
      "tensor([[ 0.9937, -0.1118],\n",
      "        [ 0.0728,  0.9973],\n",
      "        [-0.9987, -0.0504]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5962],\n",
      "        [6.3079],\n",
      "        [5.9116]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 47, Loss: 0.7819281509673419\n",
      "tensor([[ 0.9953, -0.0967],\n",
      "        [ 0.0607,  0.9982],\n",
      "        [-0.9981, -0.0614]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.5980],\n",
      "        [6.3571],\n",
      "        [5.8675]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 48, Loss: 0.7808229594519493\n",
      "tensor([[ 0.9967, -0.0817],\n",
      "        [ 0.0472,  0.9989],\n",
      "        [-0.9974, -0.0714]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6002],\n",
      "        [6.4052],\n",
      "        [5.8243]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 49, Loss: 0.797492118260619\n",
      "tensor([[ 0.9978, -0.0670],\n",
      "        [ 0.0301,  0.9995],\n",
      "        [-0.9967, -0.0814]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6029],\n",
      "        [6.4518],\n",
      "        [5.7818]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 50, Loss: 0.7898383190453744\n",
      "tensor([[ 0.9986, -0.0529],\n",
      "        [ 0.0266,  0.9996],\n",
      "        [-0.9959, -0.0905]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6060],\n",
      "        [6.4969],\n",
      "        [5.7405]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 51, Loss: 0.7882897298913916\n",
      "tensor([[ 0.9992, -0.0393],\n",
      "        [ 0.0294,  0.9996],\n",
      "        [-0.9952, -0.0974]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6096],\n",
      "        [6.5414],\n",
      "        [5.6993]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 52, Loss: 0.780599599881715\n",
      "tensor([[ 0.9997, -0.0259],\n",
      "        [ 0.0287,  0.9996],\n",
      "        [-0.9945, -0.1043]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6137],\n",
      "        [6.5845],\n",
      "        [5.6581]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 53, Loss: 0.7910774500653005\n",
      "tensor([[ 0.9999, -0.0134],\n",
      "        [ 0.0336,  0.9994],\n",
      "        [-0.9940, -0.1091]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6183],\n",
      "        [6.6266],\n",
      "        [5.6170]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 54, Loss: 0.7773676529307966\n",
      "tensor([[ 1.0000, -0.0017],\n",
      "        [ 0.0405,  0.9992],\n",
      "        [-0.9937, -0.1118]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6232],\n",
      "        [6.6683],\n",
      "        [5.5765]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 55, Loss: 0.774499265592343\n",
      "tensor([[ 1.0000,  0.0091],\n",
      "        [ 0.0460,  0.9989],\n",
      "        [-0.9937, -0.1120]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6285],\n",
      "        [6.7096],\n",
      "        [5.5365]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 56, Loss: 0.7777760028766906\n",
      "tensor([[ 0.9998,  0.0193],\n",
      "        [ 0.0497,  0.9988],\n",
      "        [-0.9940, -0.1097]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6342],\n",
      "        [6.7509],\n",
      "        [5.4972]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 57, Loss: 0.7608547277033102\n",
      "tensor([[ 0.9996,  0.0288],\n",
      "        [ 0.0499,  0.9988],\n",
      "        [-0.9944, -0.1053]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6402],\n",
      "        [6.7914],\n",
      "        [5.4592]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 58, Loss: 0.7748046191262704\n",
      "tensor([[ 0.9993,  0.0375],\n",
      "        [ 0.0459,  0.9989],\n",
      "        [-0.9950, -0.1000]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6463],\n",
      "        [6.8308],\n",
      "        [5.4229]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 59, Loss: 0.7860312936451606\n",
      "tensor([[ 0.9990,  0.0455],\n",
      "        [ 0.0446,  0.9990],\n",
      "        [-0.9954, -0.0959]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6528],\n",
      "        [6.8700],\n",
      "        [5.3883]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 60, Loss: 0.7610027180985197\n",
      "tensor([[ 0.9986,  0.0528],\n",
      "        [ 0.0406,  0.9992],\n",
      "        [-0.9956, -0.0942]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6596],\n",
      "        [6.9080],\n",
      "        [5.3520]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 61, Loss: 0.7906898116318766\n",
      "tensor([[ 0.9982,  0.0596],\n",
      "        [ 0.0352,  0.9994],\n",
      "        [-0.9957, -0.0932]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6667],\n",
      "        [6.9464],\n",
      "        [5.3156]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 62, Loss: 0.7658041108009539\n",
      "tensor([[ 0.9979,  0.0655],\n",
      "        [ 0.0315,  0.9995],\n",
      "        [-0.9958, -0.0917]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6740],\n",
      "        [6.9832],\n",
      "        [5.2805]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 63, Loss: 0.787515101264947\n",
      "tensor([[ 0.9975,  0.0704],\n",
      "        [ 0.0380,  0.9993],\n",
      "        [-0.9959, -0.0902]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6816],\n",
      "        [7.0191],\n",
      "        [5.2460]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 64, Loss: 0.7633653639597049\n",
      "tensor([[ 0.9972,  0.0746],\n",
      "        [ 0.0528,  0.9986],\n",
      "        [-0.9960, -0.0893]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6895],\n",
      "        [7.0546],\n",
      "        [5.2109]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 65, Loss: 0.785039997729872\n",
      "tensor([[ 0.9969,  0.0782],\n",
      "        [ 0.0583,  0.9983],\n",
      "        [-0.9961, -0.0877]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.6976],\n",
      "        [7.0899],\n",
      "        [5.1756]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 66, Loss: 0.7760130938249187\n",
      "tensor([[ 0.9967,  0.0810],\n",
      "        [ 0.0604,  0.9982],\n",
      "        [-0.9963, -0.0854]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7060],\n",
      "        [7.1241],\n",
      "        [5.1400]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 67, Loss: 0.7805578521311245\n",
      "tensor([[ 0.9965,  0.0830],\n",
      "        [ 0.0620,  0.9981],\n",
      "        [-0.9966, -0.0828]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7148],\n",
      "        [7.1577],\n",
      "        [5.1056]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 68, Loss: 0.7613520160716623\n",
      "tensor([[ 0.9964,  0.0844],\n",
      "        [ 0.0651,  0.9979],\n",
      "        [-0.9967, -0.0813]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7237],\n",
      "        [7.1906],\n",
      "        [5.0726]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 69, Loss: 0.768789057244147\n",
      "tensor([[ 0.9964,  0.0851],\n",
      "        [ 0.0633,  0.9980],\n",
      "        [-0.9968, -0.0796]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7329],\n",
      "        [7.2229],\n",
      "        [5.0431]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 70, Loss: 0.744386576052176\n",
      "tensor([[ 0.9964,  0.0853],\n",
      "        [ 0.0568,  0.9984],\n",
      "        [-0.9971, -0.0764]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7423],\n",
      "        [7.2545],\n",
      "        [5.0142]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 71, Loss: 0.7824664017638373\n",
      "tensor([[ 0.9963,  0.0854],\n",
      "        [ 0.0428,  0.9991],\n",
      "        [-0.9974, -0.0727]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7518],\n",
      "        [7.2849],\n",
      "        [4.9867]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 72, Loss: 0.7883076744868864\n",
      "tensor([[ 0.9964,  0.0848],\n",
      "        [ 0.0394,  0.9992],\n",
      "        [-0.9975, -0.0701]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7613],\n",
      "        [7.3151],\n",
      "        [4.9590]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 73, Loss: 0.7827107033790393\n",
      "tensor([[ 0.9965,  0.0830],\n",
      "        [ 0.0522,  0.9986],\n",
      "        [-0.9976, -0.0686]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7709],\n",
      "        [7.3429],\n",
      "        [4.9322]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 74, Loss: 0.7993269329249998\n",
      "tensor([[ 0.9967,  0.0810],\n",
      "        [ 0.0660,  0.9978],\n",
      "        [-0.9977, -0.0682]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7806],\n",
      "        [7.3710],\n",
      "        [4.9069]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 75, Loss: 0.770671708496137\n",
      "tensor([[ 0.9969,  0.0785],\n",
      "        [ 0.0833,  0.9965],\n",
      "        [-0.9977, -0.0683]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.7903],\n",
      "        [7.3981],\n",
      "        [4.8828]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 76, Loss: 0.7946394819535316\n",
      "tensor([[ 0.9971,  0.0756],\n",
      "        [ 0.0893,  0.9960],\n",
      "        [-0.9975, -0.0707]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8000],\n",
      "        [7.4250],\n",
      "        [4.8598]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 77, Loss: 0.7773185403135722\n",
      "tensor([[ 0.9974,  0.0727],\n",
      "        [ 0.0790,  0.9969],\n",
      "        [-0.9972, -0.0741]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8097],\n",
      "        [7.4518],\n",
      "        [4.8392]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 78, Loss: 0.7666934702778992\n",
      "tensor([[ 0.9976,  0.0698],\n",
      "        [ 0.0584,  0.9983],\n",
      "        [-0.9970, -0.0769]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8195],\n",
      "        [7.4786],\n",
      "        [4.8211]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 79, Loss: 0.7686153405145649\n",
      "tensor([[ 0.9977,  0.0673],\n",
      "        [ 0.0347,  0.9994],\n",
      "        [-0.9966, -0.0821]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8294],\n",
      "        [7.5048],\n",
      "        [4.7997]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 80, Loss: 0.7990649897844988\n",
      "tensor([[ 0.9979,  0.0643],\n",
      "        [ 0.0129,  0.9999],\n",
      "        [-0.9963, -0.0863]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8393],\n",
      "        [7.5314],\n",
      "        [4.7810]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 81, Loss: 0.7519260581524345\n",
      "tensor([[ 0.9981,  0.0613],\n",
      "        [ 0.0063,  1.0000],\n",
      "        [-0.9960, -0.0894]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8494],\n",
      "        [7.5573],\n",
      "        [4.7638]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 82, Loss: 0.7706246813175165\n",
      "tensor([[ 0.9983,  0.0584],\n",
      "        [ 0.0102,  0.9999],\n",
      "        [-0.9956, -0.0932]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8596],\n",
      "        [7.5830],\n",
      "        [4.7453]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 83, Loss: 0.7828435588017573\n",
      "tensor([[ 0.9985,  0.0554],\n",
      "        [ 0.0359,  0.9994],\n",
      "        [-0.9952, -0.0974]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8697],\n",
      "        [7.6079],\n",
      "        [4.7271]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 84, Loss: 0.7812565392346612\n",
      "tensor([[ 0.9986,  0.0528],\n",
      "        [ 0.0562,  0.9984],\n",
      "        [-0.9950, -0.1000]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8799],\n",
      "        [7.6334],\n",
      "        [4.7115]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 85, Loss: 0.7548736333457949\n",
      "tensor([[ 0.9987,  0.0502],\n",
      "        [ 0.0709,  0.9975],\n",
      "        [-0.9947, -0.1031]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.8903],\n",
      "        [7.6594],\n",
      "        [4.6991]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 86, Loss: 0.7416531126971234\n",
      "tensor([[ 0.9989,  0.0476],\n",
      "        [ 0.0786,  0.9969],\n",
      "        [-0.9942, -0.1072]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9007],\n",
      "        [7.6861],\n",
      "        [4.6864]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 87, Loss: 0.7452176362619892\n",
      "tensor([[ 0.9990,  0.0451],\n",
      "        [ 0.0774,  0.9970],\n",
      "        [-0.9941, -0.1087]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9113],\n",
      "        [7.7117],\n",
      "        [4.6749]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 88, Loss: 0.7639865676537243\n",
      "tensor([[ 0.9991,  0.0429],\n",
      "        [ 0.0643,  0.9979],\n",
      "        [-0.9940, -0.1093]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9220],\n",
      "        [7.7371],\n",
      "        [4.6651]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 89, Loss: 0.7458480713835239\n",
      "tensor([[ 0.9992,  0.0408],\n",
      "        [ 0.0549,  0.9985],\n",
      "        [-0.9941, -0.1083]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9326],\n",
      "        [7.7622],\n",
      "        [4.6541]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 90, Loss: 0.7829558901154897\n",
      "tensor([[ 0.9992,  0.0391],\n",
      "        [ 0.0461,  0.9989],\n",
      "        [-0.9941, -0.1081]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9433],\n",
      "        [7.7881],\n",
      "        [4.6449]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 91, Loss: 0.7388771702912594\n",
      "tensor([[ 0.9993,  0.0369],\n",
      "        [ 0.0518,  0.9987],\n",
      "        [-0.9941, -0.1087]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9542],\n",
      "        [7.8125],\n",
      "        [4.6354]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 92, Loss: 0.7677460337458861\n",
      "tensor([[ 0.9994,  0.0353],\n",
      "        [ 0.0603,  0.9982],\n",
      "        [-0.9941, -0.1084]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9652],\n",
      "        [7.8369],\n",
      "        [4.6265]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 93, Loss: 0.7630763373305163\n",
      "tensor([[ 0.9994,  0.0337],\n",
      "        [ 0.0583,  0.9983],\n",
      "        [-0.9941, -0.1084]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9762],\n",
      "        [7.8589],\n",
      "        [4.6177]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 94, Loss: 0.7820175130744145\n",
      "tensor([[ 0.9995,  0.0327],\n",
      "        [ 0.0503,  0.9987],\n",
      "        [-0.9943, -0.1067]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9871],\n",
      "        [7.8815],\n",
      "        [4.6088]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 95, Loss: 0.7772299852822108\n",
      "tensor([[ 0.9995,  0.0326],\n",
      "        [ 0.0263,  0.9997],\n",
      "        [-0.9946, -0.1034]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[7.9978],\n",
      "        [7.9053],\n",
      "        [4.6018]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 96, Loss: 0.7470150685662166\n",
      "tensor([[ 0.9995,  0.0331],\n",
      "        [ 0.0115,  0.9999],\n",
      "        [-0.9950, -0.1001]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0086],\n",
      "        [7.9292],\n",
      "        [4.5961]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 97, Loss: 0.7558500338515522\n",
      "tensor([[ 9.9942e-01,  3.4142e-02],\n",
      "        [ 8.9493e-04,  1.0000e+00],\n",
      "        [-9.9544e-01, -9.5417e-02]], dtype=torch.float64,\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0195],\n",
      "        [7.9537],\n",
      "        [4.5920]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 98, Loss: 0.749441210545937\n",
      "tensor([[ 0.9994,  0.0350],\n",
      "        [ 0.0193,  0.9998],\n",
      "        [-0.9958, -0.0911]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0303],\n",
      "        [7.9762],\n",
      "        [4.5879]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 99, Loss: 0.7864293059585231\n",
      "tensor([[ 0.9994,  0.0356],\n",
      "        [ 0.0426,  0.9991],\n",
      "        [-0.9961, -0.0877]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0412],\n",
      "        [7.9985],\n",
      "        [4.5812]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 100, Loss: 0.7705175067152367\n",
      "tensor([[ 0.9994,  0.0360],\n",
      "        [ 0.0837,  0.9965],\n",
      "        [-0.9963, -0.0857]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0523],\n",
      "        [8.0192],\n",
      "        [4.5747]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 101, Loss: 0.7811688990901092\n",
      "tensor([[ 0.9993,  0.0369],\n",
      "        [ 0.1114,  0.9938],\n",
      "        [-0.9965, -0.0831]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0631],\n",
      "        [8.0398],\n",
      "        [4.5688]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 102, Loss: 0.7911659635164181\n",
      "tensor([[ 0.9993,  0.0382],\n",
      "        [ 0.1152,  0.9933],\n",
      "        [-0.9967, -0.0811]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0740],\n",
      "        [8.0606],\n",
      "        [4.5657]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 103, Loss: 0.760955430615097\n",
      "tensor([[ 0.9992,  0.0392],\n",
      "        [ 0.1083,  0.9941],\n",
      "        [-0.9968, -0.0795]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0848],\n",
      "        [8.0806],\n",
      "        [4.5634]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 104, Loss: 0.7812523758146074\n",
      "tensor([[ 0.9992,  0.0403],\n",
      "        [ 0.0807,  0.9967],\n",
      "        [-0.9969, -0.0792]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.0955],\n",
      "        [8.1002],\n",
      "        [4.5612]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 105, Loss: 0.7757983160014025\n",
      "tensor([[ 0.9991,  0.0415],\n",
      "        [ 0.0508,  0.9987],\n",
      "        [-0.9968, -0.0801]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1065],\n",
      "        [8.1182],\n",
      "        [4.5588]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 106, Loss: 0.7775551441972444\n",
      "tensor([[ 0.9991,  0.0431],\n",
      "        [ 0.0240,  0.9997],\n",
      "        [-0.9967, -0.0813]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1175],\n",
      "        [8.1371],\n",
      "        [4.5566]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 107, Loss: 0.7591745590758855\n",
      "tensor([[ 0.9990,  0.0446],\n",
      "        [ 0.0068,  1.0000],\n",
      "        [-0.9966, -0.0824]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1286],\n",
      "        [8.1542],\n",
      "        [4.5541]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 108, Loss: 0.7824367052194261\n",
      "tensor([[ 0.9989,  0.0462],\n",
      "        [ 0.0032,  1.0000],\n",
      "        [-0.9965, -0.0832]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1396],\n",
      "        [8.1725],\n",
      "        [4.5533]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 109, Loss: 0.750118569459275\n",
      "tensor([[ 0.9989,  0.0477],\n",
      "        [ 0.0073,  1.0000],\n",
      "        [-0.9964, -0.0851]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1506],\n",
      "        [8.1900],\n",
      "        [4.5504]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 110, Loss: 0.7875453150148168\n",
      "tensor([[ 0.9988,  0.0489],\n",
      "        [ 0.0205,  0.9998],\n",
      "        [-0.9963, -0.0864]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1617],\n",
      "        [8.2059],\n",
      "        [4.5503]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 111, Loss: 0.7702556500009861\n",
      "tensor([[ 0.9987,  0.0502],\n",
      "        [ 0.0448,  0.9990],\n",
      "        [-0.9961, -0.0883]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1727],\n",
      "        [8.2207],\n",
      "        [4.5487]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 112, Loss: 0.7974197904332827\n",
      "tensor([[ 0.9987,  0.0513],\n",
      "        [ 0.0726,  0.9974],\n",
      "        [-0.9957, -0.0923]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1837],\n",
      "        [8.2357],\n",
      "        [4.5451]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 113, Loss: 0.772691005168399\n",
      "tensor([[ 0.9986,  0.0522],\n",
      "        [ 0.0942,  0.9956],\n",
      "        [-0.9954, -0.0956]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.1950],\n",
      "        [8.2512],\n",
      "        [4.5416]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 114, Loss: 0.7533179974003347\n",
      "tensor([[ 0.9986,  0.0535],\n",
      "        [ 0.0923,  0.9957],\n",
      "        [-0.9950, -0.1001]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2062],\n",
      "        [8.2681],\n",
      "        [4.5365]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 115, Loss: 0.7549344491477189\n",
      "tensor([[ 0.9985,  0.0547],\n",
      "        [ 0.0717,  0.9974],\n",
      "        [-0.9945, -0.1043]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2176],\n",
      "        [8.2840],\n",
      "        [4.5299]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 116, Loss: 0.7807390517117587\n",
      "tensor([[ 0.9984,  0.0557],\n",
      "        [ 0.0432,  0.9991],\n",
      "        [-0.9942, -0.1074]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2290],\n",
      "        [8.3006],\n",
      "        [4.5251]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 117, Loss: 0.7524850869139261\n",
      "tensor([[ 0.9984,  0.0565],\n",
      "        [ 0.0058,  1.0000],\n",
      "        [-0.9940, -0.1093]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2404],\n",
      "        [8.3164],\n",
      "        [4.5194]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 118, Loss: 0.7800709487948714\n",
      "tensor([[ 0.9983,  0.0577],\n",
      "        [-0.0118,  0.9999],\n",
      "        [-0.9937, -0.1118]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2518],\n",
      "        [8.3335],\n",
      "        [4.5105]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 119, Loss: 0.7831422161447109\n",
      "tensor([[ 0.9983,  0.0590],\n",
      "        [-0.0125,  0.9999],\n",
      "        [-0.9935, -0.1139]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2630],\n",
      "        [8.3511],\n",
      "        [4.5043]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 120, Loss: 0.7591989786434095\n",
      "tensor([[ 0.9982,  0.0594],\n",
      "        [ 0.0069,  1.0000],\n",
      "        [-0.9935, -0.1142]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2743],\n",
      "        [8.3687],\n",
      "        [4.4961]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 121, Loss: 0.7776674501574835\n",
      "tensor([[ 0.9982,  0.0597],\n",
      "        [ 0.0307,  0.9995],\n",
      "        [-0.9936, -0.1127]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2857],\n",
      "        [8.3865],\n",
      "        [4.4891]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 122, Loss: 0.7494631143154297\n",
      "tensor([[ 0.9982,  0.0593],\n",
      "        [ 0.0654,  0.9979],\n",
      "        [-0.9937, -0.1118]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.2971],\n",
      "        [8.4036],\n",
      "        [4.4831]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 123, Loss: 0.7646884296804032\n",
      "tensor([[ 0.9983,  0.0588],\n",
      "        [ 0.0918,  0.9958],\n",
      "        [-0.9939, -0.1099]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3086],\n",
      "        [8.4218],\n",
      "        [4.4766]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 124, Loss: 0.7541653360615466\n",
      "tensor([[ 0.9983,  0.0581],\n",
      "        [ 0.1008,  0.9949],\n",
      "        [-0.9941, -0.1085]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3203],\n",
      "        [8.4394],\n",
      "        [4.4675]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 125, Loss: 0.7617123252302934\n",
      "tensor([[ 0.9984,  0.0571],\n",
      "        [ 0.1015,  0.9948],\n",
      "        [-0.9944, -0.1054]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3320],\n",
      "        [8.4548],\n",
      "        [4.4600]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 126, Loss: 0.7779741268097453\n",
      "tensor([[ 0.9984,  0.0559],\n",
      "        [ 0.0883,  0.9961],\n",
      "        [-0.9947, -0.1024]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3437],\n",
      "        [8.4706],\n",
      "        [4.4509]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 127, Loss: 0.7796794503769449\n",
      "tensor([[ 0.9985,  0.0549],\n",
      "        [ 0.0801,  0.9968],\n",
      "        [-0.9952, -0.0982]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3554],\n",
      "        [8.4867],\n",
      "        [4.4428]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 128, Loss: 0.7648832102938051\n",
      "tensor([[ 0.9986,  0.0534],\n",
      "        [ 0.0686,  0.9976],\n",
      "        [-0.9956, -0.0936]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3671],\n",
      "        [8.5027],\n",
      "        [4.4347]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 129, Loss: 0.7645233886897597\n",
      "tensor([[ 0.9986,  0.0521],\n",
      "        [ 0.0493,  0.9988],\n",
      "        [-0.9959, -0.0908]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3787],\n",
      "        [8.5174],\n",
      "        [4.4274]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 130, Loss: 0.7750042313649963\n",
      "tensor([[ 0.9987,  0.0503],\n",
      "        [ 0.0216,  0.9998],\n",
      "        [-0.9959, -0.0910]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.3904],\n",
      "        [8.5307],\n",
      "        [4.4209]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 131, Loss: 0.7669451287352513\n",
      "tensor([[ 0.9988,  0.0491],\n",
      "        [-0.0029,  1.0000],\n",
      "        [-0.9960, -0.0897]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4019],\n",
      "        [8.5457],\n",
      "        [4.4181]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 132, Loss: 0.752559126736499\n",
      "tensor([[ 0.9989,  0.0476],\n",
      "        [-0.0095,  1.0000],\n",
      "        [-0.9960, -0.0894]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4135],\n",
      "        [8.5605],\n",
      "        [4.4174]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 133, Loss: 0.7615097383429704\n",
      "tensor([[ 9.9892e-01,  4.6512e-02],\n",
      "        [ 8.8913e-04,  1.0000e+00],\n",
      "        [-9.9603e-01, -8.9020e-02]], dtype=torch.float64,\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4252],\n",
      "        [8.5740],\n",
      "        [4.4162]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 134, Loss: 0.7747354498742927\n",
      "tensor([[ 0.9989,  0.0464],\n",
      "        [ 0.0094,  1.0000],\n",
      "        [-0.9960, -0.0896]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4367],\n",
      "        [8.5888],\n",
      "        [4.4142]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 135, Loss: 0.7604384203946001\n",
      "tensor([[ 0.9989,  0.0461],\n",
      "        [ 0.0402,  0.9992],\n",
      "        [-0.9958, -0.0912]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4484],\n",
      "        [8.6040],\n",
      "        [4.4108]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 136, Loss: 0.7562688896653679\n",
      "tensor([[ 0.9990,  0.0457],\n",
      "        [ 0.0782,  0.9969],\n",
      "        [-0.9957, -0.0925]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4602],\n",
      "        [8.6187],\n",
      "        [4.4093]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 137, Loss: 0.7596144963508815\n",
      "tensor([[ 0.9990,  0.0455],\n",
      "        [ 0.0853,  0.9964],\n",
      "        [-0.9955, -0.0952]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4718],\n",
      "        [8.6323],\n",
      "        [4.4103]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 138, Loss: 0.7697826683538238\n",
      "tensor([[ 0.9990,  0.0450],\n",
      "        [ 0.0857,  0.9963],\n",
      "        [-0.9954, -0.0956]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4833],\n",
      "        [8.6461],\n",
      "        [4.4118]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 139, Loss: 0.7782935976323188\n",
      "tensor([[ 0.9990,  0.0446],\n",
      "        [ 0.0703,  0.9975],\n",
      "        [-0.9954, -0.0960]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.4947],\n",
      "        [8.6602],\n",
      "        [4.4164]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 140, Loss: 0.7470246386995472\n",
      "tensor([[ 0.9990,  0.0444],\n",
      "        [ 0.0474,  0.9989],\n",
      "        [-0.9952, -0.0978]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5061],\n",
      "        [8.6749],\n",
      "        [4.4226]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 141, Loss: 0.7511727385592476\n",
      "tensor([[ 0.9990,  0.0442],\n",
      "        [ 0.0328,  0.9995],\n",
      "        [-0.9951, -0.0994]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5176],\n",
      "        [8.6899],\n",
      "        [4.4302]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 142, Loss: 0.7449200061093778\n",
      "tensor([[ 0.9990,  0.0437],\n",
      "        [ 0.0232,  0.9997],\n",
      "        [-0.9947, -0.1026]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5292],\n",
      "        [8.7043],\n",
      "        [4.4340]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 143, Loss: 0.7657781559577568\n",
      "tensor([[ 0.9991,  0.0428],\n",
      "        [ 0.0295,  0.9996],\n",
      "        [-0.9946, -0.1035]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5408],\n",
      "        [8.7193],\n",
      "        [4.4357]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 144, Loss: 0.7706792359619218\n",
      "tensor([[ 0.9991,  0.0418],\n",
      "        [ 0.0412,  0.9992],\n",
      "        [-0.9946, -0.1036]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5523],\n",
      "        [8.7341],\n",
      "        [4.4370]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 145, Loss: 0.7635788206538484\n",
      "tensor([[ 0.9992,  0.0404],\n",
      "        [ 0.0538,  0.9986],\n",
      "        [-0.9946, -0.1040]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5638],\n",
      "        [8.7478],\n",
      "        [4.4364]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 146, Loss: 0.7805766788536351\n",
      "tensor([[ 0.9993,  0.0382],\n",
      "        [ 0.0710,  0.9975],\n",
      "        [-0.9945, -0.1044]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5754],\n",
      "        [8.7615],\n",
      "        [4.4377]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 147, Loss: 0.7375624490216266\n",
      "tensor([[ 0.9993,  0.0366],\n",
      "        [ 0.0784,  0.9969],\n",
      "        [-0.9946, -0.1037]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5871],\n",
      "        [8.7754],\n",
      "        [4.4413]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 148, Loss: 0.7486625710021257\n",
      "tensor([[ 0.9994,  0.0352],\n",
      "        [ 0.0815,  0.9967],\n",
      "        [-0.9946, -0.1037]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.5988],\n",
      "        [8.7892],\n",
      "        [4.4436]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 149, Loss: 0.7635392215361477\n",
      "tensor([[ 0.9994,  0.0340],\n",
      "        [ 0.0739,  0.9973],\n",
      "        [-0.9948, -0.1023]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.6106],\n",
      "        [8.8033],\n",
      "        [4.4469]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 150, Loss: 0.7423730425130323\n",
      "tensor([[ 0.9995,  0.0328],\n",
      "        [ 0.0596,  0.9982],\n",
      "        [-0.9948, -0.1018]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.6224],\n",
      "        [8.8168],\n",
      "        [4.4513]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 151, Loss: 0.7578090465461407\n",
      "tensor([[ 0.9995,  0.0316],\n",
      "        [ 0.0499,  0.9988],\n",
      "        [-0.9947, -0.1025]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.6344],\n",
      "        [8.8294],\n",
      "        [4.4546]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 152, Loss: 0.7608967114875049\n",
      "tensor([[ 0.9995,  0.0319],\n",
      "        [ 0.0300,  0.9995],\n",
      "        [-0.9948, -0.1023]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.6461],\n",
      "        [8.8427],\n",
      "        [4.4607]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 153, Loss: 0.7537151671928594\n",
      "tensor([[ 0.9995,  0.0324],\n",
      "        [ 0.0345,  0.9994],\n",
      "        [-0.9948, -0.1017]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.6579],\n",
      "        [8.8548],\n",
      "        [4.4649]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 154, Loss: 0.7841843474884833\n",
      "tensor([[ 0.9995,  0.0330],\n",
      "        [ 0.0502,  0.9987],\n",
      "        [-0.9951, -0.0994]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.6696],\n",
      "        [8.8660],\n",
      "        [4.4693]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 155, Loss: 0.7772878803470777\n",
      "tensor([[ 0.9995,  0.0330],\n",
      "        [ 0.0713,  0.9975],\n",
      "        [-0.9954, -0.0960]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.6812],\n",
      "        [8.8779],\n",
      "        [4.4750]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 156, Loss: 0.737129570077307\n",
      "tensor([[ 0.9994,  0.0337],\n",
      "        [ 0.0812,  0.9967],\n",
      "        [-0.9957, -0.0925]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.6929],\n",
      "        [8.8905],\n",
      "        [4.4802]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 157, Loss: 0.7576587255628198\n",
      "tensor([[ 0.9994,  0.0338],\n",
      "        [ 0.0831,  0.9965],\n",
      "        [-0.9958, -0.0911]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.7046],\n",
      "        [8.9042],\n",
      "        [4.4831]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 158, Loss: 0.7392288413203459\n",
      "tensor([[ 0.9994,  0.0339],\n",
      "        [ 0.0762,  0.9971],\n",
      "        [-0.9958, -0.0920]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.7164],\n",
      "        [8.9179],\n",
      "        [4.4832]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 159, Loss: 0.7623608993147978\n",
      "tensor([[ 0.9994,  0.0345],\n",
      "        [ 0.0566,  0.9984],\n",
      "        [-0.9955, -0.0945]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.7281],\n",
      "        [8.9302],\n",
      "        [4.4810]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 160, Loss: 0.7852529314452055\n",
      "tensor([[ 0.9994,  0.0353],\n",
      "        [ 0.0240,  0.9997],\n",
      "        [-0.9952, -0.0977]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.7396],\n",
      "        [8.9430],\n",
      "        [4.4779]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 161, Loss: 0.7619993905756645\n",
      "tensor([[ 0.9994,  0.0358],\n",
      "        [-0.0028,  1.0000],\n",
      "        [-0.9949, -0.1012]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.7512],\n",
      "        [8.9548],\n",
      "        [4.4781]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 162, Loss: 0.7471624679869852\n",
      "tensor([[ 0.9993,  0.0369],\n",
      "        [-0.0176,  0.9998],\n",
      "        [-0.9945, -0.1046]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.7629],\n",
      "        [8.9668],\n",
      "        [4.4786]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 163, Loss: 0.740964187320836\n",
      "tensor([[ 9.9927e-01,  3.8125e-02],\n",
      "        [-4.8815e-04,  1.0000e+00],\n",
      "        [-9.9428e-01, -1.0677e-01]], dtype=torch.float64,\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.7748],\n",
      "        [8.9770],\n",
      "        [4.4781]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 164, Loss: 0.773177610734782\n",
      "tensor([[ 0.9992,  0.0396],\n",
      "        [ 0.0225,  0.9997],\n",
      "        [-0.9943, -0.1064]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.7868],\n",
      "        [8.9879],\n",
      "        [4.4792]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 165, Loss: 0.7277784628901675\n",
      "tensor([[ 0.9991,  0.0415],\n",
      "        [ 0.0421,  0.9991],\n",
      "        [-0.9944, -0.1052]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.7988],\n",
      "        [8.9996],\n",
      "        [4.4817]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 166, Loss: 0.7394109568691372\n",
      "tensor([[ 0.9991,  0.0426],\n",
      "        [ 0.0701,  0.9975],\n",
      "        [-0.9948, -0.1022]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.8111],\n",
      "        [9.0093],\n",
      "        [4.4848]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 167, Loss: 0.7509833916820906\n",
      "tensor([[ 0.9991,  0.0430],\n",
      "        [ 0.0966,  0.9953],\n",
      "        [-0.9948, -0.1015]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.8233],\n",
      "        [9.0175],\n",
      "        [4.4869]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 168, Loss: 0.7757292293101873\n",
      "tensor([[ 0.9990,  0.0440],\n",
      "        [ 0.1029,  0.9947],\n",
      "        [-0.9950, -0.1000]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.8356],\n",
      "        [9.0260],\n",
      "        [4.4890]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 169, Loss: 0.7515121760758919\n",
      "tensor([[ 0.9990,  0.0456],\n",
      "        [ 0.0838,  0.9965],\n",
      "        [-0.9950, -0.0999]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.8478],\n",
      "        [9.0332],\n",
      "        [4.4918]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 170, Loss: 0.7704352580783905\n",
      "tensor([[ 0.9989,  0.0471],\n",
      "        [ 0.0599,  0.9982],\n",
      "        [-0.9949, -0.1004]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.8600],\n",
      "        [9.0407],\n",
      "        [4.4979]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 171, Loss: 0.7385949218028521\n",
      "tensor([[ 0.9988,  0.0484],\n",
      "        [ 0.0329,  0.9995],\n",
      "        [-0.9948, -0.1019]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.8721],\n",
      "        [9.0484],\n",
      "        [4.4989]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 172, Loss: 0.792992358555148\n",
      "tensor([[ 0.9988,  0.0499],\n",
      "        [ 0.0098,  1.0000],\n",
      "        [-0.9948, -0.1014]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.8841],\n",
      "        [9.0576],\n",
      "        [4.5016]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 173, Loss: 0.7293982951054306\n",
      "tensor([[ 0.9987,  0.0506],\n",
      "        [ 0.0089,  1.0000],\n",
      "        [-0.9949, -0.1009]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.8963],\n",
      "        [9.0668],\n",
      "        [4.5032]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 174, Loss: 0.7663582693277119\n",
      "tensor([[ 0.9987,  0.0511],\n",
      "        [ 0.0183,  0.9998],\n",
      "        [-0.9951, -0.0987]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.9084],\n",
      "        [9.0757],\n",
      "        [4.5057]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 175, Loss: 0.7620181679291614\n",
      "tensor([[ 0.9987,  0.0509],\n",
      "        [ 0.0385,  0.9993],\n",
      "        [-0.9954, -0.0959]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.9204],\n",
      "        [9.0824],\n",
      "        [4.5068]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 176, Loss: 0.7936062420902186\n",
      "tensor([[ 0.9988,  0.0497],\n",
      "        [ 0.0530,  0.9986],\n",
      "        [-0.9956, -0.0935]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.9321],\n",
      "        [9.0901],\n",
      "        [4.5086]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 177, Loss: 0.7498810991619894\n",
      "tensor([[ 0.9988,  0.0487],\n",
      "        [ 0.0634,  0.9980],\n",
      "        [-0.9957, -0.0923]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.9439],\n",
      "        [9.0974],\n",
      "        [4.5115]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 178, Loss: 0.7656465602072015\n",
      "tensor([[ 0.9989,  0.0473],\n",
      "        [ 0.0681,  0.9977],\n",
      "        [-0.9957, -0.0921]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.9559],\n",
      "        [9.1030],\n",
      "        [4.5138]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 179, Loss: 0.7618644585464199\n",
      "tensor([[ 0.9990,  0.0458],\n",
      "        [ 0.0637,  0.9980],\n",
      "        [-0.9955, -0.0944]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.9680],\n",
      "        [9.1066],\n",
      "        [4.5152]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 180, Loss: 0.7737952547300624\n",
      "tensor([[ 0.9990,  0.0437],\n",
      "        [ 0.0616,  0.9981],\n",
      "        [-0.9953, -0.0971]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.9800],\n",
      "        [9.1106],\n",
      "        [4.5153]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 181, Loss: 0.7680365695267738\n",
      "tensor([[ 0.9991,  0.0415],\n",
      "        [ 0.0558,  0.9984],\n",
      "        [-0.9951, -0.0991]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[8.9921],\n",
      "        [9.1133],\n",
      "        [4.5150]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 182, Loss: 0.7688277469959075\n",
      "tensor([[ 0.9992,  0.0398],\n",
      "        [ 0.0406,  0.9992],\n",
      "        [-0.9948, -0.1019]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.0041],\n",
      "        [9.1167],\n",
      "        [4.5137]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 183, Loss: 0.7715423786073417\n",
      "tensor([[ 0.9993,  0.0386],\n",
      "        [ 0.0190,  0.9998],\n",
      "        [-0.9947, -0.1027]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.0161],\n",
      "        [9.1186],\n",
      "        [4.5142]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 184, Loss: 0.7624249969834106\n",
      "tensor([[ 0.9993,  0.0379],\n",
      "        [ 0.0191,  0.9998],\n",
      "        [-0.9948, -0.1016]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.0283],\n",
      "        [9.1219],\n",
      "        [4.5150]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 185, Loss: 0.7427235844239526\n",
      "tensor([[ 0.9993,  0.0368],\n",
      "        [ 0.0448,  0.9990],\n",
      "        [-0.9951, -0.0993]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.0408],\n",
      "        [9.1236],\n",
      "        [4.5177]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 186, Loss: 0.7518797786033475\n",
      "tensor([[ 0.9993,  0.0367],\n",
      "        [ 0.0751,  0.9972],\n",
      "        [-0.9953, -0.0965]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.0532],\n",
      "        [9.1255],\n",
      "        [4.5195]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 187, Loss: 0.7750745017825802\n",
      "tensor([[ 0.9993,  0.0370],\n",
      "        [ 0.1005,  0.9949],\n",
      "        [-0.9958, -0.0917]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.0652],\n",
      "        [9.1299],\n",
      "        [4.5207]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 188, Loss: 0.7627944067211978\n",
      "tensor([[ 0.9993,  0.0374],\n",
      "        [ 0.0993,  0.9951],\n",
      "        [-0.9961, -0.0881]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.0772],\n",
      "        [9.1333],\n",
      "        [4.5230]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 189, Loss: 0.7597904821896254\n",
      "tensor([[ 0.9993,  0.0374],\n",
      "        [ 0.0852,  0.9964],\n",
      "        [-0.9962, -0.0873]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.0893],\n",
      "        [9.1374],\n",
      "        [4.5251]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 190, Loss: 0.743899164731412\n",
      "tensor([[ 0.9993,  0.0375],\n",
      "        [ 0.0628,  0.9980],\n",
      "        [-0.9962, -0.0872]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.1015],\n",
      "        [9.1422],\n",
      "        [4.5271]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 191, Loss: 0.7402738986910203\n",
      "tensor([[ 0.9993,  0.0377],\n",
      "        [ 0.0336,  0.9994],\n",
      "        [-0.9961, -0.0883]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.1136],\n",
      "        [9.1477],\n",
      "        [4.5276]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 192, Loss: 0.7495497147931635\n",
      "tensor([[ 0.9993,  0.0381],\n",
      "        [ 0.0107,  0.9999],\n",
      "        [-0.9959, -0.0903]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.1258],\n",
      "        [9.1524],\n",
      "        [4.5302]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 193, Loss: 0.7589909224062334\n",
      "tensor([[ 0.9993,  0.0384],\n",
      "        [-0.0043,  1.0000],\n",
      "        [-0.9955, -0.0943]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.1383],\n",
      "        [9.1569],\n",
      "        [4.5347]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 194, Loss: 0.7268000837973009\n",
      "tensor([[ 0.9992,  0.0389],\n",
      "        [-0.0044,  1.0000],\n",
      "        [-0.9951, -0.0988]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.1508],\n",
      "        [9.1611],\n",
      "        [4.5403]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 195, Loss: 0.7588581206883139\n",
      "tensor([[ 0.9992,  0.0392],\n",
      "        [ 0.0159,  0.9999],\n",
      "        [-0.9947, -0.1024]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.1633],\n",
      "        [9.1644],\n",
      "        [4.5459]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 196, Loss: 0.7664686612092382\n",
      "tensor([[ 0.9992,  0.0395],\n",
      "        [ 0.0507,  0.9987],\n",
      "        [-0.9945, -0.1049]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.1757],\n",
      "        [9.1669],\n",
      "        [4.5480]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 197, Loss: 0.783255546245515\n",
      "tensor([[ 0.9992,  0.0402],\n",
      "        [ 0.0670,  0.9978],\n",
      "        [-0.9941, -0.1081]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.1882],\n",
      "        [9.1698],\n",
      "        [4.5518]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 198, Loss: 0.7349990552995451\n",
      "tensor([[ 0.9992,  0.0403],\n",
      "        [ 0.0894,  0.9960],\n",
      "        [-0.9937, -0.1121]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.2005],\n",
      "        [9.1717],\n",
      "        [4.5515]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 199, Loss: 0.7862886977088828\n",
      "tensor([[ 0.9991,  0.0415],\n",
      "        [ 0.0852,  0.9964],\n",
      "        [-0.9933, -0.1156]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.2125],\n",
      "        [9.1755],\n",
      "        [4.5525]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 200, Loss: 0.7451804885528156\n",
      "tensor([[ 0.9991,  0.0425],\n",
      "        [ 0.0726,  0.9974],\n",
      "        [-0.9933, -0.1155]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.2247],\n",
      "        [9.1787],\n",
      "        [4.5516]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 201, Loss: 0.7626723093418907\n",
      "tensor([[ 0.9991,  0.0429],\n",
      "        [ 0.0605,  0.9982],\n",
      "        [-0.9937, -0.1123]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.2369],\n",
      "        [9.1820],\n",
      "        [4.5506]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 202, Loss: 0.7460239024033801\n",
      "tensor([[ 0.9991,  0.0435],\n",
      "        [ 0.0314,  0.9995],\n",
      "        [-0.9941, -0.1083]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.2488],\n",
      "        [9.1835],\n",
      "        [4.5520]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 203, Loss: 0.779426095495227\n",
      "tensor([[ 0.9990,  0.0444],\n",
      "        [ 0.0125,  0.9999],\n",
      "        [-0.9942, -0.1072]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.2610],\n",
      "        [9.1854],\n",
      "        [4.5499]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 204, Loss: 0.7529071074879888\n",
      "tensor([[ 0.9990,  0.0451],\n",
      "        [ 0.0129,  0.9999],\n",
      "        [-0.9945, -0.1047]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.2733],\n",
      "        [9.1866],\n",
      "        [4.5498]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 205, Loss: 0.7510952949402334\n",
      "tensor([[ 0.9990,  0.0451],\n",
      "        [ 0.0448,  0.9990],\n",
      "        [-0.9950, -0.1002]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.2859],\n",
      "        [9.1857],\n",
      "        [4.5503]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 206, Loss: 0.7657146049227322\n",
      "tensor([[ 0.9990,  0.0456],\n",
      "        [ 0.0693,  0.9976],\n",
      "        [-0.9953, -0.0970]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.2985],\n",
      "        [9.1848],\n",
      "        [4.5502]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 207, Loss: 0.7672784776093863\n",
      "tensor([[ 0.9989,  0.0463],\n",
      "        [ 0.0744,  0.9972],\n",
      "        [-0.9951, -0.0985]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.3113],\n",
      "        [9.1837],\n",
      "        [4.5505]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 208, Loss: 0.7379497689400347\n",
      "tensor([[ 0.9989,  0.0463],\n",
      "        [ 0.0724,  0.9974],\n",
      "        [-0.9950, -0.1003]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.3241],\n",
      "        [9.1837],\n",
      "        [4.5494]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 209, Loss: 0.7438344902438807\n",
      "tensor([[ 0.9989,  0.0469],\n",
      "        [ 0.0529,  0.9986],\n",
      "        [-0.9948, -0.1022]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.3371],\n",
      "        [9.1828],\n",
      "        [4.5514]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 210, Loss: 0.7494898672094007\n",
      "tensor([[ 0.9989,  0.0462],\n",
      "        [ 0.0393,  0.9992],\n",
      "        [-0.9945, -0.1048]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.3498],\n",
      "        [9.1827],\n",
      "        [4.5507]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 211, Loss: 0.7598907616740087\n",
      "tensor([[ 0.9990,  0.0453],\n",
      "        [ 0.0309,  0.9995],\n",
      "        [-0.9943, -0.1062]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.3625],\n",
      "        [9.1837],\n",
      "        [4.5533]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 212, Loss: 0.7377313042351332\n",
      "tensor([[ 0.9990,  0.0443],\n",
      "        [ 0.0281,  0.9996],\n",
      "        [-0.9941, -0.1081]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.3752],\n",
      "        [9.1843],\n",
      "        [4.5556]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 213, Loss: 0.7516342863025798\n",
      "tensor([[ 0.9990,  0.0436],\n",
      "        [ 0.0346,  0.9994],\n",
      "        [-0.9938, -0.1112]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.3878],\n",
      "        [9.1846],\n",
      "        [4.5557]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 214, Loss: 0.7764749804005119\n",
      "tensor([[ 0.9991,  0.0435],\n",
      "        [ 0.0318,  0.9995],\n",
      "        [-0.9936, -0.1134]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.4004],\n",
      "        [9.1870],\n",
      "        [4.5563]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 215, Loss: 0.7269598815303971\n",
      "tensor([[ 0.9990,  0.0441],\n",
      "        [ 0.0356,  0.9994],\n",
      "        [-0.9934, -0.1145]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.4128],\n",
      "        [9.1913],\n",
      "        [4.5564]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 216, Loss: 0.7453327394232673\n",
      "tensor([[ 0.9990,  0.0443],\n",
      "        [ 0.0542,  0.9985],\n",
      "        [-0.9935, -0.1141]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.4253],\n",
      "        [9.1965],\n",
      "        [4.5535]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 217, Loss: 0.7496303314010289\n",
      "tensor([[ 0.9990,  0.0444],\n",
      "        [ 0.0745,  0.9972],\n",
      "        [-0.9938, -0.1111]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.4375],\n",
      "        [9.2016],\n",
      "        [4.5508]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 218, Loss: 0.7632776901894887\n",
      "tensor([[ 0.9990,  0.0449],\n",
      "        [ 0.0831,  0.9965],\n",
      "        [-0.9942, -0.1079]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.4498],\n",
      "        [9.2082],\n",
      "        [4.5511]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 219, Loss: 0.7261885335015783\n",
      "tensor([[ 0.9990,  0.0444],\n",
      "        [ 0.0782,  0.9969],\n",
      "        [-0.9946, -0.1036]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.4621],\n",
      "        [9.2136],\n",
      "        [4.5519]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 220, Loss: 0.7458559138620515\n",
      "tensor([[ 0.9990,  0.0440],\n",
      "        [ 0.0663,  0.9978],\n",
      "        [-0.9950, -0.0995]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.4744],\n",
      "        [9.2179],\n",
      "        [4.5487]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 221, Loss: 0.7747565210736971\n",
      "tensor([[ 0.9991,  0.0432],\n",
      "        [ 0.0540,  0.9985],\n",
      "        [-0.9956, -0.0932]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.4869],\n",
      "        [9.2195],\n",
      "        [4.5447]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 222, Loss: 0.7669116092135767\n",
      "tensor([[ 0.9991,  0.0428],\n",
      "        [ 0.0458,  0.9990],\n",
      "        [-0.9962, -0.0873]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.4993],\n",
      "        [9.2215],\n",
      "        [4.5437]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 223, Loss: 0.7505001590608785\n",
      "tensor([[ 0.9991,  0.0428],\n",
      "        [ 0.0377,  0.9993],\n",
      "        [-0.9968, -0.0799]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.5117],\n",
      "        [9.2239],\n",
      "        [4.5442]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 224, Loss: 0.7453459038847052\n",
      "tensor([[ 0.9991,  0.0429],\n",
      "        [ 0.0383,  0.9993],\n",
      "        [-0.9971, -0.0755]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.5242],\n",
      "        [9.2251],\n",
      "        [4.5438]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 225, Loss: 0.7600824534353132\n",
      "tensor([[ 0.9990,  0.0440],\n",
      "        [ 0.0274,  0.9996],\n",
      "        [-0.9974, -0.0722]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.5364],\n",
      "        [9.2254],\n",
      "        [4.5426]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 226, Loss: 0.7837670719192049\n",
      "tensor([[ 0.9990,  0.0457],\n",
      "        [ 0.0251,  0.9997],\n",
      "        [-0.9973, -0.0731]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.5486],\n",
      "        [9.2268],\n",
      "        [4.5389]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 227, Loss: 0.756734375311725\n",
      "tensor([[ 0.9989,  0.0473],\n",
      "        [ 0.0469,  0.9989],\n",
      "        [-0.9972, -0.0752]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.5608],\n",
      "        [9.2275],\n",
      "        [4.5379]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 228, Loss: 0.7648412401607577\n",
      "tensor([[ 0.9988,  0.0487],\n",
      "        [ 0.0711,  0.9975],\n",
      "        [-0.9970, -0.0772]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.5728],\n",
      "        [9.2283],\n",
      "        [4.5385]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 229, Loss: 0.7572941661855307\n",
      "tensor([[ 0.9987,  0.0501],\n",
      "        [ 0.0848,  0.9964],\n",
      "        [-0.9969, -0.0784]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.5847],\n",
      "        [9.2296],\n",
      "        [4.5417]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 230, Loss: 0.7476935518768879\n",
      "tensor([[ 0.9987,  0.0512],\n",
      "        [ 0.0774,  0.9970],\n",
      "        [-0.9967, -0.0813]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.5967],\n",
      "        [9.2310],\n",
      "        [4.5455]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 231, Loss: 0.7465248313773342\n",
      "tensor([[ 0.9987,  0.0519],\n",
      "        [ 0.0783,  0.9969],\n",
      "        [-0.9965, -0.0833]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.6085],\n",
      "        [9.2333],\n",
      "        [4.5501]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 232, Loss: 0.7580052086359427\n",
      "tensor([[ 0.9986,  0.0530],\n",
      "        [ 0.0679,  0.9977],\n",
      "        [-0.9964, -0.0850]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.6204],\n",
      "        [9.2368],\n",
      "        [4.5537]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 233, Loss: 0.7521920387989532\n",
      "tensor([[ 0.9985,  0.0540],\n",
      "        [ 0.0484,  0.9988],\n",
      "        [-0.9960, -0.0890]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.6326],\n",
      "        [9.2398],\n",
      "        [4.5550]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 234, Loss: 0.7446186447189258\n",
      "tensor([[ 0.9985,  0.0550],\n",
      "        [ 0.0305,  0.9995],\n",
      "        [-0.9955, -0.0946]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.6447],\n",
      "        [9.2431],\n",
      "        [4.5521]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 235, Loss: 0.7731956611717062\n",
      "tensor([[ 0.9984,  0.0567],\n",
      "        [ 0.0149,  0.9999],\n",
      "        [-0.9949, -0.1005]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.6567],\n",
      "        [9.2480],\n",
      "        [4.5483]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 236, Loss: 0.7483183801735269\n",
      "tensor([[ 0.9983,  0.0583],\n",
      "        [ 0.0062,  1.0000],\n",
      "        [-0.9945, -0.1050]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.6688],\n",
      "        [9.2539],\n",
      "        [4.5445]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 237, Loss: 0.7384264821641676\n",
      "tensor([[ 0.9983,  0.0591],\n",
      "        [ 0.0205,  0.9998],\n",
      "        [-0.9945, -0.1051]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.6809],\n",
      "        [9.2596],\n",
      "        [4.5394]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 238, Loss: 0.7664432970991153\n",
      "tensor([[ 0.9982,  0.0593],\n",
      "        [ 0.0389,  0.9992],\n",
      "        [-0.9944, -0.1054]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.6928],\n",
      "        [9.2662],\n",
      "        [4.5352]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 239, Loss: 0.7417767897878212\n",
      "tensor([[ 0.9983,  0.0588],\n",
      "        [ 0.0556,  0.9985],\n",
      "        [-0.9947, -0.1032]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.7048],\n",
      "        [9.2738],\n",
      "        [4.5321]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 240, Loss: 0.7363866761185583\n",
      "tensor([[ 0.9983,  0.0580],\n",
      "        [ 0.0673,  0.9977],\n",
      "        [-0.9950, -0.1001]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.7166],\n",
      "        [9.2793],\n",
      "        [4.5327]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 241, Loss: 0.7759829435608577\n",
      "tensor([[ 0.9984,  0.0571],\n",
      "        [ 0.0791,  0.9969],\n",
      "        [-0.9953, -0.0964]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.7283],\n",
      "        [9.2848],\n",
      "        [4.5330]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 242, Loss: 0.7727206590060228\n",
      "tensor([[ 0.9984,  0.0564],\n",
      "        [ 0.0751,  0.9972],\n",
      "        [-0.9957, -0.0928]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.7401],\n",
      "        [9.2895],\n",
      "        [4.5335]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 243, Loss: 0.7485423088862208\n",
      "tensor([[ 0.9984,  0.0560],\n",
      "        [ 0.0635,  0.9980],\n",
      "        [-0.9958, -0.0920]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.7518],\n",
      "        [9.2920],\n",
      "        [4.5313]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 244, Loss: 0.7859334215261707\n",
      "tensor([[ 0.9985,  0.0553],\n",
      "        [ 0.0597,  0.9982],\n",
      "        [-0.9960, -0.0898]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.7637],\n",
      "        [9.2941],\n",
      "        [4.5292]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 245, Loss: 0.755866906066183\n",
      "tensor([[ 0.9985,  0.0539],\n",
      "        [ 0.0654,  0.9979],\n",
      "        [-0.9959, -0.0900]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.7756],\n",
      "        [9.2950],\n",
      "        [4.5245]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 246, Loss: 0.7706657796777495\n",
      "tensor([[ 0.9986,  0.0522],\n",
      "        [ 0.0609,  0.9981],\n",
      "        [-0.9959, -0.0905]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.7877],\n",
      "        [9.2961],\n",
      "        [4.5158]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 247, Loss: 0.7561580388324842\n",
      "tensor([[ 0.9987,  0.0505],\n",
      "        [ 0.0518,  0.9987],\n",
      "        [-0.9957, -0.0930]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.7999],\n",
      "        [9.2968],\n",
      "        [4.5095]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 248, Loss: 0.7479101569877714\n",
      "tensor([[ 0.9988,  0.0483],\n",
      "        [ 0.0458,  0.9989],\n",
      "        [-0.9949, -0.1007]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.8123],\n",
      "        [9.2966],\n",
      "        [4.5028]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 249, Loss: 0.7569984624762183\n",
      "tensor([[ 0.9989,  0.0465],\n",
      "        [ 0.0316,  0.9995],\n",
      "        [-0.9944, -0.1056]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.8247],\n",
      "        [9.2980],\n",
      "        [4.4991]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 250, Loss: 0.7313351537630174\n",
      "tensor([[ 0.9990,  0.0444],\n",
      "        [ 0.0156,  0.9999],\n",
      "        [-0.9942, -0.1072]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.8369],\n",
      "        [9.3014],\n",
      "        [4.4973]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 251, Loss: 0.7329163590863107\n",
      "tensor([[ 0.9991,  0.0430],\n",
      "        [ 0.0256,  0.9997],\n",
      "        [-0.9943, -0.1070]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.8492],\n",
      "        [9.3046],\n",
      "        [4.4996]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 252, Loss: 0.7401054933234888\n",
      "tensor([[ 0.9991,  0.0424],\n",
      "        [ 0.0432,  0.9991],\n",
      "        [-0.9944, -0.1056]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.8611],\n",
      "        [9.3067],\n",
      "        [4.5013]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 253, Loss: 0.7822825541583406\n",
      "tensor([[ 0.9991,  0.0424],\n",
      "        [ 0.0551,  0.9985],\n",
      "        [-0.9947, -0.1024]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.8731],\n",
      "        [9.3086],\n",
      "        [4.5073]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 254, Loss: 0.738652640650583\n",
      "tensor([[ 0.9991,  0.0423],\n",
      "        [ 0.0758,  0.9971],\n",
      "        [-0.9951, -0.0985]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.8851],\n",
      "        [9.3098],\n",
      "        [4.5159]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 255, Loss: 0.7524624458186211\n",
      "tensor([[ 0.9991,  0.0428],\n",
      "        [ 0.0754,  0.9972],\n",
      "        [-0.9954, -0.0957]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.8972],\n",
      "        [9.3101],\n",
      "        [4.5227]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 256, Loss: 0.7550086424756187\n",
      "tensor([[ 0.9991,  0.0434],\n",
      "        [ 0.0757,  0.9971],\n",
      "        [-0.9956, -0.0942]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.9093],\n",
      "        [9.3102],\n",
      "        [4.5307]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 257, Loss: 0.7472881233174854\n",
      "tensor([[ 0.9990,  0.0439],\n",
      "        [ 0.0750,  0.9972],\n",
      "        [-0.9957, -0.0931]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.9213],\n",
      "        [9.3121],\n",
      "        [4.5402]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 258, Loss: 0.7397045076511016\n",
      "tensor([[ 0.9990,  0.0444],\n",
      "        [ 0.0534,  0.9986],\n",
      "        [-0.9957, -0.0929]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.9331],\n",
      "        [9.3139],\n",
      "        [4.5507]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 259, Loss: 0.7531778585408995\n",
      "tensor([[ 0.9990,  0.0447],\n",
      "        [ 0.0332,  0.9994],\n",
      "        [-0.9958, -0.0916]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.9449],\n",
      "        [9.3168],\n",
      "        [4.5631]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 260, Loss: 0.7203605080768438\n",
      "tensor([[ 0.9990,  0.0451],\n",
      "        [ 0.0272,  0.9996],\n",
      "        [-0.9960, -0.0896]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.9569],\n",
      "        [9.3202],\n",
      "        [4.5744]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 261, Loss: 0.738563724824633\n",
      "tensor([[ 0.9990,  0.0457],\n",
      "        [ 0.0421,  0.9991],\n",
      "        [-0.9962, -0.0870]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.9690],\n",
      "        [9.3210],\n",
      "        [4.5824]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 262, Loss: 0.7895676012061763\n",
      "tensor([[ 0.9989,  0.0458],\n",
      "        [ 0.0696,  0.9976],\n",
      "        [-0.9963, -0.0859]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.9810],\n",
      "        [9.3223],\n",
      "        [4.5878]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 263, Loss: 0.7492760149154433\n",
      "tensor([[ 0.9989,  0.0464],\n",
      "        [ 0.0977,  0.9952],\n",
      "        [-0.9965, -0.0834]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[9.9930],\n",
      "        [9.3233],\n",
      "        [4.5940]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 264, Loss: 0.7529578445531034\n",
      "tensor([[ 0.9989,  0.0462],\n",
      "        [ 0.1156,  0.9933],\n",
      "        [-0.9965, -0.0835]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.0052],\n",
      "        [ 9.3226],\n",
      "        [ 4.5961]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 265, Loss: 0.7702110403172684\n",
      "tensor([[ 0.9989,  0.0469],\n",
      "        [ 0.1067,  0.9943],\n",
      "        [-0.9961, -0.0879]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.0172],\n",
      "        [ 9.3230],\n",
      "        [ 4.5948]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 266, Loss: 0.7593580025326951\n",
      "tensor([[ 0.9989,  0.0479],\n",
      "        [ 0.0802,  0.9968],\n",
      "        [-0.9959, -0.0907]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.0294],\n",
      "        [ 9.3244],\n",
      "        [ 4.5967]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 267, Loss: 0.7314428105855724\n",
      "tensor([[ 0.9988,  0.0488],\n",
      "        [ 0.0376,  0.9993],\n",
      "        [-0.9956, -0.0941]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.0414],\n",
      "        [ 9.3261],\n",
      "        [ 4.5999]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 268, Loss: 0.7443221319416797\n",
      "tensor([[ 0.9988,  0.0491],\n",
      "        [ 0.0118,  0.9999],\n",
      "        [-0.9953, -0.0971]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.0532],\n",
      "        [ 9.3295],\n",
      "        [ 4.6022]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 269, Loss: 0.7444542454804427\n",
      "tensor([[ 0.9988,  0.0498],\n",
      "        [-0.0018,  1.0000],\n",
      "        [-0.9954, -0.0956]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.0650],\n",
      "        [ 9.3338],\n",
      "        [ 4.6075]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 270, Loss: 0.7334827677695016\n",
      "tensor([[ 9.9874e-01,  5.0183e-02],\n",
      "        [ 8.9043e-04,  1.0000e+00],\n",
      "        [-9.9566e-01, -9.3057e-02]], dtype=torch.float64,\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.0765],\n",
      "        [ 9.3378],\n",
      "        [ 4.6140]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 271, Loss: 0.7580575009657854\n",
      "tensor([[ 0.9987,  0.0510],\n",
      "        [ 0.0312,  0.9995],\n",
      "        [-0.9961, -0.0880]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.0880],\n",
      "        [ 9.3415],\n",
      "        [ 4.6206]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 272, Loss: 0.7482678156813546\n",
      "tensor([[ 0.9987,  0.0510],\n",
      "        [ 0.0845,  0.9964],\n",
      "        [-0.9964, -0.0850]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.0997],\n",
      "        [ 9.3442],\n",
      "        [ 4.6220]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 273, Loss: 0.7737900553209451\n",
      "tensor([[ 0.9987,  0.0509],\n",
      "        [ 0.1238,  0.9923],\n",
      "        [-0.9965, -0.0838]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.1115],\n",
      "        [ 9.3452],\n",
      "        [ 4.6208]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 274, Loss: 0.7692168493415393\n",
      "tensor([[ 0.9987,  0.0513],\n",
      "        [ 0.1307,  0.9914],\n",
      "        [-0.9966, -0.0821]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.1235],\n",
      "        [ 9.3462],\n",
      "        [ 4.6252]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 275, Loss: 0.7349379533215128\n",
      "tensor([[ 0.9986,  0.0524],\n",
      "        [ 0.1079,  0.9942],\n",
      "        [-0.9968, -0.0804]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.1353],\n",
      "        [ 9.3460],\n",
      "        [ 4.6295]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 276, Loss: 0.7692701753681243\n",
      "tensor([[ 0.9986,  0.0530],\n",
      "        [ 0.0678,  0.9977],\n",
      "        [-0.9967, -0.0816]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.1470],\n",
      "        [ 9.3452],\n",
      "        [ 4.6302]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 277, Loss: 0.7696342370357577\n",
      "tensor([[ 0.9985,  0.0539],\n",
      "        [ 0.0052,  1.0000],\n",
      "        [-0.9964, -0.0846]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.1587],\n",
      "        [ 9.3435],\n",
      "        [ 4.6282]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 278, Loss: 0.7599301580081249\n",
      "tensor([[ 0.9984,  0.0562],\n",
      "        [-0.0316,  0.9995],\n",
      "        [-0.9960, -0.0891]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.1704],\n",
      "        [ 9.3436],\n",
      "        [ 4.6260]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 279, Loss: 0.7503103235966556\n",
      "tensor([[ 0.9983,  0.0587],\n",
      "        [-0.0395,  0.9992],\n",
      "        [-0.9958, -0.0914]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.1820],\n",
      "        [ 9.3445],\n",
      "        [ 4.6200]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 280, Loss: 0.7589322302689215\n",
      "tensor([[ 0.9982,  0.0596],\n",
      "        [-0.0100,  1.0000],\n",
      "        [-0.9955, -0.0952]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.1938],\n",
      "        [ 9.3420],\n",
      "        [ 4.6086]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 281, Loss: 0.7861302865244765\n",
      "tensor([[ 0.9982,  0.0600],\n",
      "        [ 0.0386,  0.9993],\n",
      "        [-0.9952, -0.0976]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.2057],\n",
      "        [ 9.3368],\n",
      "        [ 4.5945]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 282, Loss: 0.7871419717800261\n",
      "tensor([[ 0.9982,  0.0599],\n",
      "        [ 0.0721,  0.9974],\n",
      "        [-0.9950, -0.0998]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.2173],\n",
      "        [ 9.3336],\n",
      "        [ 4.5820]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 283, Loss: 0.7428486000916813\n",
      "tensor([[ 0.9982,  0.0596],\n",
      "        [ 0.1025,  0.9947],\n",
      "        [-0.9949, -0.1004]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.2289],\n",
      "        [ 9.3321],\n",
      "        [ 4.5675]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 284, Loss: 0.7665408185038498\n",
      "tensor([[ 0.9982,  0.0606],\n",
      "        [ 0.1041,  0.9946],\n",
      "        [-0.9949, -0.1008]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.2404],\n",
      "        [ 9.3309],\n",
      "        [ 4.5501]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 285, Loss: 0.7710301961615894\n",
      "tensor([[ 0.9982,  0.0607],\n",
      "        [ 0.0954,  0.9954],\n",
      "        [-0.9951, -0.0990]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.2518],\n",
      "        [ 9.3311],\n",
      "        [ 4.5334]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 286, Loss: 0.7518240305246732\n",
      "tensor([[ 0.9982,  0.0599],\n",
      "        [ 0.0792,  0.9969],\n",
      "        [-0.9954, -0.0954]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.2633],\n",
      "        [ 9.3325],\n",
      "        [ 4.5166]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 287, Loss: 0.7434683340788519\n",
      "tensor([[ 0.9982,  0.0601],\n",
      "        [ 0.0525,  0.9986],\n",
      "        [-0.9954, -0.0953]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.2749],\n",
      "        [ 9.3338],\n",
      "        [ 4.4990]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 288, Loss: 0.7523934227352532\n",
      "tensor([[ 0.9981,  0.0609],\n",
      "        [ 0.0340,  0.9994],\n",
      "        [-0.9955, -0.0950]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.2868],\n",
      "        [ 9.3365],\n",
      "        [ 4.4853]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 289, Loss: 0.7345163210632923\n",
      "tensor([[ 0.9981,  0.0616],\n",
      "        [ 0.0236,  0.9997],\n",
      "        [-0.9955, -0.0949]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.2987],\n",
      "        [ 9.3384],\n",
      "        [ 4.4722]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 290, Loss: 0.7522388438554705\n",
      "tensor([[ 0.9980,  0.0627],\n",
      "        [ 0.0118,  0.9999],\n",
      "        [-0.9954, -0.0960]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.3105],\n",
      "        [ 9.3386],\n",
      "        [ 4.4635]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 291, Loss: 0.7635515244023722\n",
      "tensor([[ 0.9980,  0.0635],\n",
      "        [ 0.0253,  0.9997],\n",
      "        [-0.9952, -0.0978]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.3224],\n",
      "        [ 9.3395],\n",
      "        [ 4.4565]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 292, Loss: 0.7537268023145245\n",
      "tensor([[ 0.9979,  0.0648],\n",
      "        [ 0.0399,  0.9992],\n",
      "        [-0.9951, -0.0986]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.3345],\n",
      "        [ 9.3404],\n",
      "        [ 4.4478]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 293, Loss: 0.7423469310032644\n",
      "tensor([[ 0.9978,  0.0663],\n",
      "        [ 0.0587,  0.9983],\n",
      "        [-0.9951, -0.0990]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.3465],\n",
      "        [ 9.3390],\n",
      "        [ 4.4422]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 294, Loss: 0.7763816536119347\n",
      "tensor([[ 0.9978,  0.0661],\n",
      "        [ 0.0789,  0.9969],\n",
      "        [-0.9951, -0.0991]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.3587],\n",
      "        [ 9.3377],\n",
      "        [ 4.4363]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 295, Loss: 0.748877645673228\n",
      "tensor([[ 0.9978,  0.0664],\n",
      "        [ 0.0705,  0.9975],\n",
      "        [-0.9951, -0.0988]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.3709],\n",
      "        [ 9.3366],\n",
      "        [ 4.4282]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 296, Loss: 0.7506420343266416\n",
      "tensor([[ 0.9978,  0.0659],\n",
      "        [ 0.0516,  0.9987],\n",
      "        [-0.9950, -0.1001]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.3833],\n",
      "        [ 9.3341],\n",
      "        [ 4.4234]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 297, Loss: 0.7437473254741934\n",
      "tensor([[ 0.9979,  0.0650],\n",
      "        [ 0.0436,  0.9990],\n",
      "        [-0.9951, -0.0993]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.3959],\n",
      "        [ 9.3309],\n",
      "        [ 4.4169]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 298, Loss: 0.7635091112308788\n",
      "tensor([[ 0.9980,  0.0635],\n",
      "        [ 0.0398,  0.9992],\n",
      "        [-0.9950, -0.0997]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.4084],\n",
      "        [ 9.3287],\n",
      "        [ 4.4109]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 299, Loss: 0.7434762858595111\n",
      "tensor([[ 0.9981,  0.0620],\n",
      "        [ 0.0428,  0.9991],\n",
      "        [-0.9948, -0.1014]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.4209],\n",
      "        [ 9.3280],\n",
      "        [ 4.4046]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 300, Loss: 0.7459501975553884\n",
      "tensor([[ 0.9982,  0.0596],\n",
      "        [ 0.0626,  0.9980],\n",
      "        [-0.9950, -0.0998]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.4334],\n",
      "        [ 9.3256],\n",
      "        [ 4.4013]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 301, Loss: 0.7803441457388081\n",
      "tensor([[ 0.9983,  0.0575],\n",
      "        [ 0.0815,  0.9967],\n",
      "        [-0.9953, -0.0973]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.4460],\n",
      "        [ 9.3225],\n",
      "        [ 4.4009]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 302, Loss: 0.7382933360132797\n",
      "tensor([[ 0.9985,  0.0552],\n",
      "        [ 0.1038,  0.9946],\n",
      "        [-0.9954, -0.0956]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.4588],\n",
      "        [ 9.3142],\n",
      "        [ 4.4030]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 303, Loss: 0.7994792291978678\n",
      "tensor([[ 0.9986,  0.0535],\n",
      "        [ 0.1045,  0.9945],\n",
      "        [-0.9952, -0.0983]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.4715],\n",
      "        [ 9.3062],\n",
      "        [ 4.4040]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 304, Loss: 0.7544406870517523\n",
      "tensor([[ 0.9987,  0.0504],\n",
      "        [ 0.1053,  0.9944],\n",
      "        [-0.9952, -0.0977]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.4843],\n",
      "        [ 9.2969],\n",
      "        [ 4.4082]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 305, Loss: 0.7577767807644593\n",
      "tensor([[ 0.9989,  0.0467],\n",
      "        [ 0.0949,  0.9955],\n",
      "        [-0.9953, -0.0968]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.4968],\n",
      "        [ 9.2892],\n",
      "        [ 4.4175]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 306, Loss: 0.7371043288958684\n",
      "tensor([[ 0.9990,  0.0440],\n",
      "        [ 0.0711,  0.9975],\n",
      "        [-0.9955, -0.0951]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.5091],\n",
      "        [ 9.2826],\n",
      "        [ 4.4284]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 307, Loss: 0.7545926183815428\n",
      "tensor([[ 0.9991,  0.0420],\n",
      "        [ 0.0404,  0.9992],\n",
      "        [-0.9956, -0.0941]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.5213],\n",
      "        [ 9.2752],\n",
      "        [ 4.4400]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 308, Loss: 0.7759530044025993\n",
      "tensor([[ 0.9992,  0.0408],\n",
      "        [ 0.0017,  1.0000],\n",
      "        [-0.9956, -0.0940]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.5332],\n",
      "        [ 9.2697],\n",
      "        [ 4.4522]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 309, Loss: 0.73873537563785\n",
      "tensor([[ 0.9992,  0.0404],\n",
      "        [-0.0170,  0.9999],\n",
      "        [-0.9955, -0.0947]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.5450],\n",
      "        [ 9.2647],\n",
      "        [ 4.4673]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 310, Loss: 0.7409329303474465\n",
      "tensor([[ 0.9992,  0.0403],\n",
      "        [-0.0188,  0.9998],\n",
      "        [-0.9955, -0.0952]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.5568],\n",
      "        [ 9.2622],\n",
      "        [ 4.4812]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 311, Loss: 0.7332243691682431\n",
      "tensor([[ 9.9919e-01,  4.0251e-02],\n",
      "        [-1.0354e-04,  1.0000e+00],\n",
      "        [-9.9513e-01, -9.8600e-02]], dtype=torch.float64,\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.5687],\n",
      "        [ 9.2613],\n",
      "        [ 4.4909]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 312, Loss: 0.7390061071907623\n",
      "tensor([[ 0.9992,  0.0406],\n",
      "        [ 0.0429,  0.9991],\n",
      "        [-0.9948, -0.1017]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.5806],\n",
      "        [ 9.2589],\n",
      "        [ 4.5021]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 313, Loss: 0.7585936280531455\n",
      "tensor([[ 0.9991,  0.0423],\n",
      "        [ 0.0850,  0.9964],\n",
      "        [-0.9945, -0.1046]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.5925],\n",
      "        [ 9.2581],\n",
      "        [ 4.5134]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 314, Loss: 0.7434723497965772\n",
      "tensor([[ 0.9990,  0.0443],\n",
      "        [ 0.1286,  0.9917],\n",
      "        [-0.9942, -0.1078]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.6044],\n",
      "        [ 9.2544],\n",
      "        [ 4.5235]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 315, Loss: 0.7723064605516496\n",
      "tensor([[ 0.9990,  0.0453],\n",
      "        [ 0.1453,  0.9894],\n",
      "        [-0.9940, -0.1097]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.6161],\n",
      "        [ 9.2498],\n",
      "        [ 4.5318]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 316, Loss: 0.7717306653057766\n",
      "tensor([[ 0.9989,  0.0465],\n",
      "        [ 0.1246,  0.9922],\n",
      "        [-0.9939, -0.1101]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.6278],\n",
      "        [ 9.2442],\n",
      "        [ 4.5423]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 317, Loss: 0.7539902312944903\n",
      "tensor([[ 0.9988,  0.0493],\n",
      "        [ 0.0747,  0.9972],\n",
      "        [-0.9938, -0.1112]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.6394],\n",
      "        [ 9.2389],\n",
      "        [ 4.5525]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 318, Loss: 0.7555433170269026\n",
      "tensor([[ 0.9987,  0.0510],\n",
      "        [ 0.0241,  0.9997],\n",
      "        [-0.9937, -0.1122]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.6507],\n",
      "        [ 9.2324],\n",
      "        [ 4.5615]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 319, Loss: 0.7827375587936671\n",
      "tensor([[ 0.9987,  0.0517],\n",
      "        [-0.0128,  0.9999],\n",
      "        [-0.9937, -0.1125]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.6619],\n",
      "        [ 9.2283],\n",
      "        [ 4.5686]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 320, Loss: 0.7447029228121357\n",
      "tensor([[ 0.9987,  0.0516],\n",
      "        [-0.0183,  0.9998],\n",
      "        [-0.9940, -0.1094]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.6731],\n",
      "        [ 9.2249],\n",
      "        [ 4.5776]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 321, Loss: 0.744820189022957\n",
      "tensor([[ 0.9987,  0.0505],\n",
      "        [ 0.0121,  0.9999],\n",
      "        [-0.9947, -0.1033]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.6844],\n",
      "        [ 9.2177],\n",
      "        [ 4.5841]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 322, Loss: 0.78936065934281\n",
      "tensor([[ 0.9988,  0.0492],\n",
      "        [ 0.0599,  0.9982],\n",
      "        [-0.9954, -0.0960]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.6959],\n",
      "        [ 9.2091],\n",
      "        [ 4.5908]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 323, Loss: 0.7579463123458469\n",
      "tensor([[ 0.9989,  0.0476],\n",
      "        [ 0.0888,  0.9960],\n",
      "        [-0.9960, -0.0890]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.7071],\n",
      "        [ 9.1998],\n",
      "        [ 4.5966]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 324, Loss: 0.7697893237143316\n",
      "tensor([[ 0.9990,  0.0449],\n",
      "        [ 0.1323,  0.9912],\n",
      "        [-0.9965, -0.0836]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.7184],\n",
      "        [ 9.1885],\n",
      "        [ 4.6023]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 325, Loss: 0.7666519455467311\n",
      "tensor([[ 0.9991,  0.0431],\n",
      "        [ 0.1399,  0.9902],\n",
      "        [-0.9966, -0.0819]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.7296],\n",
      "        [ 9.1791],\n",
      "        [ 4.6058]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 326, Loss: 0.7496189631757951\n",
      "tensor([[ 0.9991,  0.0417],\n",
      "        [ 0.1214,  0.9926],\n",
      "        [-0.9965, -0.0832]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.7410],\n",
      "        [ 9.1694],\n",
      "        [ 4.6082]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 327, Loss: 0.7595657119420108\n",
      "tensor([[ 0.9992,  0.0410],\n",
      "        [ 0.0725,  0.9974],\n",
      "        [-0.9960, -0.0891]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.7524],\n",
      "        [ 9.1611],\n",
      "        [ 4.6105]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 328, Loss: 0.7301012676022713\n",
      "tensor([[ 0.9992,  0.0404],\n",
      "        [ 0.0138,  0.9999],\n",
      "        [-0.9954, -0.0960]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.7639],\n",
      "        [ 9.1553],\n",
      "        [ 4.6141]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 329, Loss: 0.7310723487097677\n",
      "tensor([[ 0.9992,  0.0404],\n",
      "        [-0.0209,  0.9998],\n",
      "        [-0.9946, -0.1037]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.7752],\n",
      "        [ 9.1532],\n",
      "        [ 4.6167]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 330, Loss: 0.740513813632432\n",
      "tensor([[ 0.9992,  0.0407],\n",
      "        [-0.0347,  0.9994],\n",
      "        [-0.9941, -0.1088]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.7863],\n",
      "        [ 9.1497],\n",
      "        [ 4.6182]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 331, Loss: 0.7797789638344681\n",
      "tensor([[ 0.9991,  0.0420],\n",
      "        [-0.0190,  0.9998],\n",
      "        [-0.9939, -0.1099]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.7973],\n",
      "        [ 9.1463],\n",
      "        [ 4.6189]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 332, Loss: 0.762984829890565\n",
      "tensor([[ 0.9990,  0.0442],\n",
      "        [ 0.0101,  0.9999],\n",
      "        [-0.9938, -0.1111]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.8082],\n",
      "        [ 9.1414],\n",
      "        [ 4.6179]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 333, Loss: 0.7685164611163106\n",
      "tensor([[ 0.9990,  0.0454],\n",
      "        [ 0.0520,  0.9986],\n",
      "        [-0.9941, -0.1087]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.8192],\n",
      "        [ 9.1370],\n",
      "        [ 4.6174]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 334, Loss: 0.7445395886794102\n",
      "tensor([[ 0.9989,  0.0463],\n",
      "        [ 0.0839,  0.9965],\n",
      "        [-0.9944, -0.1052]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.8302],\n",
      "        [ 9.1343],\n",
      "        [ 4.6161]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 335, Loss: 0.7377623968431993\n",
      "tensor([[ 0.9988,  0.0480],\n",
      "        [ 0.0835,  0.9965],\n",
      "        [-0.9947, -0.1024]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.8412],\n",
      "        [ 9.1313],\n",
      "        [ 4.6184]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 336, Loss: 0.7454154812075062\n",
      "tensor([[ 0.9987,  0.0506],\n",
      "        [ 0.0654,  0.9979],\n",
      "        [-0.9951, -0.0990]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.8521],\n",
      "        [ 9.1284],\n",
      "        [ 4.6127]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 337, Loss: 0.7723364495120715\n",
      "tensor([[ 0.9986,  0.0520],\n",
      "        [ 0.0416,  0.9991],\n",
      "        [-0.9956, -0.0938]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.8632],\n",
      "        [ 9.1240],\n",
      "        [ 4.6105]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 338, Loss: 0.7513053001363657\n",
      "tensor([[ 0.9985,  0.0547],\n",
      "        [ 0.0208,  0.9998],\n",
      "        [-0.9956, -0.0939]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.8740],\n",
      "        [ 9.1214],\n",
      "        [ 4.6044]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 339, Loss: 0.7668687835555927\n",
      "tensor([[ 0.9983,  0.0576],\n",
      "        [ 0.0123,  0.9999],\n",
      "        [-0.9956, -0.0933]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.8849],\n",
      "        [ 9.1213],\n",
      "        [ 4.5991]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 340, Loss: 0.7355657046481487\n",
      "tensor([[ 0.9982,  0.0607],\n",
      "        [ 0.0211,  0.9998],\n",
      "        [-0.9956, -0.0933]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.8959],\n",
      "        [ 9.1223],\n",
      "        [ 4.5935]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 341, Loss: 0.7495256664213378\n",
      "tensor([[ 0.9979,  0.0642],\n",
      "        [ 0.0412,  0.9991],\n",
      "        [-0.9959, -0.0901]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.9068],\n",
      "        [ 9.1236],\n",
      "        [ 4.5872]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 342, Loss: 0.7615863796176293\n",
      "tensor([[ 0.9978,  0.0668],\n",
      "        [ 0.0611,  0.9981],\n",
      "        [-0.9965, -0.0834]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.9177],\n",
      "        [ 9.1253],\n",
      "        [ 4.5841]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 343, Loss: 0.750682697922164\n",
      "tensor([[ 0.9976,  0.0696],\n",
      "        [ 0.0731,  0.9973],\n",
      "        [-0.9969, -0.0790]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.9286],\n",
      "        [ 9.1283],\n",
      "        [ 4.5799]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 344, Loss: 0.755425009256924\n",
      "tensor([[ 0.9974,  0.0717],\n",
      "        [ 0.0872,  0.9962],\n",
      "        [-0.9970, -0.0780]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.9398],\n",
      "        [ 9.1306],\n",
      "        [ 4.5723]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 345, Loss: 0.7636833699152078\n",
      "tensor([[ 0.9973,  0.0728],\n",
      "        [ 0.0884,  0.9961],\n",
      "        [-0.9971, -0.0760]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.9511],\n",
      "        [ 9.1305],\n",
      "        [ 4.5685]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 346, Loss: 0.756979578540648\n",
      "tensor([[ 0.9973,  0.0732],\n",
      "        [ 0.0649,  0.9979],\n",
      "        [-0.9969, -0.0781]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.9623],\n",
      "        [ 9.1316],\n",
      "        [ 4.5658]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 347, Loss: 0.7454511737099121\n",
      "tensor([[ 0.9974,  0.0726],\n",
      "        [ 0.0431,  0.9991],\n",
      "        [-0.9967, -0.0809]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.9737],\n",
      "        [ 9.1354],\n",
      "        [ 4.5626]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 348, Loss: 0.7196268073365769\n",
      "tensor([[ 0.9973,  0.0730],\n",
      "        [ 0.0245,  0.9997],\n",
      "        [-0.9964, -0.0844]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.9852],\n",
      "        [ 9.1401],\n",
      "        [ 4.5605]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 349, Loss: 0.7436993593063811\n",
      "tensor([[ 0.9973,  0.0729],\n",
      "        [ 0.0170,  0.9999],\n",
      "        [-0.9960, -0.0889]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[10.9964],\n",
      "        [ 9.1460],\n",
      "        [ 4.5577]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 350, Loss: 0.7599769122395309\n",
      "tensor([[ 0.9974,  0.0718],\n",
      "        [ 0.0174,  0.9998],\n",
      "        [-0.9955, -0.0948]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.0077],\n",
      "        [ 9.1516],\n",
      "        [ 4.5517]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 351, Loss: 0.7521681925330749\n",
      "tensor([[ 0.9975,  0.0702],\n",
      "        [ 0.0302,  0.9995],\n",
      "        [-0.9951, -0.0992]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.0190],\n",
      "        [ 9.1578],\n",
      "        [ 4.5491]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 352, Loss: 0.7315760812182921\n",
      "tensor([[ 0.9977,  0.0684],\n",
      "        [ 0.0433,  0.9991],\n",
      "        [-0.9949, -0.1007]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.0305],\n",
      "        [ 9.1645],\n",
      "        [ 4.5497]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 353, Loss: 0.7289071688676619\n",
      "tensor([[ 0.9978,  0.0660],\n",
      "        [ 0.0682,  0.9977],\n",
      "        [-0.9948, -0.1023]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.0420],\n",
      "        [ 9.1708],\n",
      "        [ 4.5445]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 354, Loss: 0.7657837495213604\n",
      "tensor([[ 0.9980,  0.0631],\n",
      "        [ 0.0817,  0.9967],\n",
      "        [-0.9945, -0.1047]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.0535],\n",
      "        [ 9.1758],\n",
      "        [ 4.5404]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 355, Loss: 0.7505756861973084\n",
      "tensor([[ 0.9982,  0.0605],\n",
      "        [ 0.0811,  0.9967],\n",
      "        [-0.9945, -0.1044]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.0649],\n",
      "        [ 9.1803],\n",
      "        [ 4.5380]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 356, Loss: 0.7613195266389379\n",
      "tensor([[ 0.9983,  0.0580],\n",
      "        [ 0.0712,  0.9975],\n",
      "        [-0.9943, -0.1064]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.0764],\n",
      "        [ 9.1843],\n",
      "        [ 4.5346]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 357, Loss: 0.7412678882725159\n",
      "tensor([[ 0.9985,  0.0545],\n",
      "        [ 0.0619,  0.9981],\n",
      "        [-0.9943, -0.1065]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.0881],\n",
      "        [ 9.1884],\n",
      "        [ 4.5328]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 358, Loss: 0.7319564659250847\n",
      "tensor([[ 0.9986,  0.0521],\n",
      "        [ 0.0604,  0.9982],\n",
      "        [-0.9946, -0.1037]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.0997],\n",
      "        [ 9.1932],\n",
      "        [ 4.5292]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 359, Loss: 0.757345556200835\n",
      "tensor([[ 0.9986,  0.0523],\n",
      "        [ 0.0501,  0.9987],\n",
      "        [-0.9950, -0.1000]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.1111],\n",
      "        [ 9.1973],\n",
      "        [ 4.5276]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 360, Loss: 0.7580966791324971\n",
      "tensor([[ 0.9986,  0.0523],\n",
      "        [ 0.0469,  0.9989],\n",
      "        [-0.9957, -0.0927]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.1226],\n",
      "        [ 9.2028],\n",
      "        [ 4.5264]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 361, Loss: 0.7354447301037369\n",
      "tensor([[ 0.9986,  0.0532],\n",
      "        [ 0.0531,  0.9986],\n",
      "        [-0.9964, -0.0850]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.1341],\n",
      "        [ 9.2079],\n",
      "        [ 4.5282]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 362, Loss: 0.7438527219994592\n",
      "tensor([[ 0.9986,  0.0537],\n",
      "        [ 0.0546,  0.9985],\n",
      "        [-0.9968, -0.0803]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.1456],\n",
      "        [ 9.2108],\n",
      "        [ 4.5328]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 363, Loss: 0.75298720006271\n",
      "tensor([[ 0.9986,  0.0537],\n",
      "        [ 0.0665,  0.9978],\n",
      "        [-0.9972, -0.0753]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.1572],\n",
      "        [ 9.2129],\n",
      "        [ 4.5352]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 364, Loss: 0.7565898429079083\n",
      "tensor([[ 0.9985,  0.0548],\n",
      "        [ 0.0790,  0.9969],\n",
      "        [-0.9974, -0.0720]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.1687],\n",
      "        [ 9.2184],\n",
      "        [ 4.5366]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 365, Loss: 0.73668696678587\n",
      "tensor([[ 0.9984,  0.0561],\n",
      "        [ 0.0846,  0.9964],\n",
      "        [-0.9971, -0.0757]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.1804],\n",
      "        [ 9.2240],\n",
      "        [ 4.5368]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 366, Loss: 0.7400069827917911\n",
      "tensor([[ 0.9982,  0.0598],\n",
      "        [ 0.0530,  0.9986],\n",
      "        [-0.9965, -0.0841]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.1921],\n",
      "        [ 9.2253],\n",
      "        [ 4.5391]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 367, Loss: 0.7729403488642149\n",
      "tensor([[ 0.9979,  0.0643],\n",
      "        [ 0.0291,  0.9996],\n",
      "        [-0.9956, -0.0938]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.2038],\n",
      "        [ 9.2272],\n",
      "        [ 4.5423]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 368, Loss: 0.7426506086494278\n",
      "tensor([[ 0.9977,  0.0684],\n",
      "        [ 0.0153,  0.9999],\n",
      "        [-0.9947, -0.1024]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.2156],\n",
      "        [ 9.2301],\n",
      "        [ 4.5456]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 369, Loss: 0.7443137472291849\n",
      "tensor([[ 0.9973,  0.0740],\n",
      "        [ 0.0141,  0.9999],\n",
      "        [-0.9944, -0.1059]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.2271],\n",
      "        [ 9.2341],\n",
      "        [ 4.5475]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 370, Loss: 0.7561281047484609\n",
      "tensor([[ 0.9969,  0.0790],\n",
      "        [ 0.0246,  0.9997],\n",
      "        [-0.9941, -0.1080]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.2387],\n",
      "        [ 9.2389],\n",
      "        [ 4.5492]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 371, Loss: 0.7485619731406172\n",
      "tensor([[ 0.9966,  0.0829],\n",
      "        [ 0.0429,  0.9991],\n",
      "        [-0.9940, -0.1094]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.2504],\n",
      "        [ 9.2454],\n",
      "        [ 4.5522]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 372, Loss: 0.7296482268261795\n",
      "tensor([[ 0.9963,  0.0854],\n",
      "        [ 0.0695,  0.9976],\n",
      "        [-0.9940, -0.1090]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.2621],\n",
      "        [ 9.2521],\n",
      "        [ 4.5568]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 373, Loss: 0.7326598894785288\n",
      "tensor([[ 0.9962,  0.0870],\n",
      "        [ 0.1006,  0.9949],\n",
      "        [-0.9942, -0.1076]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.2741],\n",
      "        [ 9.2572],\n",
      "        [ 4.5604]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 374, Loss: 0.7485158285414419\n",
      "tensor([[ 0.9962,  0.0873],\n",
      "        [ 0.1039,  0.9946],\n",
      "        [-0.9945, -0.1051]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.2859],\n",
      "        [ 9.2622],\n",
      "        [ 4.5581]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 375, Loss: 0.7562412045292831\n",
      "tensor([[ 0.9963,  0.0861],\n",
      "        [ 0.0953,  0.9954],\n",
      "        [-0.9947, -0.1033]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.2977],\n",
      "        [ 9.2678],\n",
      "        [ 4.5501]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 376, Loss: 0.7556016184451707\n",
      "tensor([[ 0.9964,  0.0842],\n",
      "        [ 0.0778,  0.9970],\n",
      "        [-0.9946, -0.1034]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.3093],\n",
      "        [ 9.2750],\n",
      "        [ 4.5404]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 377, Loss: 0.741282978162315\n",
      "tensor([[ 0.9968,  0.0796],\n",
      "        [ 0.0615,  0.9981],\n",
      "        [-0.9945, -0.1044]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.3206],\n",
      "        [ 9.2787],\n",
      "        [ 4.5309]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 378, Loss: 0.7839541932874271\n",
      "tensor([[ 0.9973,  0.0739],\n",
      "        [ 0.0507,  0.9987],\n",
      "        [-0.9942, -0.1077]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.3320],\n",
      "        [ 9.2824],\n",
      "        [ 4.5160]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 379, Loss: 0.7536503143834214\n",
      "tensor([[ 0.9977,  0.0684],\n",
      "        [ 0.0441,  0.9990],\n",
      "        [-0.9940, -0.1091]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.3435],\n",
      "        [ 9.2815],\n",
      "        [ 4.5040]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 380, Loss: 0.7668448956928884\n",
      "tensor([[ 0.9980,  0.0635],\n",
      "        [ 0.0248,  0.9997],\n",
      "        [-0.9936, -0.1129]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.3552],\n",
      "        [ 9.2819],\n",
      "        [ 4.4908]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 381, Loss: 0.728658813237339\n",
      "tensor([[ 0.9982,  0.0595],\n",
      "        [ 0.0218,  0.9998],\n",
      "        [-0.9935, -0.1140]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.3668],\n",
      "        [ 9.2823],\n",
      "        [ 4.4786]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 382, Loss: 0.7505844133561081\n",
      "tensor([[ 0.9984,  0.0561],\n",
      "        [ 0.0299,  0.9996],\n",
      "        [-0.9936, -0.1132]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.3784],\n",
      "        [ 9.2854],\n",
      "        [ 4.4652]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 383, Loss: 0.7356078631063854\n",
      "tensor([[ 0.9986,  0.0530],\n",
      "        [ 0.0454,  0.9990],\n",
      "        [-0.9938, -0.1113]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.3902],\n",
      "        [ 9.2877],\n",
      "        [ 4.4548]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 384, Loss: 0.7458566724553646\n",
      "tensor([[ 0.9987,  0.0508],\n",
      "        [ 0.0562,  0.9984],\n",
      "        [-0.9936, -0.1125]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.4018],\n",
      "        [ 9.2901],\n",
      "        [ 4.4467]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 385, Loss: 0.7443070692058007\n",
      "tensor([[ 0.9987,  0.0501],\n",
      "        [ 0.0592,  0.9982],\n",
      "        [-0.9937, -0.1123]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.4134],\n",
      "        [ 9.2942],\n",
      "        [ 4.4413]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 386, Loss: 0.7297107199853489\n",
      "tensor([[ 0.9988,  0.0497],\n",
      "        [ 0.0554,  0.9985],\n",
      "        [-0.9936, -0.1126]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.4246],\n",
      "        [ 9.2952],\n",
      "        [ 4.4392]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 387, Loss: 0.7760038095904617\n",
      "tensor([[ 0.9986,  0.0523],\n",
      "        [ 0.0318,  0.9995],\n",
      "        [-0.9935, -0.1141]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.4357],\n",
      "        [ 9.2992],\n",
      "        [ 4.4407]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 388, Loss: 0.7248416156148217\n",
      "tensor([[ 0.9984,  0.0558],\n",
      "        [ 0.0179,  0.9998],\n",
      "        [-0.9932, -0.1161]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.4468],\n",
      "        [ 9.3050],\n",
      "        [ 4.4403]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 389, Loss: 0.7317493820622772\n",
      "tensor([[ 0.9982,  0.0592],\n",
      "        [ 0.0130,  0.9999],\n",
      "        [-0.9929, -0.1190]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.4577],\n",
      "        [ 9.3094],\n",
      "        [ 4.4373]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 390, Loss: 0.7701990866704653\n",
      "tensor([[ 0.9980,  0.0630],\n",
      "        [ 0.0200,  0.9998],\n",
      "        [-0.9928, -0.1194]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.4687],\n",
      "        [ 9.3140],\n",
      "        [ 4.4345]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 391, Loss: 0.7419294739710457\n",
      "tensor([[ 0.9978,  0.0667],\n",
      "        [ 0.0411,  0.9992],\n",
      "        [-0.9928, -0.1201]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.4800],\n",
      "        [ 9.3201],\n",
      "        [ 4.4289]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 392, Loss: 0.733502124451826\n",
      "tensor([[ 0.9975,  0.0710],\n",
      "        [ 0.0617,  0.9981],\n",
      "        [-0.9932, -0.1168]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.4911],\n",
      "        [ 9.3258],\n",
      "        [ 4.4234]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 393, Loss: 0.7668243778355478\n",
      "tensor([[ 0.9973,  0.0741],\n",
      "        [ 0.0843,  0.9964],\n",
      "        [-0.9938, -0.1113]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.5022],\n",
      "        [ 9.3297],\n",
      "        [ 4.4155]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 394, Loss: 0.770403626111351\n",
      "tensor([[ 0.9970,  0.0768],\n",
      "        [ 0.0861,  0.9963],\n",
      "        [-0.9944, -0.1057]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.5128],\n",
      "        [ 9.3352],\n",
      "        [ 4.4113]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 395, Loss: 0.748958811207564\n",
      "tensor([[ 0.9968,  0.0794],\n",
      "        [ 0.0740,  0.9973],\n",
      "        [-0.9953, -0.0971]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.5237],\n",
      "        [ 9.3383],\n",
      "        [ 4.4115]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 396, Loss: 0.7449047501637501\n",
      "tensor([[ 0.9968,  0.0802],\n",
      "        [ 0.0638,  0.9980],\n",
      "        [-0.9960, -0.0899]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.5345],\n",
      "        [ 9.3434],\n",
      "        [ 4.4092]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 397, Loss: 0.7393440398306627\n",
      "tensor([[ 0.9968,  0.0800],\n",
      "        [ 0.0484,  0.9988],\n",
      "        [-0.9963, -0.0854]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.5455],\n",
      "        [ 9.3466],\n",
      "        [ 4.4146]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 398, Loss: 0.7316598547904993\n",
      "tensor([[ 0.9968,  0.0799],\n",
      "        [ 0.0338,  0.9994],\n",
      "        [-0.9966, -0.0819]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.5565],\n",
      "        [ 9.3484],\n",
      "        [ 4.4213]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 399, Loss: 0.7600835787655965\n",
      "tensor([[ 0.9968,  0.0794],\n",
      "        [ 0.0314,  0.9995],\n",
      "        [-0.9966, -0.0828]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.5675],\n",
      "        [ 9.3504],\n",
      "        [ 4.4243]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 400, Loss: 0.7573537524196915\n",
      "tensor([[ 0.9969,  0.0783],\n",
      "        [ 0.0446,  0.9990],\n",
      "        [-0.9964, -0.0842]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.5785],\n",
      "        [ 9.3518],\n",
      "        [ 4.4291]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 401, Loss: 0.7568960963077229\n",
      "tensor([[ 0.9970,  0.0773],\n",
      "        [ 0.0534,  0.9986],\n",
      "        [-0.9963, -0.0863]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.5896],\n",
      "        [ 9.3524],\n",
      "        [ 4.4394]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 402, Loss: 0.7450951974048092\n",
      "tensor([[ 0.9972,  0.0753],\n",
      "        [ 0.0740,  0.9973],\n",
      "        [-0.9960, -0.0894]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.6009],\n",
      "        [ 9.3514],\n",
      "        [ 4.4546]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 403, Loss: 0.7286423692773487\n",
      "tensor([[ 0.9974,  0.0723],\n",
      "        [ 0.0981,  0.9952],\n",
      "        [-0.9956, -0.0935]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.6124],\n",
      "        [ 9.3482],\n",
      "        [ 4.4694]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 404, Loss: 0.7551758423500068\n",
      "tensor([[ 0.9976,  0.0685],\n",
      "        [ 0.1024,  0.9947],\n",
      "        [-0.9952, -0.0974]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.6237],\n",
      "        [ 9.3450],\n",
      "        [ 4.4861]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 405, Loss: 0.7445383070977649\n",
      "tensor([[ 0.9979,  0.0652],\n",
      "        [ 0.0862,  0.9963],\n",
      "        [-0.9946, -0.1040]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.6349],\n",
      "        [ 9.3442],\n",
      "        [ 4.5026]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 406, Loss: 0.7256579819691632\n",
      "tensor([[ 0.9980,  0.0628],\n",
      "        [ 0.0498,  0.9988],\n",
      "        [-0.9939, -0.1104]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.6458],\n",
      "        [ 9.3458],\n",
      "        [ 4.5156]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 407, Loss: 0.7451921908728489\n",
      "tensor([[ 0.9982,  0.0607],\n",
      "        [ 0.0093,  1.0000],\n",
      "        [-0.9930, -0.1180]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.6566],\n",
      "        [ 9.3455],\n",
      "        [ 4.5225]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 408, Loss: 0.7704205212709937\n",
      "tensor([[ 0.9982,  0.0592],\n",
      "        [-0.0157,  0.9999],\n",
      "        [-0.9925, -0.1223]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.6672],\n",
      "        [ 9.3465],\n",
      "        [ 4.5302]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 409, Loss: 0.7458906901512086\n",
      "tensor([[ 0.9984,  0.0571],\n",
      "        [ 0.0045,  1.0000],\n",
      "        [-0.9922, -0.1249]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.6779],\n",
      "        [ 9.3429],\n",
      "        [ 4.5349]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 410, Loss: 0.7810309205835829\n",
      "tensor([[ 0.9985,  0.0549],\n",
      "        [ 0.0440,  0.9990],\n",
      "        [-0.9924, -0.1230]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.6886],\n",
      "        [ 9.3394],\n",
      "        [ 4.5385]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 411, Loss: 0.7486392668527033\n",
      "tensor([[ 0.9986,  0.0522],\n",
      "        [ 0.0854,  0.9963],\n",
      "        [-0.9928, -0.1202]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.6993],\n",
      "        [ 9.3336],\n",
      "        [ 4.5350]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 412, Loss: 0.7754421272609723\n",
      "tensor([[ 0.9988,  0.0492],\n",
      "        [ 0.1121,  0.9937],\n",
      "        [-0.9936, -0.1132]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.7098],\n",
      "        [ 9.3288],\n",
      "        [ 4.5320]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 413, Loss: 0.7496597265795267\n",
      "tensor([[ 0.9989,  0.0469],\n",
      "        [ 0.1161,  0.9932],\n",
      "        [-0.9946, -0.1034]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.7205],\n",
      "        [ 9.3241],\n",
      "        [ 4.5325]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 414, Loss: 0.7364242764190516\n",
      "tensor([[ 0.9990,  0.0457],\n",
      "        [ 0.0989,  0.9951],\n",
      "        [-0.9955, -0.0948]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.7310],\n",
      "        [ 9.3204],\n",
      "        [ 4.5359]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 415, Loss: 0.7472644733187963\n",
      "tensor([[ 0.9989,  0.0465],\n",
      "        [ 0.0657,  0.9978],\n",
      "        [-0.9960, -0.0894]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.7416],\n",
      "        [ 9.3168],\n",
      "        [ 4.5391]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 416, Loss: 0.75281717621304\n",
      "tensor([[ 0.9989,  0.0479],\n",
      "        [ 0.0279,  0.9996],\n",
      "        [-0.9961, -0.0878]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.7523],\n",
      "        [ 9.3151],\n",
      "        [ 4.5395]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 417, Loss: 0.731576379989714\n",
      "tensor([[ 0.9987,  0.0505],\n",
      "        [ 0.0037,  1.0000],\n",
      "        [-0.9961, -0.0880]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.7630],\n",
      "        [ 9.3155],\n",
      "        [ 4.5430]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 418, Loss: 0.7269315176542043\n",
      "tensor([[ 0.9985,  0.0547],\n",
      "        [-0.0033,  1.0000],\n",
      "        [-0.9962, -0.0874]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.7738],\n",
      "        [ 9.3158],\n",
      "        [ 4.5458]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 419, Loss: 0.7404912616545699\n",
      "tensor([[ 0.9983,  0.0578],\n",
      "        [ 0.0122,  0.9999],\n",
      "        [-0.9961, -0.0882]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.7848],\n",
      "        [ 9.3116],\n",
      "        [ 4.5484]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 420, Loss: 0.7680956842181175\n",
      "tensor([[ 0.9982,  0.0600],\n",
      "        [ 0.0379,  0.9993],\n",
      "        [-0.9958, -0.0919]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.7955],\n",
      "        [ 9.3082],\n",
      "        [ 4.5484]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 421, Loss: 0.762070842713003\n",
      "tensor([[ 0.9981,  0.0613],\n",
      "        [ 0.0667,  0.9978],\n",
      "        [-0.9953, -0.0972]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.8062],\n",
      "        [ 9.3040],\n",
      "        [ 4.5440]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 422, Loss: 0.7626679513833707\n",
      "tensor([[ 0.9980,  0.0635],\n",
      "        [ 0.0907,  0.9959],\n",
      "        [-0.9946, -0.1033]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.8171],\n",
      "        [ 9.2999],\n",
      "        [ 4.5364]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 423, Loss: 0.7525180885558476\n",
      "tensor([[ 0.9978,  0.0667],\n",
      "        [ 0.0970,  0.9953],\n",
      "        [-0.9938, -0.1115]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.8282],\n",
      "        [ 9.2987],\n",
      "        [ 4.5301]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 424, Loss: 0.7215968126002457\n",
      "tensor([[ 0.9976,  0.0697],\n",
      "        [ 0.0922,  0.9957],\n",
      "        [-0.9930, -0.1180]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.8392],\n",
      "        [ 9.2949],\n",
      "        [ 4.5242]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 425, Loss: 0.7648654166614548\n",
      "tensor([[ 0.9974,  0.0726],\n",
      "        [ 0.0757,  0.9971],\n",
      "        [-0.9922, -0.1249]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.8503],\n",
      "        [ 9.2894],\n",
      "        [ 4.5185]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 426, Loss: 0.7644013366228306\n",
      "tensor([[ 0.9974,  0.0722],\n",
      "        [ 0.0671,  0.9977],\n",
      "        [-0.9920, -0.1262]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.8613],\n",
      "        [ 9.2793],\n",
      "        [ 4.5150]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 427, Loss: 0.783194009614545\n",
      "tensor([[ 0.9974,  0.0725],\n",
      "        [ 0.0425,  0.9991],\n",
      "        [-0.9923, -0.1240]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.8723],\n",
      "        [ 9.2718],\n",
      "        [ 4.5111]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 428, Loss: 0.7377830161319205\n",
      "tensor([[ 0.9974,  0.0727],\n",
      "        [ 0.0340,  0.9994],\n",
      "        [-0.9929, -0.1189]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.8831],\n",
      "        [ 9.2632],\n",
      "        [ 4.5084]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 429, Loss: 0.7724152720872492\n",
      "tensor([[ 0.9975,  0.0704],\n",
      "        [ 0.0340,  0.9994],\n",
      "        [-0.9940, -0.1091]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.8939],\n",
      "        [ 9.2550],\n",
      "        [ 4.5076]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 430, Loss: 0.7393499899684347\n",
      "tensor([[ 0.9976,  0.0686],\n",
      "        [ 0.0309,  0.9995],\n",
      "        [-0.9951, -0.0986]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.9045],\n",
      "        [ 9.2493],\n",
      "        [ 4.5061]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 431, Loss: 0.7415724615277214\n",
      "tensor([[ 0.9977,  0.0671],\n",
      "        [ 0.0302,  0.9995],\n",
      "        [-0.9961, -0.0878]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1., 1., 1.], dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.9154],\n",
      "        [ 9.2449],\n",
      "        [ 4.5060]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 432, Loss: 0.7271572414205371\n",
      "tensor([[ 0.9978,  0.0657],\n",
      "        [ 0.0318,  0.9995],\n",
      "        [-0.9973, -0.0737]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.9263],\n",
      "        [ 9.2414],\n",
      "        [ 4.5095]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 433, Loss: 0.7302386588733083\n",
      "tensor([[ 0.9979,  0.0655],\n",
      "        [ 0.0326,  0.9995],\n",
      "        [-0.9982, -0.0605]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.9373],\n",
      "        [ 9.2388],\n",
      "        [ 4.5154]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 434, Loss: 0.7323712760299342\n",
      "tensor([[ 0.9978,  0.0668],\n",
      "        [ 0.0322,  0.9995],\n",
      "        [-0.9986, -0.0526]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.9482],\n",
      "        [ 9.2371],\n",
      "        [ 4.5208]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 435, Loss: 0.759004770573441\n",
      "tensor([[ 0.9977,  0.0672],\n",
      "        [ 0.0393,  0.9992],\n",
      "        [-0.9990, -0.0457]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.9591],\n",
      "        [ 9.2346],\n",
      "        [ 4.5287]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 436, Loss: 0.7492925375629496\n",
      "tensor([[ 0.9977,  0.0682],\n",
      "        [ 0.0388,  0.9992],\n",
      "        [-0.9989, -0.0463]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.9698],\n",
      "        [ 9.2341],\n",
      "        [ 4.5372]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 437, Loss: 0.7423733863110111\n",
      "tensor([[ 0.9977,  0.0678],\n",
      "        [ 0.0430,  0.9991],\n",
      "        [-0.9988, -0.0481]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.9804],\n",
      "        [ 9.2334],\n",
      "        [ 4.5464]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 438, Loss: 0.7438973256207603\n",
      "tensor([[ 0.9978,  0.0670],\n",
      "        [ 0.0618,  0.9981],\n",
      "        [-0.9984, -0.0568]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[11.9913],\n",
      "        [ 9.2317],\n",
      "        [ 4.5531]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 439, Loss: 0.7578906301430618\n",
      "tensor([[ 0.9977,  0.0678],\n",
      "        [ 0.0665,  0.9978],\n",
      "        [-0.9976, -0.0686]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.0021],\n",
      "        [ 9.2315],\n",
      "        [ 4.5630]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 440, Loss: 0.7410986928177207\n",
      "tensor([[ 0.9977,  0.0680],\n",
      "        [ 0.0589,  0.9983],\n",
      "        [-0.9963, -0.0861]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.0129],\n",
      "        [ 9.2334],\n",
      "        [ 4.5715]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 441, Loss: 0.7257501012850113\n",
      "tensor([[ 0.9977,  0.0671],\n",
      "        [ 0.0464,  0.9989],\n",
      "        [-0.9948, -0.1022]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.0236],\n",
      "        [ 9.2360],\n",
      "        [ 4.5768]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 442, Loss: 0.7549725582090637\n",
      "tensor([[ 0.9978,  0.0662],\n",
      "        [ 0.0299,  0.9996],\n",
      "        [-0.9935, -0.1140]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.0342],\n",
      "        [ 9.2402],\n",
      "        [ 4.5820]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 443, Loss: 0.7413248477973072\n",
      "tensor([[ 0.9979,  0.0655],\n",
      "        [ 0.0194,  0.9998],\n",
      "        [-0.9924, -0.1231]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.0444],\n",
      "        [ 9.2462],\n",
      "        [ 4.5836]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 444, Loss: 0.7450212072100725\n",
      "tensor([[ 0.9979,  0.0640],\n",
      "        [ 0.0193,  0.9998],\n",
      "        [-0.9917, -0.1285]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.0549],\n",
      "        [ 9.2534],\n",
      "        [ 4.5852]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 445, Loss: 0.726510942767856\n",
      "tensor([[ 0.9980,  0.0630],\n",
      "        [ 0.0239,  0.9997],\n",
      "        [-0.9913, -0.1316]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.0654],\n",
      "        [ 9.2611],\n",
      "        [ 4.5812]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 446, Loss: 0.7493071300033708\n",
      "tensor([[ 0.9980,  0.0629],\n",
      "        [ 0.0287,  0.9996],\n",
      "        [-0.9909, -0.1347]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.0759],\n",
      "        [ 9.2704],\n",
      "        [ 4.5708]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 447, Loss: 0.7391348215311675\n",
      "tensor([[ 0.9981,  0.0620],\n",
      "        [ 0.0262,  0.9997],\n",
      "        [-0.9908, -0.1354]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.0862],\n",
      "        [ 9.2760],\n",
      "        [ 4.5613]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 448, Loss: 0.774291228971505\n",
      "tensor([[ 0.9981,  0.0623],\n",
      "        [ 0.0176,  0.9998],\n",
      "        [-0.9914, -0.1309]], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], dtype=torch.float64,\n",
      "       grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[12.0962],\n",
      "        [ 9.2815],\n",
      "        [ 4.5493]], dtype=torch.float64, requires_grad=True)\n",
      "Iteration 449, Loss: 0.7581453376633099\n"
     ]
    }
   ],
   "source": [
    "import geotorch\n",
    "\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "\n",
    "class GalleryParams(torch.nn.Module):\n",
    "    def __init__(self, init_mean, init_kappa):\n",
    "        super(GalleryParams, self).__init__()\n",
    "        self.gallery_means = torch.nn.Parameter(torch.tensor(init_mean))\n",
    "        # self.gallery_kappas =  torch.nn.Parameter(torch.tensor(init_kappa))\n",
    "        # self.gallery_means = torch.nn.Parameter(torch.rand(3, 2, dtype=torch.float64))\n",
    "        self.gallery_kappas = torch.nn.Parameter(\n",
    "            torch.rand(3, 1, dtype=torch.float64) * 10\n",
    "        )\n",
    "\n",
    "\n",
    "gallery_params = GalleryParams(init_mean, init_kappa)\n",
    "target_class = torch.tensor(gallery_subject_ids_sorted)\n",
    "T = torch.nn.Parameter(torch.tensor(1.0))\n",
    "geotorch.sphere(gallery_params, \"gallery_means\")\n",
    "\n",
    "\n",
    "# train\n",
    "M = 10\n",
    "mc_prob = MonteCarloPredictiveProb(M=M)\n",
    "\n",
    "# optimizer = torch.optim.Adam([gallery_means, gallery_kappas], lr=10e-3)\n",
    "optimizer = torch.optim.Adam(gallery_params.parameters(), lr=0.1)\n",
    "\n",
    "num_steps = 450\n",
    "\n",
    "for iter in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    # compute nll loss\n",
    "    log_probs = mc_prob(\n",
    "        gallery_features,\n",
    "        gallery_unc,\n",
    "        gallery_params.gallery_means,\n",
    "        gallery_params.gallery_kappas,\n",
    "        T,\n",
    "    )[:, :, :-1]\n",
    "    probs = torch.exp(log_probs)\n",
    "    mean_probs = torch.mean(probs, axis=1)\n",
    "    log_probs_new = torch.log(mean_probs)\n",
    "    loss = nll_loss(log_probs_new, target_class)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(gallery_params.gallery_means)\n",
    "    print(torch.norm(gallery_params.gallery_means, dim=-1))\n",
    "    print(gallery_params.gallery_kappas)\n",
    "    print(f\"Iteration {iter}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fontsize = 20\n",
    "linewidth = 3\n",
    "dot_size = 80\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "draw_circle(ax, linewidth)\n",
    "for class_id, (class_mean, kappa) in enumerate(\n",
    "    zip(gallery_params.gallery_means, gallery_params.gallery_kappas)\n",
    "):\n",
    "    color = colors[class_id]\n",
    "    class_mean = class_mean.detach().numpy()\n",
    "    kappa = kappa.detach().numpy()\n",
    "    class_point_angle = np.angle([class_mean[0] + 1j * class_mean[1]])[0]\n",
    "    draw_dencity(\n",
    "        class_point_angle,\n",
    "        kappa,\n",
    "        ax,\n",
    "        linewidth=3,\n",
    "        color=color,\n",
    "        range=np.pi / 2,\n",
    "        draw_center=True,\n",
    "        dot_size=dot_size,\n",
    "        type=\"power\",\n",
    "    )\n",
    "    for position in np.where(gallery_subject_ids_sorted == class_id)[0]:\n",
    "        point_angle = np.angle(\n",
    "            [gallery_features[position][0] + 1j * gallery_features[position][1]]\n",
    "        )[0]\n",
    "        draw_dencity(\n",
    "            point_angle,\n",
    "            gallery_unc[position],\n",
    "            ax,\n",
    "            linewidth=1,\n",
    "            color=color,\n",
    "            range=np.pi / 2,\n",
    "            scale=0.1,\n",
    "            draw_center=True,\n",
    "            dot_size=20,\n",
    "        )\n",
    "fig.gca().set_aspect(\"equal\")\n",
    "fig.show()\n",
    "plt.savefig(\"/app/outputs/images/trained.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Probability estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_angles = np.array([np.pi / 3])\n",
    "test_vectors = get_vectors_by_angle(test_angles)\n",
    "test_kappa = np.array([[10]])\n",
    "M = 100\n",
    "mc_prob = MonteCarloPredictiveProb(M=M)\n",
    "\n",
    "log_probs = mc_prob(\n",
    "    test_vectors,\n",
    "    test_kappa,\n",
    "    gallery_params.gallery_means,\n",
    "    gallery_params.gallery_kappas,\n",
    "    T,\n",
    ")  # [:, :, :-1]\n",
    "probs = torch.exp(log_probs)\n",
    "mean_probs = torch.mean(probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1303, 0.4251, 0.0018, 0.4428]], dtype=torch.float64,\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fontsize = 20\n",
    "linewidth = 3\n",
    "dot_size = 80\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "draw_circle(ax, linewidth)\n",
    "draw_dencity(\n",
    "    test_angles[0],\n",
    "    test_kappa[0],\n",
    "    ax,\n",
    "    linewidth=1,\n",
    "    color=\"tab:cyan\",\n",
    "    range=np.pi / 2,\n",
    "    scale=1,\n",
    "    draw_center=True,\n",
    "    dot_size=20,\n",
    ")\n",
    "for class_id, (class_mean, kappa) in enumerate(\n",
    "    zip(gallery_params.gallery_means, gallery_params.gallery_kappas)\n",
    "):\n",
    "    color = colors[class_id]\n",
    "    class_mean = class_mean.detach().numpy()\n",
    "    kappa = kappa.detach().numpy()\n",
    "    class_point_angle = np.angle([class_mean[0] + 1j * class_mean[1]])[0]\n",
    "    draw_dencity(\n",
    "        class_point_angle,\n",
    "        kappa,\n",
    "        ax,\n",
    "        linewidth=3,\n",
    "        color=color,\n",
    "        range=np.pi / 2,\n",
    "        draw_center=True,\n",
    "        dot_size=dot_size,\n",
    "        type=\"power\",\n",
    "    )\n",
    "fig.gca().set_aspect(\"equal\")\n",
    "fig.show()\n",
    "plt.savefig(\"/app/outputs/images/test.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tab:blue', 'tab:orange', 'tab:green']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
