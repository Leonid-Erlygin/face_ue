{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import SphereClassifier, WhaleDataModule\n",
    "from src.dataset import load_df\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from config.config import Config, load_config\n",
    "\n",
    "# cuda = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SphereClassifier.load_from_checkpoint(\n",
    "#     checkpoint_path=\"/app/sandbox/happy_whale/kaggle-happywhale-1st-place/result/b6_bottleneck_feature_fix_nb/1/last-v4.ckpt\"\n",
    "# )\n",
    "# model.to(cuda)\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "# image = torch.rand(1, 3, 528, 528).to(cuda)\n",
    "# logits_ids, logits_species = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot\n",
    "\n",
    "# image = torch.rand(1, 3, 528, 528).to(cuda)\n",
    "# yhat = model(image)\n",
    "# make_dot(yhat, params=dict(list(model.named_parameters()))).render(\n",
    "#     \"b6_torchviz\", \"b6.png\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute embs on train ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used default config lr_backbone: 0.0016\n",
      "used default config lr_head: 0.016\n",
      "used default config lr_decay_scale: 0.01\n",
      "used default config num_classes: 15587\n",
      "used default config num_species_classes: 26\n",
      "used default config pretrained: True\n",
      "used default config val_bbox: fullbody\n",
      "used default config test_bboxes: ['fullbody', 'fullbody_charm']\n",
      "used default config bboxes: {'fullbody_charm': 0.15, 'fullbody': 0.6, 'backfin': 0.15, 'detic': 0.05, 'none': 0.05}\n",
      "used default config bbox_conf_threshold: 0.01\n",
      "used default config n_data: -1\n",
      "used default config global_pool: {'arch': 'GeM', 'p': 3, 'train': False}\n",
      "used default config normalization: batchnorm\n",
      "used default config optimizer: AdamW\n",
      "used default config loss_fn: CrossEntropy\n",
      "used default config loss_id_ratio: 0.437338\n",
      "used default config margin_coef_id: 0.27126\n",
      "used default config margin_coef_species: 0.226253\n",
      "used default config margin_power_id: -0.364399\n",
      "used default config margin_power_species: -0.720133\n",
      "used default config s_id: 20.9588\n",
      "used default config s_species: 33.1383\n",
      "used default config margin_cons_id: 0.05\n",
      "used default config margin_cons_species: 0.05\n",
      "used default config n_center_id: 1\n",
      "used default config n_center_species: 1\n",
      "used default config aug: {'rotate': 15, 'translate': 0.25, 'shear': 3, 'p_affine': 0.5, 'crop_scale': 0.9, 'crop_l': 0.75, 'crop_r': 1.3333333333333333, 'p_gray': 0.1, 'p_blur': 0.05, 'p_noise': 0.05, 'p_downscale': 0.0, 'p_shuffle': 0.3, 'p_posterize': 0.2, 'p_bright_contrast': 0.5, 'p_cutout': 0.05, 'p_snow': 0.1, 'p_rain': 0.05}\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"config/efficientnet_b6_new.yaml\", \"config/default.yaml\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detic low conf: 0 / 51033\n",
      "fullbody low conf: 0 / 51033\n",
      "fullbody_charm low conf: 10 / 51033\n",
      "backfin low conf: 1587 / 51033\n"
     ]
    }
   ],
   "source": [
    "df = load_df(\"input\", cfg, \"train.csv\", True)\n",
    "data_module = WhaleDataModule(\n",
    "    df,\n",
    "    cfg,\n",
    "    f\"input/train_images\",\n",
    "    cfg.val_bbox,\n",
    "    -1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_module.get_dataset(df, False)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=32,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = []\n",
    "# model.eval()\n",
    "\n",
    "# for batch in tqdm(train_loader):\n",
    "#     images = batch[\"image\"].to(cuda)\n",
    "#     out = model(images)\n",
    "#     bottleneck_feat = model.get_bottleneck_feature(images)\n",
    "#     feats = model.backbone_head_bn(model.backbone_head(bottleneck_feat))\n",
    "#     feats = F.normalize(feats, p=2.0, dim=1)\n",
    "#     predictions.append(feats.detach().cpu())\n",
    "#     break\n",
    "# embs = torch.cat(predictions, axis=0).numpy()\n",
    "# np.savez(f\"whale_train_emb.npz\", embs=embs)\n",
    "# out[0].max(), out[1].sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Whale Train OSFR protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check scf embs\n",
    "# a = np.load(\"whale_train_emb.npz\")\n",
    "# b = np.load(\"/app/cache/features/scf_embs_whale.npz\")\n",
    "# a[\"embs\"], b[\"embs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create whale validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set_id_num = 5000\n",
    "rng = np.random.default_rng(1)\n",
    "\n",
    "unique_ids, count_ids = np.unique(train_dataset.ids, return_counts=True)\n",
    "validation_id_idx = rng.choice(\n",
    "    unique_ids.shape[0], validation_set_id_num, replace=False\n",
    ")\n",
    "test_id_idx = np.array(\n",
    "    sorted(list(set(range(unique_ids.shape[0])) - set(validation_id_idx)))\n",
    ")\n",
    "assert validation_id_idx.shape[0] + test_id_idx.shape[0] == unique_ids.shape[0]\n",
    "assert set(validation_id_idx).intersection(test_id_idx) == set()\n",
    "\n",
    "validation_unique_ids, validation_count_ids = (\n",
    "    unique_ids[validation_id_idx],\n",
    "    count_ids[validation_id_idx],\n",
    ")\n",
    "test_unique_ids, test_count_ids = unique_ids[test_id_idx], count_ids[test_id_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def create_whale_dataset(train_dataset, unique_ids, count_ids, full_ds_embs, ds_name):\n",
    "\n",
    "    out_of_gallery_ids = unique_ids[count_ids == 1]  # single image ids\n",
    "    in_gallery_ids = unique_ids[count_ids > 1]\n",
    "    print(out_of_gallery_ids.shape, in_gallery_ids.shape)\n",
    "\n",
    "    # construct gallery and probe temlates\n",
    "    image_path_to_template_id = {}\n",
    "    image_path_to_subject_id = {}\n",
    "    image_path_to_emb = {}\n",
    "    image_path_to_unc = {}\n",
    "\n",
    "    # select embeddings\n",
    "    img_names = []  # train_dataset.x_paths\n",
    "    for subject in tqdm(unique_ids):\n",
    "        local_idx = train_dataset.ids == subject\n",
    "        subject_images_paths = train_dataset.x_paths[local_idx]\n",
    "        subject_embs = full_ds_embs[\"embs\"][local_idx]\n",
    "        subject_unc = full_ds_embs[\"unc\"][local_idx]\n",
    "        for image_path, emb, unc in zip(\n",
    "            subject_images_paths, subject_embs, subject_unc\n",
    "        ):\n",
    "            img_names.append(image_path)\n",
    "            image_path_to_emb[image_path] = emb[np.newaxis, :]\n",
    "            image_path_to_unc[image_path] = unc[np.newaxis, :]\n",
    "\n",
    "    gallery_templates = []\n",
    "    known_probe_templates = []\n",
    "\n",
    "    subject_id = 0\n",
    "    gallery_template_id = 0\n",
    "    probe_template_id = 10000\n",
    "    for subject in tqdm(in_gallery_ids):\n",
    "        subject_images_paths = train_dataset.x_paths[train_dataset.ids == subject]\n",
    "\n",
    "        image_count = len(subject_images_paths)\n",
    "        for i, image_path in enumerate(subject_images_paths):\n",
    "            image_path_to_subject_id[image_path] = subject_id\n",
    "            if i < image_count // 2:\n",
    "                image_path_to_template_id[image_path] = gallery_template_id\n",
    "            if i >= image_count // 2:\n",
    "                image_path_to_template_id[image_path] = probe_template_id\n",
    "\n",
    "        gallery_templates.append(\n",
    "            (subject_images_paths[: image_count // 2], gallery_template_id, subject_id)\n",
    "        )\n",
    "        known_probe_templates.append(\n",
    "            (subject_images_paths[image_count // 2 :], probe_template_id, subject_id)\n",
    "        )\n",
    "        gallery_template_id += 1\n",
    "        probe_template_id += 1\n",
    "        subject_id += 1\n",
    "\n",
    "    assert gallery_template_id < 10000\n",
    "    unknown_probe_templates = []\n",
    "\n",
    "    for probe_subject in tqdm(out_of_gallery_ids):\n",
    "        probe_images_paths = train_dataset.x_paths[train_dataset.ids == probe_subject]\n",
    "        for image_path in probe_images_paths:\n",
    "            image_path = str(image_path)\n",
    "            image_path_to_subject_id[image_path] = subject_id\n",
    "            image_path_to_template_id[image_path] = probe_template_id\n",
    "        unknown_probe_templates.append(\n",
    "            (probe_images_paths, probe_template_id, subject_id)\n",
    "        )\n",
    "        probe_template_id += 1\n",
    "        subject_id += 1\n",
    "\n",
    "    # assert len(image_path_to_template_id) == len(train_dataset.x_paths)\n",
    "    # assert len(image_path_to_subject_id) == len(train_dataset.x_paths)\n",
    "    assert len(set(image_path_to_subject_id.values())) == len(unique_ids)\n",
    "    assert len(set(image_path_to_template_id.values())) == len(unique_ids) + len(\n",
    "        in_gallery_ids\n",
    "    )\n",
    "    print(\n",
    "        len(gallery_templates), len(known_probe_templates), len(unknown_probe_templates)\n",
    "    )\n",
    "    print(\n",
    "        len(unknown_probe_templates)\n",
    "        / (len(known_probe_templates) + len(unknown_probe_templates))\n",
    "    )\n",
    "    print(len(known_probe_templates) + len(unknown_probe_templates))\n",
    "\n",
    "    # create meta files\n",
    "    # tid mid\n",
    "    identification_ds_path = Path(f\"/app/datasets/{ds_name}\")\n",
    "    identification_ds_path.mkdir(exist_ok=True)\n",
    "    meta_path = identification_ds_path / \"meta\"\n",
    "    embeddings_path = identification_ds_path / \"embeddings\"\n",
    "    embeddings_path.mkdir(exist_ok=True)\n",
    "    meta_path.mkdir(exist_ok=True)\n",
    "\n",
    "    mids = np.arange(len(img_names))\n",
    "    tids = []\n",
    "    sids = []\n",
    "\n",
    "    for image_path in img_names:\n",
    "        tids.append(image_path_to_template_id[image_path])\n",
    "        sids.append(image_path_to_subject_id[image_path])\n",
    "\n",
    "    out_file_tid_mid = meta_path / Path(f\"{ds_name}_face_tid_mid.txt\")\n",
    "    with open(out_file_tid_mid, \"w\") as fd:\n",
    "        for name, tid, sid, mid in zip(img_names, tids, sids, mids):\n",
    "            fd.write(f\"{name} {tid} {mid} {sid}\\n\")\n",
    "\n",
    "    out_file_probe = meta_path / Path(f\"{ds_name}_1N_probe_mixed.csv\")\n",
    "    out_file_gallery = meta_path / Path(f\"{ds_name}_1N_gallery_G1.csv\")\n",
    "\n",
    "    tids_probe = []\n",
    "    sids_probe = []\n",
    "    names_probe = []\n",
    "    for probe_meta in known_probe_templates + unknown_probe_templates:\n",
    "        tids_probe.extend([probe_meta[1]] * len(probe_meta[0]))\n",
    "        sids_probe.extend([probe_meta[2]] * len(probe_meta[0]))\n",
    "        names_probe.extend([x.split(\"/\")[-1] for x in probe_meta[0]])\n",
    "\n",
    "    tids_gallery = []\n",
    "    sids_gallery = []\n",
    "    names_gallery = []\n",
    "\n",
    "    for gallery_meta in gallery_templates:\n",
    "        tids_gallery.extend([gallery_meta[1]] * len(gallery_meta[0]))\n",
    "        sids_gallery.extend([gallery_meta[2]] * len(gallery_meta[0]))\n",
    "        names_gallery.extend([x.split(\"/\")[-1] for x in gallery_meta[0]])\n",
    "\n",
    "    assert len(tids_gallery) + len(tids_probe) == len(img_names)\n",
    "    probe = pd.DataFrame(\n",
    "        {\n",
    "            \"TEMPLATE_ID\": tids_probe,\n",
    "            \"SUBJECT_ID\": sids_probe,\n",
    "            \"FILENAME\": names_probe,\n",
    "        }\n",
    "    )\n",
    "    gallery = pd.DataFrame(\n",
    "        {\n",
    "            \"TEMPLATE_ID\": tids_gallery,\n",
    "            \"SUBJECT_ID\": sids_gallery,\n",
    "            \"FILENAME\": names_gallery,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    probe.to_csv(out_file_probe, sep=\",\", index=False)\n",
    "    gallery.to_csv(out_file_gallery, sep=\",\", index=False)\n",
    "\n",
    "    # save embedding\n",
    "    embs = []\n",
    "    uncs = []\n",
    "    for image_name in image_path_to_emb.keys():\n",
    "        embs.append(image_path_to_emb[image_name])\n",
    "        uncs.append(image_path_to_unc[image_name])\n",
    "    embs = np.concatenate(embs, axis=0)\n",
    "    uncs = np.concatenate(uncs, axis=0)\n",
    "    print(embs.shape, uncs.shape)\n",
    "    np.savez(embeddings_path / f\"scf_embs_{ds_name}.npz\", embs=embs, unc=uncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/5000 [00:00<04:54, 16.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2985,) (2015,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [04:11<00:00, 19.88it/s]\n",
      "100%|██████████| 2015/2015 [00:00<00:00, 37515.80it/s]\n",
      "100%|██████████| 2985/2985 [00:00<00:00, 44559.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015 2015 2985\n",
      "0.597\n",
      "5000\n",
      "(17271, 512) (17271, 1)\n"
     ]
    }
   ],
   "source": [
    "full_ds_embs = np.load(\"/app/cache/features/scf_embs_whale.npz\")\n",
    "create_whale_dataset(\n",
    "    train_dataset,\n",
    "    validation_unique_ids,\n",
    "    validation_count_ids,\n",
    "    full_ds_embs,\n",
    "    \"whale_val\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/10587 [00:00<09:16, 19.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6273,) (4314,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10587/10587 [08:54<00:00, 19.83it/s]\n",
      "100%|██████████| 4314/4314 [00:00<00:00, 38457.28it/s]\n",
      "100%|██████████| 6273/6273 [00:00<00:00, 44744.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4314 4314 6273\n",
      "0.5925191272315103\n",
      "10587\n",
      "(33762, 512) (33762, 1)\n"
     ]
    }
   ],
   "source": [
    "create_whale_dataset(\n",
    "    train_dataset, test_unique_ids, test_count_ids, full_ds_embs, \"whale\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.62920386, -0.009191476)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load(\"/app/datasets/whale_val/embeddings/scf_embs_whale.npz\")\n",
    "a[\"embs\"][0] @ a[\"embs\"][1], a[\"embs\"][0] @ a[\"embs\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.80075425, 0.8756635, 0.8109231, 0.709551, 0.12658957)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"embs\"][4] @ a[\"embs\"][5], a[\"embs\"][4] @ a[\"embs\"][6], a[\"embs\"][4] @ a[\"embs\"][\n",
    "    9\n",
    "], a[\"embs\"][4] @ a[\"embs\"][10], a[\"embs\"][4] @ a[\"embs\"][16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51033, 512), (51033, 1))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ds_embs[\"embs\"].shape, full_ds_embs[\"unc\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_whale_dataset(\n",
    "    train_dataset, test_unique_ids, test_count_ids, full_ds_embs, \"whale\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_dataset.x_paths).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dir = identification_ds_path / \"embeddings\"\n",
    "emb_dir.mkdir(exist_ok=True)\n",
    "np.savez(emb_dir / \"b6_embs_whale.npz\", **a, unc=np.ones((a[\"embs\"].shape[0], 1)) * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones((a[\"embs\"].shape[0], 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
